{
  "django__django-11790": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-11790",
    "model_patch": "diff --git a/django/contrib/auth/forms.py b/django/contrib/auth/forms.py\nindex a0cfed0995..658daf9690 100644\n--- a/django/contrib/auth/forms.py\n+++ b/django/contrib/auth/forms.py\n@@ -62,11 +62,16 @@ class UsernameField(forms.CharField):\n         return unicodedata.normalize('NFKC', super().to_python(value))\n \n     def widget_attrs(self, widget):\n-        return {\n+        attrs = {\n             **super().widget_attrs(widget),\n             'autocapitalize': 'none',\n             'autocomplete': 'username',\n         }\n+        # Add maxlength attribute to the widget when max_length is set on the field.\n+        max_length = getattr(self, 'max_length', None)\n+        if max_length:\n+            attrs['maxlength'] = str(max_length)\n+        return attrs\n \n \n class UserCreationForm(forms.ModelForm):\n"
  },
  "django__django-11815": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-11815",
    "model_patch": "diff --git a/django/db/migrations/serializer.py b/django/db/migrations/serializer.py\nindex 27b5cbd379..03daf692ca 100644\n--- a/django/db/migrations/serializer.py\n+++ b/django/db/migrations/serializer.py\n@@ -120,9 +120,11 @@ class EnumSerializer(BaseSerializer):\n     def serialize(self):\n         enum_class = self.value.__class__\n         module = enum_class.__module__\n-        v_string, v_imports = serializer_factory(self.value.value).serialize()\n-        imports = {'import %s' % module, *v_imports}\n-        return \"%s.%s(%s)\" % (module, enum_class.__name__, v_string), imports\n+        # Serialize enums by member name lookup (EnumClass['NAME']) so that\n+        # translatable or mutable enum values don't break migrations.\n+        name = self.value.name\n+        imports = {'import %s' % module}\n+        return \"%s.%s['%s']\" % (module, enum_class.__name__, name), imports\n \n \n class FloatSerializer(BaseSimpleSerializer):\n"
  },
  "django__django-11848": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-11848",
    "model_patch": "diff --git a/django/utils/http.py b/django/utils/http.py\nindex 572cfb4347..79cb4a490c 100644\n--- a/django/utils/http.py\n+++ b/django/utils/http.py\n@@ -176,10 +176,16 @@ def parse_http_date(date):\n     try:\n         year = int(m.group('year'))\n         if year < 100:\n-            if year < 70:\n-                year += 2000\n-            else:\n-                year += 1900\n+            # Interpret two-digit years per RFC 7231: recipients MUST interpret a\n+            # timestamp that appears to be more than 50 years in the future as\n+            # representing the most recent year in the past that had the same\n+            # last two digits. Compute a candidate year in the current century\n+            # and, if it is more than 50 years ahead, roll back a century.\n+            now = datetime.datetime.utcnow().year\n+            century = (now // 100) * 100\n+            year = century + year\n+            if year - now > 50:\n+                year -= 100\n         month = MONTHS.index(m.group('mon').lower()) + 1\n         day = int(m.group('day'))\n         hour = int(m.group('hour'))\n"
  },
  "django__django-11880": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-11880",
    "model_patch": "diff --git a/django/forms/fields.py b/django/forms/fields.py\nindex 03cd8af8d7..4cc3133ca0 100644\n--- a/django/forms/fields.py\n+++ b/django/forms/fields.py\n@@ -200,6 +200,7 @@ class Field:\n         memo[id(self)] = result\n         result.widget = copy.deepcopy(self.widget, memo)\n         result.validators = self.validators[:]\n+        result.error_messages = copy.deepcopy(self.error_messages, memo)\n         return result\n \n \n"
  },
  "django__django-11885": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-11885",
    "model_patch": "diff --git a/django/db/models/deletion.py b/django/db/models/deletion.py\nindex ae82d26340..0825c08536 100644\n--- a/django/db/models/deletion.py\n+++ b/django/db/models/deletion.py\n@@ -309,10 +309,33 @@ class Collector:\n                         sender=model, instance=obj, using=self.using\n                     )\n \n-            # fast deletes\n+            # fast deletes: combine by model when possible by aggregating PKs\n+            from collections import defaultdict\n+            grouped = defaultdict(list)\n             for qs in self.fast_deletes:\n-                count = qs._raw_delete(using=self.using)\n-                deleted_counter[qs.model._meta.label] += count\n+                grouped[qs.model].append(qs)\n+            for model, qss in grouped.items():\n+                # try to aggregate PKs from simple querysets\n+                pk_set = set()\n+                fallback_qs = []\n+                for qs in qss:\n+                    try:\n+                        # values_list will evaluate the queryset; that's acceptable for fast deletes\n+                        vals = list(qs.values_list(qs.model._meta.pk.attname, flat=True))\n+                    except Exception:\n+                        fallback_qs.append(qs)\n+                    else:\n+                        if not vals:\n+                            continue\n+                        pk_set.update(vals)\n+                if pk_set:\n+                    query = sql.DeleteQuery(model)\n+                    count = query.delete_batch(list(pk_set), self.using)\n+                    deleted_counter[model._meta.label] += count\n+                # fallback to raw delete for any queries we couldn't aggregate\n+                for qs in fallback_qs:\n+                    count = qs._raw_delete(using=self.using)\n+                    deleted_counter[qs.model._meta.label] += count\n \n             # update fields\n             for model, instances_for_fieldvalues in self.field_updates.items():\n"
  },
  "django__django-11951": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-11951",
    "model_patch": "diff --git a/django/db/models/query.py b/django/db/models/query.py\nindex 794e0faae7..9aed4a714d 100644\n--- a/django/db/models/query.py\n+++ b/django/db/models/query.py\n@@ -472,6 +472,9 @@ class QuerySet:\n         objs = list(objs)\n         self._populate_pk_values(objs)\n         with transaction.atomic(using=self.db, savepoint=False):\n+            # Determine max compatible batch size per backend and cap provided batch_size\n+            max_batch_size = connection.ops.bulk_batch_size(fields)\n+            batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)\n             if objs_with_pk:\n                 returned_columns = self._batched_insert(\ndiff --git a/django/db/models/query.py.bak b/django/db/models/query.py.bak\nnew file mode 100644\nindex 0000000000..794e0faae7\n--- /dev/null\n+++ b/django/db/models/query.py.bak\n@@ -0,0 +1,1922 @@\n+\"\"\"\n+The main QuerySet implementation. This provides the public API for the ORM.\n+\"\"\"\n+\n+import copy\n+import operator\n+import warnings\n+from collections import namedtuple\n+from functools import lru_cache\n+from itertools import chain\n+\n+from django.conf import settings\n+from django.core import exceptions\n+from django.db import (\n+    DJANGO_VERSION_PICKLE_KEY, IntegrityError, connections, router,\n+    transaction,\n+)\n+from django.db.models import DateField, DateTimeField, sql\n+from django.db.models.constants import LOOKUP_SEP\n+from django.db.models.deletion import Collector\n+from django.db.models.expressions import Case, Expression, F, Value, When\n+from django.db.models.fields import AutoField\n+from django.db.models.functions import Cast, Trunc\n+from django.db.models.query_utils import FilteredRelation, InvalidQuery, Q\n+from django.db.models.sql.constants import CURSOR, GET_ITERATOR_CHUNK_SIZE\n+from django.db.utils import NotSupportedError\n+from django.utils import timezone\n+from django.utils.functional import cached_property, partition\n+from django.utils.version import get_version\n+\n+# The maximum number of results to fetch in a get() query.\n+MAX_GET_RESULTS = 21\n+\n+# The maximum number of items to display in a QuerySet.__repr__\n+REPR_OUTPUT_SIZE = 20\n+\n+\n+class BaseIterable:\n+    def __init__(self, queryset, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):\n+        self.queryset = queryset\n+        self.chunked_fetch = chunked_fetch\n+        self.chunk_size = chunk_size\n+\n+\n+class ModelIterable(BaseIterable):\n+    \"\"\"Iterable that yields a model instance for each row.\"\"\"\n+\n+    def __iter__(self):\n+        queryset = self.queryset\n+        db = queryset.db\n+        compiler = queryset.query.get_compiler(using=db)\n+        # Execute the query. This will also fill compiler.select, klass_info,\n+        # and annotations.\n+        results = compiler.execute_sql(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)\n+        select, klass_info, annotation_col_map = (compiler.select, compiler.klass_info,\n+                                                  compiler.annotation_col_map)\n+        model_cls = klass_info['model']\n+        select_fields = klass_info['select_fields']\n+        model_fields_start, model_fields_end = select_fields[0], select_fields[-1] + 1\n+        init_list = [f[0].target.attname\n+                     for f in select[model_fields_start:model_fields_end]]\n+        related_populators = get_related_populators(klass_info, select, db)\n+        known_related_objects = [\n+            (field, related_objs, operator.attrgetter(*[\n+                field.attname\n+                if from_field == 'self' else\n+                queryset.model._meta.get_field(from_field).attname\n+                for from_field in field.from_fields\n+            ])) for field, related_objs in queryset._known_related_objects.items()\n+        ]\n+        for row in compiler.results_iter(results):\n+            obj = model_cls.from_db(db, init_list, row[model_fields_start:model_fields_end])\n+            for rel_populator in related_populators:\n+                rel_populator.populate(row, obj)\n+            if annotation_col_map:\n+                for attr_name, col_pos in annotation_col_map.items():\n+                    setattr(obj, attr_name, row[col_pos])\n+\n+            # Add the known related objects to the model.\n+            for field, rel_objs, rel_getter in known_related_objects:\n+                # Avoid overwriting objects loaded by, e.g., select_related().\n+                if field.is_cached(obj):\n+                    continue\n+                rel_obj_id = rel_getter(obj)\n+                try:\n+                    rel_obj = rel_objs[rel_obj_id]\n+                except KeyError:\n+                    pass  # May happen in qs1 | qs2 scenarios.\n+                else:\n+                    setattr(obj, field.name, rel_obj)\n+\n+            yield obj\n+\n+\n+class ValuesIterable(BaseIterable):\n+    \"\"\"\n+    Iterable returned by QuerySet.values() that yields a dict for each row.\n+    \"\"\"\n+\n+    def __iter__(self):\n+        queryset = self.queryset\n+        query = queryset.query\n+        compiler = query.get_compiler(queryset.db)\n+\n+        # extra(select=...) cols are always at the start of the row.\n+        names = [\n+            *query.extra_select,\n+            *query.values_select,\n+            *query.annotation_select,\n+        ]\n+        indexes = range(len(names))\n+        for row in compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size):\n+            yield {names[i]: row[i] for i in indexes}\n+\n+\n+class ValuesListIterable(BaseIterable):\n+    \"\"\"\n+    Iterable returned by QuerySet.values_list(flat=False) that yields a tuple\n+    for each row.\n+    \"\"\"\n+\n+    def __iter__(self):\n+        queryset = self.queryset\n+        query = queryset.query\n+        compiler = query.get_compiler(queryset.db)\n+\n+        if queryset._fields:\n+            # extra(select=...) cols are always at the start of the row.\n+            names = [\n+                *query.extra_select,\n+                *query.values_select,\n+                *query.annotation_select,\n+            ]\n+            fields = [*queryset._fields, *(f for f in query.annotation_select if f not in queryset._fields)]\n+            if fields != names:\n+                # Reorder according to fields.\n+                index_map = {name: idx for idx, name in enumerate(names)}\n+                rowfactory = operator.itemgetter(*[index_map[f] for f in fields])\n+                return map(\n+                    rowfactory,\n+                    compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)\n+                )\n+        return compiler.results_iter(tuple_expected=True, chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)\n+\n+\n+class NamedValuesListIterable(ValuesListIterable):\n+    \"\"\"\n+    Iterable returned by QuerySet.values_list(named=True) that yields a\n+    namedtuple for each row.\n+    \"\"\"\n+\n+    @staticmethod\n+    @lru_cache()\n+    def create_namedtuple_class(*names):\n+        # Cache namedtuple() with @lru_cache() since it's too slow to be\n+        # called for every QuerySet evaluation.\n+        return namedtuple('Row', names)\n+\n+    def __iter__(self):\n+        queryset = self.queryset\n+        if queryset._fields:\n+            names = queryset._fields\n+        else:\n+            query = queryset.query\n+            names = [*query.extra_select, *query.values_select, *query.annotation_select]\n+        tuple_class = self.create_namedtuple_class(*names)\n+        new = tuple.__new__\n+        for row in super().__iter__():\n+            yield new(tuple_class, row)\n+\n+\n+class FlatValuesListIterable(BaseIterable):\n+    \"\"\"\n+    Iterable returned by QuerySet.values_list(flat=True) that yields single\n+    values.\n+    \"\"\"\n+\n+    def __iter__(self):\n+        queryset = self.queryset\n+        compiler = queryset.query.get_compiler(queryset.db)\n+        for row in compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size):\n+            yield row[0]\n+\n+\n+class QuerySet:\n+    \"\"\"Represent a lazy database lookup for a set of objects.\"\"\"\n+\n+    def __init__(self, model=None, query=None, using=None, hints=None):\n+        self.model = model\n+        self._db = using\n+        self._hints = hints or {}\n+        self.query = query or sql.Query(self.model)\n+        self._result_cache = None\n+        self._sticky_filter = False\n+        self._for_write = False\n+        self._prefetch_related_lookups = ()\n+        self._prefetch_done = False\n+        self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}\n+        self._iterable_class = ModelIterable\n+        self._fields = None\n+\n+    def as_manager(cls):\n+        # Address the circular dependency between `Queryset` and `Manager`.\n+        from django.db.models.manager import Manager\n+        manager = Manager.from_queryset(cls)()\n+        manager._built_with_as_manager = True\n+        return manager\n+    as_manager.queryset_only = True\n+    as_manager = classmethod(as_manager)\n+\n+    ########################\n+    # PYTHON MAGIC METHODS #\n+    ########################\n+\n+    def __deepcopy__(self, memo):\n+        \"\"\"Don't populate the QuerySet's cache.\"\"\"\n+        obj = self.__class__()\n+        for k, v in self.__dict__.items():\n+            if k == '_result_cache':\n+                obj.__dict__[k] = None\n+            else:\n+                obj.__dict__[k] = copy.deepcopy(v, memo)\n+        return obj\n+\n+    def __getstate__(self):\n+        # Force the cache to be fully populated.\n+        self._fetch_all()\n+        return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}\n+\n+    def __setstate__(self, state):\n+        msg = None\n+        pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)\n+        if pickled_version:\n+            current_version = get_version()\n+            if current_version != pickled_version:\n+                msg = (\n+                    \"Pickled queryset instance's Django version %s does not \"\n+                    \"match the current version %s.\" % (pickled_version, current_version)\n+                )\n+        else:\n+            msg = \"Pickled queryset instance's Django version is not specified.\"\n+\n+        if msg:\n+            warnings.warn(msg, RuntimeWarning, stacklevel=2)\n+\n+        self.__dict__.update(state)\n+\n+    def __repr__(self):\n+        data = list(self[:REPR_OUTPUT_SIZE + 1])\n+        if len(data) > REPR_OUTPUT_SIZE:\n+            data[-1] = \"...(remaining elements truncated)...\"\n+        return '<%s %r>' % (self.__class__.__name__, data)\n+\n+    def __len__(self):\n+        self._fetch_all()\n+        return len(self._result_cache)\n+\n+    def __iter__(self):\n+        \"\"\"\n+        The queryset iterator protocol uses three nested iterators in the\n+        default case:\n+            1. sql.compiler.execute_sql()\n+               - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)\n+                 using cursor.fetchmany(). This part is responsible for\n+                 doing some column masking, and returning the rows in chunks.\n+            2. sql.compiler.results_iter()\n+               - Returns one row at time. At this point the rows are still just\n+                 tuples. In some cases the return values are converted to\n+                 Python values at this location.\n+            3. self.iterator()\n+               - Responsible for turning the rows into model objects.\n+        \"\"\"\n+        self._fetch_all()\n+        return iter(self._result_cache)\n+\n+    def __bool__(self):\n+        self._fetch_all()\n+        return bool(self._result_cache)\n+\n+    def __getitem__(self, k):\n+        \"\"\"Retrieve an item or slice from the set of results.\"\"\"\n+        if not isinstance(k, (int, slice)):\n+            raise TypeError(\n+                'QuerySet indices must be integers or slices, not %s.'\n+                % type(k).__name__\n+            )\n+        assert ((not isinstance(k, slice) and (k >= 0)) or\n+                (isinstance(k, slice) and (k.start is None or k.start >= 0) and\n+                 (k.stop is None or k.stop >= 0))), \\\n+            \"Negative indexing is not supported.\"\n+\n+        if self._result_cache is not None:\n+            return self._result_cache[k]\n+\n+        if isinstance(k, slice):\n+            qs = self._chain()\n+            if k.start is not None:\n+                start = int(k.start)\n+            else:\n+                start = None\n+            if k.stop is not None:\n+                stop = int(k.stop)\n+            else:\n+                stop = None\n+            qs.query.set_limits(start, stop)\n+            return list(qs)[::k.step] if k.step else qs\n+\n+        qs = self._chain()\n+        qs.query.set_limits(k, k + 1)\n+        qs._fetch_all()\n+        return qs._result_cache[0]\n+\n+    def __and__(self, other):\n+        self._merge_sanity_check(other)\n+        if isinstance(other, EmptyQuerySet):\n+            return other\n+        if isinstance(self, EmptyQuerySet):\n+            return self\n+        combined = self._chain()\n+        combined._merge_known_related_objects(other)\n+        combined.query.combine(other.query, sql.AND)\n+        return combined\n+\n+    def __or__(self, other):\n+        self._merge_sanity_check(other)\n+        if isinstance(self, EmptyQuerySet):\n+            return other\n+        if isinstance(other, EmptyQuerySet):\n+            return self\n+        query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))\n+        combined = query._chain()\n+        combined._merge_known_related_objects(other)\n+        if not other.query.can_filter():\n+            other = other.model._base_manager.filter(pk__in=other.values('pk'))\n+        combined.query.combine(other.query, sql.OR)\n+        return combined\n+\n+    ####################################\n+    # METHODS THAT DO DATABASE QUERIES #\n+    ####################################\n+\n+    def _iterator(self, use_chunked_fetch, chunk_size):\n+        yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)\n+\n+    def iterator(self, chunk_size=2000):\n+        \"\"\"\n+        An iterator over the results from applying this QuerySet to the\n+        database.\n+        \"\"\"\n+        if chunk_size <= 0:\n+            raise ValueError('Chunk size must be strictly positive.')\n+        use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')\n+        return self._iterator(use_chunked_fetch, chunk_size)\n+\n+    def aggregate(self, *args, **kwargs):\n+        \"\"\"\n+        Return a dictionary containing the calculations (aggregation)\n+        over the current queryset.\n+\n+        If args is present the expression is passed as a kwarg using\n+        the Aggregate object's default alias.\n+        \"\"\"\n+        if self.query.distinct_fields:\n+            raise NotImplementedError(\"aggregate() + distinct(fields) not implemented.\")\n+        self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')\n+        for arg in args:\n+            # The default_alias property raises TypeError if default_alias\n+            # can't be set automatically or AttributeError if it isn't an\n+            # attribute.\n+            try:\n+                arg.default_alias\n+            except (AttributeError, TypeError):\n+                raise TypeError(\"Complex aggregates require an alias\")\n+            kwargs[arg.default_alias] = arg\n+\n+        query = self.query.chain()\n+        for (alias, aggregate_expr) in kwargs.items():\n+            query.add_annotation(aggregate_expr, alias, is_summary=True)\n+            if not query.annotations[alias].contains_aggregate:\n+                raise TypeError(\"%s is not an aggregate expression\" % alias)\n+        return query.get_aggregation(self.db, kwargs)\n+\n+    def count(self):\n+        \"\"\"\n+        Perform a SELECT COUNT() and return the number of records as an\n+        integer.\n+\n+        If the QuerySet is already fully cached, return the length of the\n+        cached results set to avoid multiple SELECT COUNT(*) calls.\n+        \"\"\"\n+        if self._result_cache is not None:\n+            return len(self._result_cache)\n+\n+        return self.query.get_count(using=self.db)\n+\n+    def get(self, *args, **kwargs):\n+        \"\"\"\n+        Perform the query and return a single object matching the given\n+        keyword arguments.\n+        \"\"\"\n+        clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)\n+        if self.query.can_filter() and not self.query.distinct_fields:\n+            clone = clone.order_by()\n+        limit = None\n+        if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:\n+            limit = MAX_GET_RESULTS\n+            clone.query.set_limits(high=limit)\n+        num = len(clone)\n+        if num == 1:\n+            return clone._result_cache[0]\n+        if not num:\n+            raise self.model.DoesNotExist(\n+                \"%s matching query does not exist.\" %\n+                self.model._meta.object_name\n+            )\n+        raise self.model.MultipleObjectsReturned(\n+            'get() returned more than one %s -- it returned %s!' % (\n+                self.model._meta.object_name,\n+                num if not limit or num < limit else 'more than %s' % (limit - 1),\n+            )\n+        )\n+\n+    def create(self, **kwargs):\n+        \"\"\"\n+        Create a new object with the given kwargs, saving it to the database\n+        and returning the created object.\n+        \"\"\"\n+        obj = self.model(**kwargs)\n+        self._for_write = True\n+        obj.save(force_insert=True, using=self.db)\n+        return obj\n+\n+    def _populate_pk_values(self, objs):\n+        for obj in objs:\n+            if obj.pk is None:\n+                obj.pk = obj._meta.pk.get_pk_value_on_save(obj)\n+\n+    def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):\n+        \"\"\"\n+        Insert each of the instances into the database. Do *not* call\n+        save() on each of the instances, do not send any pre/post_save\n+        signals, and do not set the primary key attribute if it is an\n+        autoincrement field (except if features.can_return_rows_from_bulk_insert=True).\n+        Multi-table models are not supported.\n+        \"\"\"\n+        # When you bulk insert you don't get the primary keys back (if it's an\n+        # autoincrement, except if can_return_rows_from_bulk_insert=True), so\n+        # you can't insert into the child tables which references this. There\n+        # are two workarounds:\n+        # 1) This could be implemented if you didn't have an autoincrement pk\n+        # 2) You could do it by doing O(n) normal inserts into the parent\n+        #    tables to get the primary keys back and then doing a single bulk\n+        #    insert into the childmost table.\n+        # We currently set the primary keys on the objects when using\n+        # PostgreSQL via the RETURNING ID clause. It should be possible for\n+        # Oracle as well, but the semantics for extracting the primary keys is\n+        # trickier so it's not done yet.\n+        assert batch_size is None or batch_size > 0\n+        # Check that the parents share the same concrete model with the our\n+        # model to detect the inheritance pattern ConcreteGrandParent ->\n+        # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy\n+        # would not identify that case as involving multiple tables.\n+        for parent in self.model._meta.get_parent_list():\n+            if parent._meta.concrete_model is not self.model._meta.concrete_model:\n+                raise ValueError(\"Can't bulk create a multi-table inherited model\")\n+        if not objs:\n+            return objs\n+        self._for_write = True\n+        connection = connections[self.db]\n+        opts = self.model._meta\n+        fields = opts.concrete_fields\n+        objs = list(objs)\n+        self._populate_pk_values(objs)\n+        with transaction.atomic(using=self.db, savepoint=False):\n+            objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)\n+            if objs_with_pk:\n+                returned_columns = self._batched_insert(\n+                    objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,\n+                )\n+                for obj_with_pk, results in zip(objs_with_pk, returned_columns):\n+                    for result, field in zip(results, opts.db_returning_fields):\n+                        if field != opts.pk:\n+                            setattr(obj_with_pk, field.attname, result)\n+                for obj_with_pk in objs_with_pk:\n+                    obj_with_pk._state.adding = False\n+                    obj_with_pk._state.db = self.db\n+            if objs_without_pk:\n+                fields = [f for f in fields if not isinstance(f, AutoField)]\n+                returned_columns = self._batched_insert(\n+                    objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,\n+                )\n+                if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:\n+                    assert len(returned_columns) == len(objs_without_pk)\n+                for obj_without_pk, results in zip(objs_without_pk, returned_columns):\n+                    for result, field in zip(results, opts.db_returning_fields):\n+                        setattr(obj_without_pk, field.attname, result)\n+                    obj_without_pk._state.adding = False\n+                    obj_without_pk._state.db = self.db\n+\n+        return objs\n+\n+    def bulk_update(self, objs, fields, batch_size=None):\n+        \"\"\"\n+        Update the given fields in each of the given objects in the database.\n+        \"\"\"\n+        if batch_size is not None and batch_size < 0:\n+            raise ValueError('Batch size must be a positive integer.')\n+        if not fields:\n+            raise ValueError('Field names must be given to bulk_update().')\n+        objs = tuple(objs)\n+        if any(obj.pk is None for obj in objs):\n+            raise ValueError('All bulk_update() objects must have a primary key set.')\n+        fields = [self.model._meta.get_field(name) for name in fields]\n+        if any(not f.concrete or f.many_to_many for f in fields):\n+            raise ValueError('bulk_update() can only be used with concrete fields.')\n+        if any(f.primary_key for f in fields):\n+            raise ValueError('bulk_update() cannot be used with primary key fields.')\n+        if not objs:\n+            return\n+        # PK is used twice in the resulting update query, once in the filter\n+        # and once in the WHEN. Each field will also have one CAST.\n+        max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)\n+        batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n+        requires_casting = connections[self.db].features.requires_casted_case_in_updates\n+        batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))\n+        updates = []\n+        for batch_objs in batches:\n+            update_kwargs = {}\n+            for field in fields:\n+                when_statements = []\n+                for obj in batch_objs:\n+                    attr = getattr(obj, field.attname)\n+                    if not isinstance(attr, Expression):\n+                        attr = Value(attr, output_field=field)\n+                    when_statements.append(When(pk=obj.pk, then=attr))\n+                case_statement = Case(*when_statements, output_field=field)\n+                if requires_casting:\n+                    case_statement = Cast(case_statement, output_field=field)\n+                update_kwargs[field.attname] = case_statement\n+            updates.append(([obj.pk for obj in batch_objs], update_kwargs))\n+        with transaction.atomic(using=self.db, savepoint=False):\n+            for pks, update_kwargs in updates:\n+                self.filter(pk__in=pks).update(**update_kwargs)\n+    bulk_update.alters_data = True\n+\n+    def get_or_create(self, defaults=None, **kwargs):\n+        \"\"\"\n+        Look up an object with the given kwargs, creating one if necessary.\n+        Return a tuple of (object, created), where created is a boolean\n+        specifying whether an object was created.\n+        \"\"\"\n+        # The get() needs to be targeted at the write database in order\n+        # to avoid potential transaction consistency problems.\n+        self._for_write = True\n+        try:\n+            return self.get(**kwargs), False\n+        except self.model.DoesNotExist:\n+            params = self._extract_model_params(defaults, **kwargs)\n+            return self._create_object_from_params(kwargs, params)\n+\n+    def update_or_create(self, defaults=None, **kwargs):\n+        \"\"\"\n+        Look up an object with the given kwargs, updating one with defaults\n+        if it exists, otherwise create a new one.\n+        Return a tuple (object, created), where created is a boolean\n+        specifying whether an object was created.\n+        \"\"\"\n+        defaults = defaults or {}\n+        self._for_write = True\n+        with transaction.atomic(using=self.db):\n+            try:\n+                obj = self.select_for_update().get(**kwargs)\n+            except self.model.DoesNotExist:\n+                params = self._extract_model_params(defaults, **kwargs)\n+                # Lock the row so that a concurrent update is blocked until\n+                # after update_or_create() has performed its save.\n+                obj, created = self._create_object_from_params(kwargs, params, lock=True)\n+                if created:\n+                    return obj, created\n+            for k, v in defaults.items():\n+                setattr(obj, k, v() if callable(v) else v)\n+            obj.save(using=self.db)\n+        return obj, False\n+\n+    def _create_object_from_params(self, lookup, params, lock=False):\n+        \"\"\"\n+        Try to create an object using passed params. Used by get_or_create()\n+        and update_or_create().\n+        \"\"\"\n+        try:\n+            with transaction.atomic(using=self.db):\n+                params = {k: v() if callable(v) else v for k, v in params.items()}\n+                obj = self.create(**params)\n+            return obj, True\n+        except IntegrityError as e:\n+            try:\n+                qs = self.select_for_update() if lock else self\n+                return qs.get(**lookup), False\n+            except self.model.DoesNotExist:\n+                pass\n+            raise e\n+\n+    def _extract_model_params(self, defaults, **kwargs):\n+        \"\"\"\n+        Prepare `params` for creating a model instance based on the given\n+        kwargs; for use by get_or_create() and update_or_create().\n+        \"\"\"\n+        defaults = defaults or {}\n+        params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}\n+        params.update(defaults)\n+        property_names = self.model._meta._property_names\n+        invalid_params = []\n+        for param in params:\n+            try:\n+                self.model._meta.get_field(param)\n+            except exceptions.FieldDoesNotExist:\n+                # It's okay to use a model's property if it has a setter.\n+                if not (param in property_names and getattr(self.model, param).fset):\n+                    invalid_params.append(param)\n+        if invalid_params:\n+            raise exceptions.FieldError(\n+                \"Invalid field name(s) for model %s: '%s'.\" % (\n+                    self.model._meta.object_name,\n+                    \"', '\".join(sorted(invalid_params)),\n+                ))\n+        return params\n+\n+    def _earliest(self, *fields):\n+        \"\"\"\n+        Return the earliest object according to fields (if given) or by the\n+        model's Meta.get_latest_by.\n+        \"\"\"\n+        if fields:\n+            order_by = fields\n+        else:\n+            order_by = getattr(self.model._meta, 'get_latest_by')\n+            if order_by and not isinstance(order_by, (tuple, list)):\n+                order_by = (order_by,)\n+        if order_by is None:\n+            raise ValueError(\n+                \"earliest() and latest() require either fields as positional \"\n+                \"arguments or 'get_latest_by' in the model's Meta.\"\n+            )\n+\n+        assert not self.query.is_sliced, \\\n+            \"Cannot change a query once a slice has been taken.\"\n+        obj = self._chain()\n+        obj.query.set_limits(high=1)\n+        obj.query.clear_ordering(force_empty=True)\n+        obj.query.add_ordering(*order_by)\n+        return obj.get()\n+\n+    def earliest(self, *fields):\n+        return self._earliest(*fields)\n+\n+    def latest(self, *fields):\n+        return self.reverse()._earliest(*fields)\n+\n+    def first(self):\n+        \"\"\"Return the first object of a query or None if no match is found.\"\"\"\n+        for obj in (self if self.ordered else self.order_by('pk'))[:1]:\n+            return obj\n+\n+    def last(self):\n+        \"\"\"Return the last object of a query or None if no match is found.\"\"\"\n+        for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:\n+            return obj\n+\n+    def in_bulk(self, id_list=None, *, field_name='pk'):\n+        \"\"\"\n+        Return a dictionary mapping each of the given IDs to the object with\n+        that ID. If `id_list` isn't provided, evaluate the entire QuerySet.\n+        \"\"\"\n+        assert not self.query.is_sliced, \\\n+            \"Cannot use 'limit' or 'offset' with in_bulk\"\n+        if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:\n+            raise ValueError(\"in_bulk()'s field_name must be a unique field but %r isn't.\" % field_name)\n+        if id_list is not None:\n+            if not id_list:\n+                return {}\n+            filter_key = '{}__in'.format(field_name)\n+            batch_size = connections[self.db].features.max_query_params\n+            id_list = tuple(id_list)\n+            # If the database has a limit on the number of query parameters\n+            # (e.g. SQLite), retrieve objects in batches if necessary.\n+            if batch_size and batch_size < len(id_list):\n+                qs = ()\n+                for offset in range(0, len(id_list), batch_size):\n+                    batch = id_list[offset:offset + batch_size]\n+                    qs += tuple(self.filter(**{filter_key: batch}).order_by())\n+            else:\n+                qs = self.filter(**{filter_key: id_list}).order_by()\n+        else:\n+            qs = self._chain()\n+        return {getattr(obj, field_name): obj for obj in qs}\n+\n+    def delete(self):\n+        \"\"\"Delete the records in the current QuerySet.\"\"\"\n+        assert not self.query.is_sliced, \\\n+            \"Cannot use 'limit' or 'offset' with delete.\"\n+\n+        if self._fields is not None:\n+            raise TypeError(\"Cannot call delete() after .values() or .values_list()\")\n+\n+        del_query = self._chain()\n+\n+        # The delete is actually 2 queries - one to find related objects,\n+        # and one to delete. Make sure that the discovery of related\n+        # objects is performed on the same database as the deletion.\n+        del_query._for_write = True\n+\n+        # Disable non-supported fields.\n+        del_query.query.select_for_update = False\n+        del_query.query.select_related = False\n+        del_query.query.clear_ordering(force_empty=True)\n+\n+        collector = Collector(using=del_query.db)\n+        collector.collect(del_query)\n+        deleted, _rows_count = collector.delete()\n+\n+        # Clear the result cache, in case this QuerySet gets reused.\n+        self._result_cache = None\n+        return deleted, _rows_count\n+\n+    delete.alters_data = True\n+    delete.queryset_only = True\n+\n+    def _raw_delete(self, using):\n+        \"\"\"\n+        Delete objects found from the given queryset in single direct SQL\n+        query. No signals are sent and there is no protection for cascades.\n+        \"\"\"\n+        return sql.DeleteQuery(self.model).delete_qs(self, using)\n+    _raw_delete.alters_data = True\n+\n+    def update(self, **kwargs):\n+        \"\"\"\n+        Update all elements in the current QuerySet, setting all the given\n+        fields to the appropriate values.\n+        \"\"\"\n+        assert not self.query.is_sliced, \\\n+            \"Cannot update a query once a slice has been taken.\"\n+        self._for_write = True\n+        query = self.query.chain(sql.UpdateQuery)\n+        query.add_update_values(kwargs)\n+        # Clear any annotations so that they won't be present in subqueries.\n+        query.annotations = {}\n+        with transaction.mark_for_rollback_on_error(using=self.db):\n+            rows = query.get_compiler(self.db).execute_sql(CURSOR)\n+        self._result_cache = None\n+        return rows\n+    update.alters_data = True\n+\n+    def _update(self, values):\n+        \"\"\"\n+        A version of update() that accepts field objects instead of field names.\n+        Used primarily for model saving and not intended for use by general\n+        code (it requires too much poking around at model internals to be\n+        useful at that level).\n+        \"\"\"\n+        assert not self.query.is_sliced, \\\n+            \"Cannot update a query once a slice has been taken.\"\n+        query = self.query.chain(sql.UpdateQuery)\n+        query.add_update_fields(values)\n+        # Clear any annotations so that they won't be present in subqueries.\n+        query.annotations = {}\n+        self._result_cache = None\n+        return query.get_compiler(self.db).execute_sql(CURSOR)\n+    _update.alters_data = True\n+    _update.queryset_only = False\n+\n+    def exists(self):\n+        if self._result_cache is None:\n+            return self.query.has_results(using=self.db)\n+        return bool(self._result_cache)\n+\n+    def _prefetch_related_objects(self):\n+        # This method can only be called once the result cache has been filled.\n+        prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)\n+        self._prefetch_done = True\n+\n+    def explain(self, *, format=None, **options):\n+        return self.query.explain(using=self.db, format=format, **options)\n+\n+    ##################################################\n+    # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #\n+    ##################################################\n+\n+    def raw(self, raw_query, params=None, translations=None, using=None):\n+        if using is None:\n+            using = self.db\n+        qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)\n+        qs._prefetch_related_lookups = self._prefetch_related_lookups[:]\n+        return qs\n+\n+    def _values(self, *fields, **expressions):\n+        clone = self._chain()\n+        if expressions:\n+            clone = clone.annotate(**expressions)\n+        clone._fields = fields\n+        clone.query.set_values(fields)\n+        return clone\n+\n+    def values(self, *fields, **expressions):\n+        fields += tuple(expressions)\n+        clone = self._values(*fields, **expressions)\n+        clone._iterable_class = ValuesIterable\n+        return clone\n+\n+    def values_list(self, *fields, flat=False, named=False):\n+        if flat and named:\n+            raise TypeError(\"'flat' and 'named' can't be used together.\")\n+        if flat and len(fields) > 1:\n+            raise TypeError(\"'flat' is not valid when values_list is called with more than one field.\")\n+\n+        field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}\n+        _fields = []\n+        expressions = {}\n+        counter = 1\n+        for field in fields:\n+            if hasattr(field, 'resolve_expression'):\n+                field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())\n+                while True:\n+                    field_id = field_id_prefix + str(counter)\n+                    counter += 1\n+                    if field_id not in field_names:\n+                        break\n+                expressions[field_id] = field\n+                _fields.append(field_id)\n+            else:\n+                _fields.append(field)\n+\n+        clone = self._values(*_fields, **expressions)\n+        clone._iterable_class = (\n+            NamedValuesListIterable if named\n+            else FlatValuesListIterable if flat\n+            else ValuesListIterable\n+        )\n+        return clone\n+\n+    def dates(self, field_name, kind, order='ASC'):\n+        \"\"\"\n+        Return a list of date objects representing all available dates for\n+        the given field_name, scoped to 'kind'.\n+        \"\"\"\n+        assert kind in ('year', 'month', 'week', 'day'), \\\n+            \"'kind' must be one of 'year', 'month', 'week', or 'day'.\"\n+        assert order in ('ASC', 'DESC'), \\\n+            \"'order' must be either 'ASC' or 'DESC'.\"\n+        return self.annotate(\n+            datefield=Trunc(field_name, kind, output_field=DateField()),\n+            plain_field=F(field_name)\n+        ).values_list(\n+            'datefield', flat=True\n+        ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')\n+\n+    def datetimes(self, field_name, kind, order='ASC', tzinfo=None):\n+        \"\"\"\n+        Return a list of datetime objects representing all available\n+        datetimes for the given field_name, scoped to 'kind'.\n+        \"\"\"\n+        assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \\\n+            \"'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'.\"\n+        assert order in ('ASC', 'DESC'), \\\n+            \"'order' must be either 'ASC' or 'DESC'.\"\n+        if settings.USE_TZ:\n+            if tzinfo is None:\n+                tzinfo = timezone.get_current_timezone()\n+        else:\n+            tzinfo = None\n+        return self.annotate(\n+            datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),\n+            plain_field=F(field_name)\n+        ).values_list(\n+            'datetimefield', flat=True\n+        ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')\n+\n+    def none(self):\n+        \"\"\"Return an empty QuerySet.\"\"\"\n+        clone = self._chain()\n+        clone.query.set_empty()\n+        return clone\n+\n+    ##################################################################\n+    # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #\n+    ##################################################################\n+\n+    def all(self):\n+        \"\"\"\n+        Return a new QuerySet that is a copy of the current one. This allows a\n+        QuerySet to proxy for a model manager in some cases.\n+        \"\"\"\n+        return self._chain()\n+\n+    def filter(self, *args, **kwargs):\n+        \"\"\"\n+        Return a new QuerySet instance with the args ANDed to the existing\n+        set.\n+        \"\"\"\n+        self._not_support_combined_queries('filter')\n+        return self._filter_or_exclude(False, *args, **kwargs)\n+\n+    def exclude(self, *args, **kwargs):\n+        \"\"\"\n+        Return a new QuerySet instance with NOT (args) ANDed to the existing\n+        set.\n+        \"\"\"\n+        self._not_support_combined_queries('exclude')\n+        return self._filter_or_exclude(True, *args, **kwargs)\n+\n+    def _filter_or_exclude(self, negate, *args, **kwargs):\n+        if args or kwargs:\n+            assert not self.query.is_sliced, \\\n+                \"Cannot filter a query once a slice has been taken.\"\n+\n+        clone = self._chain()\n+        if negate:\n+            clone.query.add_q(~Q(*args, **kwargs))\n+        else:\n+            clone.query.add_q(Q(*args, **kwargs))\n+        return clone\n+\n+    def complex_filter(self, filter_obj):\n+        \"\"\"\n+        Return a new QuerySet instance with filter_obj added to the filters.\n+\n+        filter_obj can be a Q object or a dictionary of keyword lookup\n+        arguments.\n+\n+        This exists to support framework features such as 'limit_choices_to',\n+        and usually it will be more natural to use other methods.\n+        \"\"\"\n+        if isinstance(filter_obj, Q):\n+            clone = self._chain()\n+            clone.query.add_q(filter_obj)\n+            return clone\n+        else:\n+            return self._filter_or_exclude(None, **filter_obj)\n+\n+    def _combinator_query(self, combinator, *other_qs, all=False):\n+        # Clone the query to inherit the select list and everything\n+        clone = self._chain()\n+        # Clear limits and ordering so they can be reapplied\n+        clone.query.clear_ordering(True)\n+        clone.query.clear_limits()\n+        clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)\n+        clone.query.combinator = combinator\n+        clone.query.combinator_all = all\n+        return clone\n+\n+    def union(self, *other_qs, all=False):\n+        # If the query is an EmptyQuerySet, combine all nonempty querysets.\n+        if isinstance(self, EmptyQuerySet):\n+            qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]\n+            return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self\n+        return self._combinator_query('union', *other_qs, all=all)\n+\n+    def intersection(self, *other_qs):\n+        # If any query is an EmptyQuerySet, return it.\n+        if isinstance(self, EmptyQuerySet):\n+            return self\n+        for other in other_qs:\n+            if isinstance(other, EmptyQuerySet):\n+                return other\n+        return self._combinator_query('intersection', *other_qs)\n+\n+    def difference(self, *other_qs):\n+        # If the query is an EmptyQuerySet, return it.\n+        if isinstance(self, EmptyQuerySet):\n+            return self\n+        return self._combinator_query('difference', *other_qs)\n+\n+    def select_for_update(self, nowait=False, skip_locked=False, of=()):\n+        \"\"\"\n+        Return a new QuerySet instance that will select objects with a\n+        FOR UPDATE lock.\n+        \"\"\"\n+        if nowait and skip_locked:\n+            raise ValueError('The nowait option cannot be used with skip_locked.')\n+        obj = self._chain()\n+        obj._for_write = True\n+        obj.query.select_for_update = True\n+        obj.query.select_for_update_nowait = nowait\n+        obj.query.select_for_update_skip_locked = skip_locked\n+        obj.query.select_for_update_of = of\n+        return obj\n+\n+    def select_related(self, *fields):\n+        \"\"\"\n+        Return a new QuerySet instance that will select related objects.\n+\n+        If fields are specified, they must be ForeignKey fields and only those\n+        related objects are included in the selection.\n+\n+        If select_related(None) is called, clear the list.\n+        \"\"\"\n+        self._not_support_combined_queries('select_related')\n+        if self._fields is not None:\n+            raise TypeError(\"Cannot call select_related() after .values() or .values_list()\")\n+\n+        obj = self._chain()\n+        if fields == (None,):\n+            obj.query.select_related = False\n+        elif fields:\n+            obj.query.add_select_related(fields)\n+        else:\n+            obj.query.select_related = True\n+        return obj\n+\n+    def prefetch_related(self, *lookups):\n+        \"\"\"\n+        Return a new QuerySet instance that will prefetch the specified\n+        Many-To-One and Many-To-Many related objects when the QuerySet is\n+        evaluated.\n+\n+        When prefetch_related() is called more than once, append to the list of\n+        prefetch lookups. If prefetch_related(None) is called, clear the list.\n+        \"\"\"\n+        self._not_support_combined_queries('prefetch_related')\n+        clone = self._chain()\n+        if lookups == (None,):\n+            clone._prefetch_related_lookups = ()\n+        else:\n+            for lookup in lookups:\n+                if isinstance(lookup, Prefetch):\n+                    lookup = lookup.prefetch_to\n+                lookup = lookup.split(LOOKUP_SEP, 1)[0]\n+                if lookup in self.query._filtered_relations:\n+                    raise ValueError('prefetch_related() is not supported with FilteredRelation.')\n+            clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups\n+        return clone\n+\n+    def annotate(self, *args, **kwargs):\n+        \"\"\"\n+        Return a query set in which the returned objects have been annotated\n+        with extra data or aggregations.\n+        \"\"\"\n+        self._not_support_combined_queries('annotate')\n+        self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')\n+        annotations = {}\n+        for arg in args:\n+            # The default_alias property may raise a TypeError.\n+            try:\n+                if arg.default_alias in kwargs:\n+                    raise ValueError(\"The named annotation '%s' conflicts with the \"\n+                                     \"default name for another annotation.\"\n+                                     % arg.default_alias)\n+            except TypeError:\n+                raise TypeError(\"Complex annotations require an alias\")\n+            annotations[arg.default_alias] = arg\n+        annotations.update(kwargs)\n+\n+        clone = self._chain()\n+        names = self._fields\n+        if names is None:\n+            names = set(chain.from_iterable(\n+                (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)\n+                for field in self.model._meta.get_fields()\n+            ))\n+\n+        for alias, annotation in annotations.items():\n+            if alias in names:\n+                raise ValueError(\"The annotation '%s' conflicts with a field on \"\n+                                 \"the model.\" % alias)\n+            if isinstance(annotation, FilteredRelation):\n+                clone.query.add_filtered_relation(annotation, alias)\n+            else:\n+                clone.query.add_annotation(annotation, alias, is_summary=False)\n+\n+        for alias, annotation in clone.query.annotations.items():\n+            if alias in annotations and annotation.contains_aggregate:\n+                if clone._fields is None:\n+                    clone.query.group_by = True\n+                else:\n+                    clone.query.set_group_by()\n+                break\n+\n+        return clone\n+\n+    def order_by(self, *field_names):\n+        \"\"\"Return a new QuerySet instance with the ordering changed.\"\"\"\n+        assert not self.query.is_sliced, \\\n+            \"Cannot reorder a query once a slice has been taken.\"\n+        obj = self._chain()\n+        obj.query.clear_ordering(force_empty=False)\n+        obj.query.add_ordering(*field_names)\n+        return obj\n+\n+    def distinct(self, *field_names):\n+        \"\"\"\n+        Return a new QuerySet instance that will select only distinct results.\n+        \"\"\"\n+        assert not self.query.is_sliced, \\\n+            \"Cannot create distinct fields once a slice has been taken.\"\n+        obj = self._chain()\n+        obj.query.add_distinct_fields(*field_names)\n+        return obj\n+\n+    def extra(self, select=None, where=None, params=None, tables=None,\n+              order_by=None, select_params=None):\n+        \"\"\"Add extra SQL fragments to the query.\"\"\"\n+        self._not_support_combined_queries('extra')\n+        assert not self.query.is_sliced, \\\n+            \"Cannot change a query once a slice has been taken\"\n+        clone = self._chain()\n+        clone.query.add_extra(select, select_params, where, params, tables, order_by)\n+        return clone\n+\n+    def reverse(self):\n+        \"\"\"Reverse the ordering of the QuerySet.\"\"\"\n+        if self.query.is_sliced:\n+            raise TypeError('Cannot reverse a query once a slice has been taken.')\n+        clone = self._chain()\n+        clone.query.standard_ordering = not clone.query.standard_ordering\n+        return clone\n+\n+    def defer(self, *fields):\n+        \"\"\"\n+        Defer the loading of data for certain fields until they are accessed.\n+        Add the set of deferred fields to any existing set of deferred fields.\n+        The only exception to this is if None is passed in as the only\n+        parameter, in which case removal all deferrals.\n+        \"\"\"\n+        self._not_support_combined_queries('defer')\n+        if self._fields is not None:\n+            raise TypeError(\"Cannot call defer() after .values() or .values_list()\")\n+        clone = self._chain()\n+        if fields == (None,):\n+            clone.query.clear_deferred_loading()\n+        else:\n+            clone.query.add_deferred_loading(fields)\n+        return clone\n+\n+    def only(self, *fields):\n+        \"\"\"\n+        Essentially, the opposite of defer(). Only the fields passed into this\n+        method and that are not already specified as deferred are loaded\n+        immediately when the queryset is evaluated.\n+        \"\"\"\n+        self._not_support_combined_queries('only')\n+        if self._fields is not None:\n+            raise TypeError(\"Cannot call only() after .values() or .values_list()\")\n+        if fields == (None,):\n+            # Can only pass None to defer(), not only(), as the rest option.\n+            # That won't stop people trying to do this, so let's be explicit.\n+            raise TypeError(\"Cannot pass None as an argument to only().\")\n+        for field in fields:\n+            field = field.split(LOOKUP_SEP, 1)[0]\n+            if field in self.query._filtered_relations:\n+                raise ValueError('only() is not supported with FilteredRelation.')\n+        clone = self._chain()\n+        clone.query.add_immediate_loading(fields)\n+        return clone\n+\n+    def using(self, alias):\n+        \"\"\"Select which database this QuerySet should execute against.\"\"\"\n+        clone = self._chain()\n+        clone._db = alias\n+        return clone\n+\n+    ###################################\n+    # PUBLIC INTROSPECTION ATTRIBUTES #\n+    ###################################\n+\n+    @property\n+    def ordered(self):\n+        \"\"\"\n+        Return True if the QuerySet is ordered -- i.e. has an order_by()\n+        clause or a default ordering on the model (or is empty).\n+        \"\"\"\n+        if isinstance(self, EmptyQuerySet):\n+            return True\n+        if self.query.extra_order_by or self.query.order_by:\n+            return True\n+        elif self.query.default_ordering and self.query.get_meta().ordering:\n+            return True\n+        else:\n+            return False\n+\n+    @property\n+    def db(self):\n+        \"\"\"Return the database used if this query is executed now.\"\"\"\n+        if self._for_write:\n+            return self._db or router.db_for_write(self.model, **self._hints)\n+        return self._db or router.db_for_read(self.model, **self._hints)\n+\n+    ###################\n+    # PRIVATE METHODS #\n+    ###################\n+\n+    def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):\n+        \"\"\"\n+        Insert a new record for the given model. This provides an interface to\n+        the InsertQuery class and is how Model.save() is implemented.\n+        \"\"\"\n+        self._for_write = True\n+        if using is None:\n+            using = self.db\n+        query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)\n+        query.insert_values(fields, objs, raw=raw)\n+        return query.get_compiler(using=using).execute_sql(returning_fields)\n+    _insert.alters_data = True\n+    _insert.queryset_only = False\n+\n+    def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):\n+        \"\"\"\n+        Helper method for bulk_create() to insert objs one batch at a time.\n+        \"\"\"\n+        if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:\n+            raise NotSupportedError('This database backend does not support ignoring conflicts.')\n+        ops = connections[self.db].ops\n+        batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))\n+        inserted_rows = []\n+        bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert\n+        for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:\n+            if bulk_return and not ignore_conflicts:\n+                inserted_columns = self._insert(\n+                    item, fields=fields, using=self.db,\n+                    returning_fields=self.model._meta.db_returning_fields,\n+                    ignore_conflicts=ignore_conflicts,\n+                )\n+                if isinstance(inserted_columns, list):\n+                    inserted_rows.extend(inserted_columns)\n+                else:\n+                    inserted_rows.append(inserted_columns)\n+            else:\n+                self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)\n+        return inserted_rows\n+\n+    def _chain(self, **kwargs):\n+        \"\"\"\n+        Return a copy of the current QuerySet that's ready for another\n+        operation.\n+        \"\"\"\n+        obj = self._clone()\n+        if obj._sticky_filter:\n+            obj.query.filter_is_sticky = True\n+            obj._sticky_filter = False\n+        obj.__dict__.update(kwargs)\n+        return obj\n+\n+    def _clone(self):\n+        \"\"\"\n+        Return a copy of the current QuerySet. A lightweight alternative\n+        to deepcopy().\n+        \"\"\"\n+        c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)\n+        c._sticky_filter = self._sticky_filter\n+        c._for_write = self._for_write\n+        c._prefetch_related_lookups = self._prefetch_related_lookups[:]\n+        c._known_related_objects = self._known_related_objects\n+        c._iterable_class = self._iterable_class\n+        c._fields = self._fields\n+        return c\n+\n+    def _fetch_all(self):\n+        if self._result_cache is None:\n+            self._result_cache = list(self._iterable_class(self))\n+        if self._prefetch_related_lookups and not self._prefetch_done:\n+            self._prefetch_related_objects()\n+\n+    def _next_is_sticky(self):\n+        \"\"\"\n+        Indicate that the next filter call and the one following that should\n+        be treated as a single filter. This is only important when it comes to\n+        determining when to reuse tables for many-to-many filters. Required so\n+        that we can filter naturally on the results of related managers.\n+\n+        This doesn't return a clone of the current QuerySet (it returns\n+        \"self\"). The method is only used internally and should be immediately\n+        followed by a filter() that does create a clone.\n+        \"\"\"\n+        self._sticky_filter = True\n+        return self\n+\n+    def _merge_sanity_check(self, other):\n+        \"\"\"Check that two QuerySet classes may be merged.\"\"\"\n+        if self._fields is not None and (\n+                set(self.query.values_select) != set(other.query.values_select) or\n+                set(self.query.extra_select) != set(other.query.extra_select) or\n+                set(self.query.annotation_select) != set(other.query.annotation_select)):\n+            raise TypeError(\n+                \"Merging '%s' classes must involve the same values in each case.\"\n+                % self.__class__.__name__\n+            )\n+\n+    def _merge_known_related_objects(self, other):\n+        \"\"\"\n+        Keep track of all known related objects from either QuerySet instance.\n+        \"\"\"\n+        for field, objects in other._known_related_objects.items():\n+            self._known_related_objects.setdefault(field, {}).update(objects)\n+\n+    def resolve_expression(self, *args, **kwargs):\n+        if self._fields and len(self._fields) > 1:\n+            # values() queryset can only be used as nested queries\n+            # if they are set up to select only a single field.\n+            raise TypeError('Cannot use multi-field values as a filter value.')\n+        query = self.query.resolve_expression(*args, **kwargs)\n+        query._db = self._db\n+        return query\n+    resolve_expression.queryset_only = True\n+\n+    def _add_hints(self, **hints):\n+        \"\"\"\n+        Update hinting information for use by routers. Add new key/values or\n+        overwrite existing key/values.\n+        \"\"\"\n+        self._hints.update(hints)\n+\n+    def _has_filters(self):\n+        \"\"\"\n+        Check if this QuerySet has any filtering going on. This isn't\n+        equivalent with checking if all objects are present in results, for\n+        example, qs[1:]._has_filters() -> False.\n+        \"\"\"\n+        return self.query.has_filters()\n+\n+    @staticmethod\n+    def _validate_values_are_expressions(values, method_name):\n+        invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))\n+        if invalid_args:\n+            raise TypeError(\n+                'QuerySet.%s() received non-expression(s): %s.' % (\n+                    method_name,\n+                    ', '.join(invalid_args),\n+                )\n+            )\n+\n+    def _not_support_combined_queries(self, operation_name):\n+        if self.query.combinator:\n+            raise NotSupportedError(\n+                'Calling QuerySet.%s() after %s() is not supported.'\n+                % (operation_name, self.query.combinator)\n+            )\n+\n+\n+class InstanceCheckMeta(type):\n+    def __instancecheck__(self, instance):\n+        return isinstance(instance, QuerySet) and instance.query.is_empty()\n+\n+\n+class EmptyQuerySet(metaclass=InstanceCheckMeta):\n+    \"\"\"\n+    Marker class to checking if a queryset is empty by .none():\n+        isinstance(qs.none(), EmptyQuerySet) -> True\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        raise TypeError(\"EmptyQuerySet can't be instantiated\")\n+\n+\n+class RawQuerySet:\n+    \"\"\"\n+    Provide an iterator which converts the results of raw SQL queries into\n+    annotated model instances.\n+    \"\"\"\n+    def __init__(self, raw_query, model=None, query=None, params=None,\n+                 translations=None, using=None, hints=None):\n+        self.raw_query = raw_query\n+        self.model = model\n+        self._db = using\n+        self._hints = hints or {}\n+        self.query = query or sql.RawQuery(sql=raw_query, using=self.db, params=params)\n+        self.params = params or ()\n+        self.translations = translations or {}\n+        self._result_cache = None\n+        self._prefetch_related_lookups = ()\n+        self._prefetch_done = False\n+\n+    def resolve_model_init_order(self):\n+        \"\"\"Resolve the init field names and value positions.\"\"\"\n+        converter = connections[self.db].introspection.identifier_converter\n+        model_init_fields = [f for f in self.model._meta.fields if converter(f.column) in self.columns]\n+        annotation_fields = [(column, pos) for pos, column in enumerate(self.columns)\n+                             if column not in self.model_fields]\n+        model_init_order = [self.columns.index(converter(f.column)) for f in model_init_fields]\n+        model_init_names = [f.attname for f in model_init_fields]\n+        return model_init_names, model_init_order, annotation_fields\n+\n+    def prefetch_related(self, *lookups):\n+        \"\"\"Same as QuerySet.prefetch_related()\"\"\"\n+        clone = self._clone()\n+        if lookups == (None,):\n+            clone._prefetch_related_lookups = ()\n+        else:\n+            clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups\n+        return clone\n+\n+    def _prefetch_related_objects(self):\n+        prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)\n+        self._prefetch_done = True\n+\n+    def _clone(self):\n+        \"\"\"Same as QuerySet._clone()\"\"\"\n+        c = self.__class__(\n+            self.raw_query, model=self.model, query=self.query, params=self.params,\n+            translations=self.translations, using=self._db, hints=self._hints\n+        )\n+        c._prefetch_related_lookups = self._prefetch_related_lookups[:]\n+        return c\n+\n+    def _fetch_all(self):\n+        if self._result_cache is None:\n+            self._result_cache = list(self.iterator())\n+        if self._prefetch_related_lookups and not self._prefetch_done:\n+            self._prefetch_related_objects()\n+\n+    def __len__(self):\n+        self._fetch_all()\n+        return len(self._result_cache)\n+\n+    def __bool__(self):\n+        self._fetch_all()\n+        return bool(self._result_cache)\n+\n+    def __iter__(self):\n+        self._fetch_all()\n+        return iter(self._result_cache)\n+\n+    def iterator(self):\n+        # Cache some things for performance reasons outside the loop.\n+        db = self.db\n+        compiler = connections[db].ops.compiler('SQLCompiler')(\n+            self.query, connections[db], db\n+        )\n+\n+        query = iter(self.query)\n+\n+        try:\n+            model_init_names, model_init_pos, annotation_fields = self.resolve_model_init_order()\n+            if self.model._meta.pk.attname not in model_init_names:\n+                raise InvalidQuery('Raw query must include the primary key')\n+            model_cls = self.model\n+            fields = [self.model_fields.get(c) for c in self.columns]\n+            converters = compiler.get_converters([\n+                f.get_col(f.model._meta.db_table) if f else None for f in fields\n+            ])\n+            if converters:\n+                query = compiler.apply_converters(query, converters)\n+            for values in query:\n+                # Associate fields to values\n+                model_init_values = [values[pos] for pos in model_init_pos]\n+                instance = model_cls.from_db(db, model_init_names, model_init_values)\n+                if annotation_fields:\n+                    for column, pos in annotation_fields:\n+                        setattr(instance, column, values[pos])\n+                yield instance\n+        finally:\n+            # Done iterating the Query. If it has its own cursor, close it.\n+            if hasattr(self.query, 'cursor') and self.query.cursor:\n+                self.query.cursor.close()\n+\n+    def __repr__(self):\n+        return \"<%s: %s>\" % (self.__class__.__name__, self.query)\n+\n+    def __getitem__(self, k):\n+        return list(self)[k]\n+\n+    @property\n+    def db(self):\n+        \"\"\"Return the database used if this query is executed now.\"\"\"\n+        return self._db or router.db_for_read(self.model, **self._hints)\n+\n+    def using(self, alias):\n+        \"\"\"Select the database this RawQuerySet should execute against.\"\"\"\n+        return RawQuerySet(\n+            self.raw_query, model=self.model,\n+            query=self.query.chain(using=alias),\n+            params=self.params, translations=self.translations,\n+            using=alias,\n+        )\n+\n+    @cached_property\n+    def columns(self):\n+        \"\"\"\n+        A list of model field names in the order they'll appear in the\n+        query results.\n+        \"\"\"\n+        columns = self.query.get_columns()\n+        # Adjust any column names which don't match field names\n+        for (query_name, model_name) in self.translations.items():\n+            # Ignore translations for nonexistent column names\n+            try:\n+                index = columns.index(query_name)\n+            except ValueError:\n+                pass\n+            else:\n+                columns[index] = model_name\n+        return columns\n+\n+    @cached_property\n+    def model_fields(self):\n+        \"\"\"A dict mapping column names to model field names.\"\"\"\n+        converter = connections[self.db].introspection.identifier_converter\n+        model_fields = {}\n+        for field in self.model._meta.fields:\n+            name, column = field.get_attname_column()\n+            model_fields[converter(column)] = field\n+        return model_fields\n+\n+\n+class Prefetch:\n+    def __init__(self, lookup, queryset=None, to_attr=None):\n+        # `prefetch_through` is the path we traverse to perform the prefetch.\n+        self.prefetch_through = lookup\n+        # `prefetch_to` is the path to the attribute that stores the result.\n+        self.prefetch_to = lookup\n+        if queryset is not None and not issubclass(queryset._iterable_class, ModelIterable):\n+            raise ValueError('Prefetch querysets cannot use values().')\n+        if to_attr:\n+            self.prefetch_to = LOOKUP_SEP.join(lookup.split(LOOKUP_SEP)[:-1] + [to_attr])\n+\n+        self.queryset = queryset\n+        self.to_attr = to_attr\n+\n+    def __getstate__(self):\n+        obj_dict = self.__dict__.copy()\n+        if self.queryset is not None:\n+            # Prevent the QuerySet from being evaluated\n+            obj_dict['queryset'] = self.queryset._chain(\n+                _result_cache=[],\n+                _prefetch_done=True,\n+            )\n+        return obj_dict\n+\n+    def add_prefix(self, prefix):\n+        self.prefetch_through = prefix + LOOKUP_SEP + self.prefetch_through\n+        self.prefetch_to = prefix + LOOKUP_SEP + self.prefetch_to\n+\n+    def get_current_prefetch_to(self, level):\n+        return LOOKUP_SEP.join(self.prefetch_to.split(LOOKUP_SEP)[:level + 1])\n+\n+    def get_current_to_attr(self, level):\n+        parts = self.prefetch_to.split(LOOKUP_SEP)\n+        to_attr = parts[level]\n+        as_attr = self.to_attr and level == len(parts) - 1\n+        return to_attr, as_attr\n+\n+    def get_current_queryset(self, level):\n+        if self.get_current_prefetch_to(level) == self.prefetch_to:\n+            return self.queryset\n+        return None\n+\n+    def __eq__(self, other):\n+        if not isinstance(other, Prefetch):\n+            return NotImplemented\n+        return self.prefetch_to == other.prefetch_to\n+\n+    def __hash__(self):\n+        return hash((self.__class__, self.prefetch_to))\n+\n+\n+def normalize_prefetch_lookups(lookups, prefix=None):\n+    \"\"\"Normalize lookups into Prefetch objects.\"\"\"\n+    ret = []\n+    for lookup in lookups:\n+        if not isinstance(lookup, Prefetch):\n+            lookup = Prefetch(lookup)\n+        if prefix:\n+            lookup.add_prefix(prefix)\n+        ret.append(lookup)\n+    return ret\n+\n+\n+def prefetch_related_objects(model_instances, *related_lookups):\n+    \"\"\"\n+    Populate prefetched object caches for a list of model instances based on\n+    the lookups/Prefetch instances given.\n+    \"\"\"\n+    if not model_instances:\n+        return  # nothing to do\n+\n+    # We need to be able to dynamically add to the list of prefetch_related\n+    # lookups that we look up (see below).  So we need some book keeping to\n+    # ensure we don't do duplicate work.\n+    done_queries = {}    # dictionary of things like 'foo__bar': [results]\n+\n+    auto_lookups = set()  # we add to this as we go through.\n+    followed_descriptors = set()  # recursion protection\n+\n+    all_lookups = normalize_prefetch_lookups(reversed(related_lookups))\n+    while all_lookups:\n+        lookup = all_lookups.pop()\n+        if lookup.prefetch_to in done_queries:\n+            if lookup.queryset is not None:\n+                raise ValueError(\"'%s' lookup was already seen with a different queryset. \"\n+                                 \"You may need to adjust the ordering of your lookups.\" % lookup.prefetch_to)\n+\n+            continue\n+\n+        # Top level, the list of objects to decorate is the result cache\n+        # from the primary QuerySet. It won't be for deeper levels.\n+        obj_list = model_instances\n+\n+        through_attrs = lookup.prefetch_through.split(LOOKUP_SEP)\n+        for level, through_attr in enumerate(through_attrs):\n+            # Prepare main instances\n+            if not obj_list:\n+                break\n+\n+            prefetch_to = lookup.get_current_prefetch_to(level)\n+            if prefetch_to in done_queries:\n+                # Skip any prefetching, and any object preparation\n+                obj_list = done_queries[prefetch_to]\n+                continue\n+\n+            # Prepare objects:\n+            good_objects = True\n+            for obj in obj_list:\n+                # Since prefetching can re-use instances, it is possible to have\n+                # the same instance multiple times in obj_list, so obj might\n+                # already be prepared.\n+                if not hasattr(obj, '_prefetched_objects_cache'):\n+                    try:\n+                        obj._prefetched_objects_cache = {}\n+                    except (AttributeError, TypeError):\n+                        # Must be an immutable object from\n+                        # values_list(flat=True), for example (TypeError) or\n+                        # a QuerySet subclass that isn't returning Model\n+                        # instances (AttributeError), either in Django or a 3rd\n+                        # party. prefetch_related() doesn't make sense, so quit.\n+                        good_objects = False\n+                        break\n+            if not good_objects:\n+                break\n+\n+            # Descend down tree\n+\n+            # We assume that objects retrieved are homogeneous (which is the premise\n+            # of prefetch_related), so what applies to first object applies to all.\n+            first_obj = obj_list[0]\n+            to_attr = lookup.get_current_to_attr(level)[0]\n+            prefetcher, descriptor, attr_found, is_fetched = get_prefetcher(first_obj, through_attr, to_attr)\n+\n+            if not attr_found:\n+                raise AttributeError(\"Cannot find '%s' on %s object, '%s' is an invalid \"\n+                                     \"parameter to prefetch_related()\" %\n+                                     (through_attr, first_obj.__class__.__name__, lookup.prefetch_through))\n+\n+            if level == len(through_attrs) - 1 and prefetcher is None:\n+                # Last one, this *must* resolve to something that supports\n+                # prefetching, otherwise there is no point adding it and the\n+                # developer asking for it has made a mistake.\n+                raise ValueError(\"'%s' does not resolve to an item that supports \"\n+                                 \"prefetching - this is an invalid parameter to \"\n+                                 \"prefetch_related().\" % lookup.prefetch_through)\n+\n+            if prefetcher is not None and not is_fetched:\n+                obj_list, additional_lookups = prefetch_one_level(obj_list, prefetcher, lookup, level)\n+                # We need to ensure we don't keep adding lookups from the\n+                # same relationships to stop infinite recursion. So, if we\n+                # are already on an automatically added lookup, don't add\n+                # the new lookups from relationships we've seen already.\n+                if not (prefetch_to in done_queries and lookup in auto_lookups and descriptor in followed_descriptors):\n+                    done_queries[prefetch_to] = obj_list\n+                    new_lookups = normalize_prefetch_lookups(reversed(additional_lookups), prefetch_to)\n+                    auto_lookups.update(new_lookups)\n+                    all_lookups.extend(new_lookups)\n+                followed_descriptors.add(descriptor)\n+            else:\n+                # Either a singly related object that has already been fetched\n+                # (e.g. via select_related), or hopefully some other property\n+                # that doesn't support prefetching but needs to be traversed.\n+\n+                # We replace the current list of parent objects with the list\n+                # of related objects, filtering out empty or missing values so\n+                # that we can continue with nullable or reverse relations.\n+                new_obj_list = []\n+                for obj in obj_list:\n+                    if through_attr in getattr(obj, '_prefetched_objects_cache', ()):\n+                        # If related objects have been prefetched, use the\n+                        # cache rather than the object's through_attr.\n+                        new_obj = list(obj._prefetched_objects_cache.get(through_attr))\n+                    else:\n+                        try:\n+                            new_obj = getattr(obj, through_attr)\n+                        except exceptions.ObjectDoesNotExist:\n+                            continue\n+                    if new_obj is None:\n+                        continue\n+                    # We special-case `list` rather than something more generic\n+                    # like `Iterable` because we don't want to accidentally match\n+                    # user models that define __iter__.\n+                    if isinstance(new_obj, list):\n+                        new_obj_list.extend(new_obj)\n+                    else:\n+                        new_obj_list.append(new_obj)\n+                obj_list = new_obj_list\n+\n+\n+def get_prefetcher(instance, through_attr, to_attr):\n+    \"\"\"\n+    For the attribute 'through_attr' on the given instance, find\n+    an object that has a get_prefetch_queryset().\n+    Return a 4 tuple containing:\n+    (the object with get_prefetch_queryset (or None),\n+     the descriptor object representing this relationship (or None),\n+     a boolean that is False if the attribute was not found at all,\n+     a boolean that is True if the attribute has already been fetched)\n+    \"\"\"\n+    prefetcher = None\n+    is_fetched = False\n+\n+    # For singly related objects, we have to avoid getting the attribute\n+    # from the object, as this will trigger the query. So we first try\n+    # on the class, in order to get the descriptor object.\n+    rel_obj_descriptor = getattr(instance.__class__, through_attr, None)\n+    if rel_obj_descriptor is None:\n+        attr_found = hasattr(instance, through_attr)\n+    else:\n+        attr_found = True\n+        if rel_obj_descriptor:\n+            # singly related object, descriptor object has the\n+            # get_prefetch_queryset() method.\n+            if hasattr(rel_obj_descriptor, 'get_prefetch_queryset'):\n+                prefetcher = rel_obj_descriptor\n+                if rel_obj_descriptor.is_cached(instance):\n+                    is_fetched = True\n+            else:\n+                # descriptor doesn't support prefetching, so we go ahead and get\n+                # the attribute on the instance rather than the class to\n+                # support many related managers\n+                rel_obj = getattr(instance, through_attr)\n+                if hasattr(rel_obj, 'get_prefetch_queryset'):\n+                    prefetcher = rel_obj\n+                if through_attr != to_attr:\n+                    # Special case cached_property instances because hasattr\n+                    # triggers attribute computation and assignment.\n+                    if isinstance(getattr(instance.__class__, to_attr, None), cached_property):\n+                        is_fetched = to_attr in instance.__dict__\n+                    else:\n+                        is_fetched = hasattr(instance, to_attr)\n+                else:\n+                    is_fetched = through_attr in instance._prefetched_objects_cache\n+    return prefetcher, rel_obj_descriptor, attr_found, is_fetched\n+\n+\n+def prefetch_one_level(instances, prefetcher, lookup, level):\n+    \"\"\"\n+    Helper function for prefetch_related_objects().\n+\n+    Run prefetches on all instances using the prefetcher object,\n+    assigning results to relevant caches in instance.\n+\n+    Return the prefetched objects along with any additional prefetches that\n+    must be done due to prefetch_related lookups found from default managers.\n+    \"\"\"\n+    # prefetcher must have a method get_prefetch_queryset() which takes a list\n+    # of instances, and returns a tuple:\n+\n+    # (queryset of instances of self.model that are related to passed in instances,\n+    #  callable that gets value to be matched for returned instances,\n+    #  callable that gets value to be matched for passed in instances,\n+    #  boolean that is True for singly related objects,\n+    #  cache or field name to assign to,\n+    #  boolean that is True when the previous argument is a cache name vs a field name).\n+\n+    # The 'values to be matched' must be hashable as they will be used\n+    # in a dictionary.\n+\n+    rel_qs, rel_obj_attr, instance_attr, single, cache_name, is_descriptor = (\n+        prefetcher.get_prefetch_queryset(instances, lookup.get_current_queryset(level)))\n+    # We have to handle the possibility that the QuerySet we just got back\n+    # contains some prefetch_related lookups. We don't want to trigger the\n+    # prefetch_related functionality by evaluating the query. Rather, we need\n+    # to merge in the prefetch_related lookups.\n+    # Copy the lookups in case it is a Prefetch object which could be reused\n+    # later (happens in nested prefetch_related).\n+    additional_lookups = [\n+        copy.copy(additional_lookup) for additional_lookup\n+        in getattr(rel_qs, '_prefetch_related_lookups', ())\n+    ]\n+    if additional_lookups:\n+        # Don't need to clone because the manager should have given us a fresh\n+        # instance, so we access an internal instead of using public interface\n+        # for performance reasons.\n+        rel_qs._prefetch_related_lookups = ()\n+\n+    all_related_objects = list(rel_qs)\n+\n+    rel_obj_cache = {}\n+    for rel_obj in all_related_objects:\n+        rel_attr_val = rel_obj_attr(rel_obj)\n+        rel_obj_cache.setdefault(rel_attr_val, []).append(rel_obj)\n+\n+    to_attr, as_attr = lookup.get_current_to_attr(level)\n+    # Make sure `to_attr` does not conflict with a field.\n+    if as_attr and instances:\n+        # We assume that objects retrieved are homogeneous (which is the premise\n+        # of prefetch_related), so what applies to first object applies to all.\n+        model = instances[0].__class__\n+        try:\n+            model._meta.get_field(to_attr)\n+        except exceptions.FieldDoesNotExist:\n+            pass\n+        else:\n+            msg = 'to_attr={} conflicts with a field on the {} model.'\n+            raise ValueError(msg.format(to_attr, model.__name__))\n+\n+    # Whether or not we're prefetching the last part of the lookup.\n+    leaf = len(lookup.prefetch_through.split(LOOKUP_SEP)) - 1 == level\n+\n+    for obj in instances:\n+        instance_attr_val = instance_attr(obj)\n+        vals = rel_obj_cache.get(instance_attr_val, [])\n+\n+        if single:\n+            val = vals[0] if vals else None\n+            if as_attr:\n+                # A to_attr has been given for the prefetch.\n+                setattr(obj, to_attr, val)\n+            elif is_descriptor:\n+                # cache_name points to a field name in obj.\n+                # This field is a descriptor for a related object.\n+                setattr(obj, cache_name, val)\n+            else:\n+                # No to_attr has been given for this prefetch operation and the\n+                # cache_name does not point to a descriptor. Store the value of\n+                # the field in the object's field cache.\n+                obj._state.fields_cache[cache_name] = val\n+        else:\n+            if as_attr:\n+                setattr(obj, to_attr, vals)\n+            else:\n+                manager = getattr(obj, to_attr)\n+                if leaf and lookup.queryset is not None:\n+                    qs = manager._apply_rel_filters(lookup.queryset)\n+                else:\n+                    qs = manager.get_queryset()\n+                qs._result_cache = vals\n+                # We don't want the individual qs doing prefetch_related now,\n+                # since we have merged this into the current work.\n+                qs._prefetch_done = True\n+                obj._prefetched_objects_cache[cache_name] = qs\n+    return all_related_objects, additional_lookups\n+\n+\n+class RelatedPopulator:\n+    \"\"\"\n+    RelatedPopulator is used for select_related() object instantiation.\n+\n+    The idea is that each select_related() model will be populated by a\n+    different RelatedPopulator instance. The RelatedPopulator instances get\n+    klass_info and select (computed in SQLCompiler) plus the used db as\n+    input for initialization. That data is used to compute which columns\n+    to use, how to instantiate the model, and how to populate the links\n+    between the objects.\n+\n+    The actual creation of the objects is done in populate() method. This\n+    method gets row and from_obj as input and populates the select_related()\n+    model instance.\n+    \"\"\"\n+    def __init__(self, klass_info, select, db):\n+        self.db = db\n+        # Pre-compute needed attributes. The attributes are:\n+        #  - model_cls: the possibly deferred model class to instantiate\n+        #  - either:\n+        #    - cols_start, cols_end: usually the columns in the row are\n+        #      in the same order model_cls.__init__ expects them, so we\n+        #      can instantiate by model_cls(*row[cols_start:cols_end])\n+        #    - reorder_for_init: When select_related descends to a child\n+        #      class, then we want to reuse the already selected parent\n+        #      data. However, in this case the parent data isn't necessarily\n+        #      in the same order that Model.__init__ expects it to be, so\n+        #      we have to reorder the parent data. The reorder_for_init\n+        #      attribute contains a function used to reorder the field data\n+        #      in the order __init__ expects it.\n+        #  - pk_idx: the index of the primary key field in the reordered\n+        #    model data. Used to check if a related object exists at all.\n+        #  - init_list: the field attnames fetched from the database. For\n+        #    deferred models this isn't the same as all attnames of the\n+        #    model's fields.\n+        #  - related_populators: a list of RelatedPopulator instances if\n+        #    select_related() descends to related models from this model.\n+        #  - local_setter, remote_setter: Methods to set cached values on\n+        #    the object being populated and on the remote object. Usually\n+        #    these are Field.set_cached_value() methods.\n+        select_fields = klass_info['select_fields']\n+        from_parent = klass_info['from_parent']\n+        if not from_parent:\n+            self.cols_start = select_fields[0]\n+            self.cols_end = select_fields[-1] + 1\n+            self.init_list = [\n+                f[0].target.attname for f in select[self.cols_start:self.cols_end]\n+            ]\n+            self.reorder_for_init = None\n+        else:\n+            attname_indexes = {select[idx][0].target.attname: idx for idx in select_fields}\n+            model_init_attnames = (f.attname for f in klass_info['model']._meta.concrete_fields)\n+            self.init_list = [attname for attname in model_init_attnames if attname in attname_indexes]\n+            self.reorder_for_init = operator.itemgetter(*[attname_indexes[attname] for attname in self.init_list])\n+\n+        self.model_cls = klass_info['model']\n+        self.pk_idx = self.init_list.index(self.model_cls._meta.pk.attname)\n+        self.related_populators = get_related_populators(klass_info, select, self.db)\n+        self.local_setter = klass_info['local_setter']\n+        self.remote_setter = klass_info['remote_setter']\n+\n+    def populate(self, row, from_obj):\n+        if self.reorder_for_init:\n+            obj_data = self.reorder_for_init(row)\n+        else:\n+            obj_data = row[self.cols_start:self.cols_end]\n+        if obj_data[self.pk_idx] is None:\n+            obj = None\n+        else:\n+            obj = self.model_cls.from_db(self.db, self.init_list, obj_data)\n+            for rel_iter in self.related_populators:\n+                rel_iter.populate(row, obj)\n+        self.local_setter(from_obj, obj)\n+        if obj is not None:\n+            self.remote_setter(obj, from_obj)\n+\n+\n+def get_related_populators(klass_info, select, db):\n+    iterators = []\n+    related_klass_infos = klass_info.get('related_klass_infos', [])\n+    for rel_klass_info in related_klass_infos:\n+        rel_cls = RelatedPopulator(rel_klass_info, select, db)\n+        iterators.append(rel_cls)\n+    return iterators\n"
  },
  "django__django-11964": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-11964",
    "model_patch": "diff --git a/django/db/models/fields/__init__.py b/django/db/models/fields/__init__.py\nindex d610dc86f0..607cc30f1a 100644\n--- a/django/db/models/fields/__init__.py\n+++ b/django/db/models/fields/__init__.py\n@@ -7,6 +7,7 @@ import uuid\n import warnings\n from base64 import b64decode, b64encode\n from functools import partialmethod, total_ordering\n+import enum\n \n from django import forms\n from django.apps import apps\n@@ -1008,6 +1009,12 @@ class CharField(Field):\n         return \"CharField\"\n \n     def to_python(self, value):\n+        # Unwrap enum.Enum instances (TextChoices/IntegerChoices) to use their\n+        # underlying value rather than the Enum member object. This ensures\n+        # freshly assigned choice values behave the same as values retrieved\n+        # from the database.\n+        if isinstance(value, enum.Enum):\n+            value = value.value\n         if isinstance(value, str) or value is None:\n             return value\n         return str(value)\n@@ -2023,6 +2030,12 @@ class TextField(Field):\n         return \"TextField\"\n \n     def to_python(self, value):\n+        # Unwrap enum.Enum instances (TextChoices/IntegerChoices) to use their\n+        # underlying value rather than the Enum member object. This ensures\n+        # freshly assigned choice values behave the same as values retrieved\n+        # from the database.\n+        if isinstance(value, enum.Enum):\n+            value = value.value\n         if isinstance(value, str) or value is None:\n             return value\n         return str(value)\n"
  },
  "django__django-11999": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-11999",
    "model_patch": "diff --git a/django/db/models/fields/__init__.py b/django/db/models/fields/__init__.py\nindex ee45bb941e..f9fa34267d 100644\n--- a/django/db/models/fields/__init__.py\n+++ b/django/db/models/fields/__init__.py\n@@ -763,8 +763,9 @@ class Field(RegisterLookupMixin):\n             if not getattr(cls, self.attname, None):\n                 setattr(cls, self.attname, self.descriptor_class(self))\n         if self.choices is not None:\n-            setattr(cls, 'get_%s_display' % self.name,\n-                    partialmethod(cls._get_FIELD_display, field=self))\n+            method_name = 'get_%s_display' % self.name\n+            if not getattr(cls, method_name, None):\n+                setattr(cls, method_name, partialmethod(cls._get_FIELD_display, field=self))\n \n     def get_filter_kwargs_for_object(self, obj):\n         \"\"\"\ndiff --git a/test_repro.py b/test_repro.py\nnew file mode 100644\nindex 0000000000..e9e42e69ab\n--- /dev/null\n+++ b/test_repro.py\n@@ -0,0 +1,18 @@\n+from django.conf import settings\n+if not settings.configured:\n+    settings.configure(INSTALLED_APPS=[])\n+import django\n+django.setup()\n+from django.db import models\n+\n+class FooBar(models.Model):\n+    foo_bar = models.CharField(\"foo\", choices=[(1, 'foo'), (2, 'bar')], max_length=10)\n+    def __str__(self):\n+        return self.get_foo_bar_display()\n+    def get_foo_bar_display(self):\n+        return \"something\"\n+    class Meta:\n+        abstract = True\n+\n+f = FooBar()\n+print(f.get_foo_bar_display())\n"
  },
  "django__django-12039": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-12039",
    "model_patch": "diff --git a/django/db/backends/ddl_references.py b/django/db/backends/ddl_references.py\nindex 44e49806f8..818c5d1420 100644\n--- a/django/db/backends/ddl_references.py\n+++ b/django/db/backends/ddl_references.py\n@@ -83,10 +83,14 @@ class Columns(TableColumns):\n \n     def __str__(self):\n         def col_str(column, idx):\n+            name = self.quote_name(column)\n             try:\n-                return self.quote_name(column) + self.col_suffixes[idx]\n+                suffix = self.col_suffixes[idx]\n             except IndexError:\n-                return self.quote_name(column)\n+                suffix = None\n+            if suffix:\n+                return '{} {}'.format(name, suffix.strip())\n+            return name\n \n         return ', '.join(col_str(column, idx) for idx, column in enumerate(self.columns))\n \n@@ -112,12 +116,20 @@ class IndexColumns(Columns):\n         def col_str(column, idx):\n             # Index.__init__() guarantees that self.opclasses is the same\n             # length as self.columns.\n-            col = '{} {}'.format(self.quote_name(column), self.opclasses[idx])\n+            parts = [self.quote_name(column)]\n             try:\n-                col = '{} {}'.format(col, self.col_suffixes[idx])\n+                opclass = self.opclasses[idx]\n             except IndexError:\n-                pass\n-            return col\n+                opclass = None\n+            if opclass:\n+                parts.append(opclass.strip())\n+            try:\n+                suffix = self.col_suffixes[idx]\n+            except IndexError:\n+                suffix = None\n+            if suffix:\n+                parts.append(suffix.strip())\n+            return ' '.join(parts)\n \n         return ', '.join(col_str(column, idx) for idx, column in enumerate(self.columns))\n \n"
  },
  "django__django-12050": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-12050",
    "model_patch": "diff --git a/django/db/models/sql/query.py b/django/db/models/sql/query.py\nindex ea51912e8b..ca065fc0f0 100644\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -1073,7 +1073,13 @@ class Query(BaseExpression):\n                         ))\n                 else:\n                     resolved_values.append(sub_value)\n-            value = tuple(resolved_values)\n+            # Preserve the input iterable type: if the input was a list, keep a list;\n+            # if it was a tuple, keep a tuple. This avoids changing the semantics\n+            # for field types that depend on the exact input type (e.g. PickledField).\n+            if isinstance(value, list):\n+                value = resolved_values\n+            else:\n+                value = tuple(resolved_values)\n         return value\n \n     def solve_lookup_type(self, lookup):\n"
  },
  "django__django-12155": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-12155",
    "model_patch": "diff --git a/django/contrib/admindocs/utils.py b/django/contrib/admindocs/utils.py\nindex 4c0e7e2a56..aa663ead5b 100644\n--- a/django/contrib/admindocs/utils.py\n+++ b/django/contrib/admindocs/utils.py\n@@ -34,7 +34,7 @@ def trim_docstring(docstring):\n         return ''\n     # Convert tabs to spaces and split into lines\n     lines = docstring.expandtabs().splitlines()\n-    indent = min(len(line) - len(line.lstrip()) for line in lines if line.lstrip())\n+    indent = min((len(line) - len(line.lstrip()) for line in lines[1:] if line.lstrip()), default=0)\n     trimmed = [lines[0].lstrip()] + [line[indent:].rstrip() for line in lines[1:]]\n     return \"\\n\".join(trimmed).strip()\n \n"
  },
  "django__django-12193": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-12193",
    "model_patch": "diff --git a/django/contrib/postgres/forms/array.py b/django/contrib/postgres/forms/array.py\nindex 2e19cd574a..18cfccf39c 100644\n--- a/django/contrib/postgres/forms/array.py\n+++ b/django/contrib/postgres/forms/array.py\n@@ -147,7 +147,7 @@ class SplitArrayWidget(forms.Widget):\n             if id_:\n                 final_attrs = {**final_attrs, 'id': '%s_%s' % (id_, i)}\n             context['widget']['subwidgets'].append(\n-                self.widget.get_context(name + '_%s' % i, widget_value, final_attrs)['widget']\n+                self.widget.get_context(name + '_%s' % i, widget_value, final_attrs.copy())['widget']\n             )\n         return context\n \n"
  },
  "django__django-12209": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-12209",
    "model_patch": "diff --git a/django/db/models/base.py b/django/db/models/base.py\nindex e3b14a41a0..f9c2f51790 100644\n--- a/django/db/models/base.py\n+++ b/django/db/models/base.py\n@@ -850,7 +850,7 @@ class Model(metaclass=ModelBase):\n         # Skip an UPDATE when adding an instance and primary key has a default.\n         if (\n             not force_insert and\n-            self._state.adding and\n+            self._state.adding and not pk_set and\n             self._meta.pk.default and\n             self._meta.pk.default is not NOT_PROVIDED\n         ):\n@@ -1099,7 +1099,7 @@ class Model(metaclass=ModelBase):\n             # allows single model to have effectively multiple primary keys.\n             # Refs #17615.\n             model_class_pk = self._get_pk_val(model_class._meta)\n-            if not self._state.adding and model_class_pk is not None:\n+            if not self._state.adding and not pk_set and model_class_pk is not None:\n                 qs = qs.exclude(pk=model_class_pk)\n             if qs.exists():\n                 if len(unique_check) == 1:\n@@ -1130,7 +1130,7 @@ class Model(metaclass=ModelBase):\n             qs = model_class._default_manager.filter(**lookup_kwargs)\n             # Exclude the current object from the query if we are editing an\n             # instance (as opposed to creating a new one)\n-            if not self._state.adding and self.pk is not None:\n+            if not self._state.adding and not pk_set and self.pk is not None:\n                 qs = qs.exclude(pk=self.pk)\n \n             if qs.exists():\ndiff --git a/testbed/repro_save.py b/testbed/repro_save.py\nnew file mode 100644\nindex 0000000000..d8409c1f54\n--- /dev/null\n+++ b/testbed/repro_save.py\n@@ -0,0 +1,22 @@\n+from uuid import uuid4\n+import django\n+from django.conf import settings\n+from django.db import models, connection\n+settings.configure(\n+    DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:'}},\n+    INSTALLED_APPS=[],\n+)\n+django.setup()\n+class Sample(models.Model):\n+    id = models.UUIDField(primary_key=True, default=uuid4)\n+    name = models.CharField(blank=True, max_length=100)\n+    class Meta:\n+        app_label = 'testbed_app'\n+from django.apps import apps\n+apps.register_model('testbed_app', Sample)\n+with connection.schema_editor() as schema_editor:\n+    schema_editor.create_model(Sample)\n+s0 = Sample.objects.create()\n+s1 = Sample(pk=s0.pk, name='Test 1')\n+s1.save()\n+print('OK', Sample.objects.count(), Sample.objects.get(pk=s0.pk).name)\n"
  },
  "django__django-12262": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-12262",
    "model_patch": "diff --git a/django/template/library.py b/django/template/library.py\nindex 20bc86dac8..a1b87b690e 100644\n--- a/django/template/library.py\n+++ b/django/template/library.py\n@@ -261,26 +261,28 @@ def parse_bits(parser, bits, params, varargs, varkw, defaults,\n         if kwarg:\n             # The kwarg was successfully extracted\n             param, value = kwarg.popitem()\n-            if param not in params and param not in unhandled_kwargs and varkw is None:\n-                # An unexpected keyword argument was supplied\n+            # If the kwarg has already been supplied, raise the proper error.\n+            if param in kwargs:\n                 raise TemplateSyntaxError(\n-                    \"'%s' received unexpected keyword argument '%s'\" %\n+                    \"'%s' received multiple values for keyword argument '%s'\" %\n                     (name, param))\n-            elif param in kwargs:\n-                # The keyword argument has already been supplied once\n+            # Validate that the kwarg is expected. Accept keyword-only arguments\n+            # even if they have defaults (they won't be listed in\n+            # unhandled_kwargs).\n+            if param not in params and param not in kwonly and varkw is None:\n+                # An unexpected keyword argument was supplied\n                 raise TemplateSyntaxError(\n-                    \"'%s' received multiple values for keyword argument '%s'\" %\n+                    \"'%s' received unexpected keyword argument '%s'\" %\n                     (name, param))\n-            else:\n-                # All good, record the keyword argument\n-                kwargs[str(param)] = value\n-                if param in unhandled_params:\n-                    # If using the keyword syntax for a positional arg, then\n-                    # consume it.\n-                    unhandled_params.remove(param)\n-                elif param in unhandled_kwargs:\n-                    # Same for keyword-only arguments\n-                    unhandled_kwargs.remove(param)\n+            # All good, record the keyword argument\n+            kwargs[str(param)] = value\n+            if param in unhandled_params:\n+                # If using the keyword syntax for a positional arg, then\n+                # consume it.\n+                unhandled_params.remove(param)\n+            elif param in unhandled_kwargs:\n+                # Same for keyword-only arguments\n+                unhandled_kwargs.remove(param)\n         else:\n             if kwargs:\n                 raise TemplateSyntaxError(\n"
  },
  "django__django-12276": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-12276",
    "model_patch": "diff --git a/django/forms/widgets.py b/django/forms/widgets.py\nindex 338d54d72f..656af5618e 100644\n--- a/django/forms/widgets.py\n+++ b/django/forms/widgets.py\n@@ -388,6 +388,10 @@ class FileInput(Input):\n         return name not in files\n \n \n+    def use_required_attribute(self, initial):\n+        return super().use_required_attribute(initial) and not initial\n+\n+\n FILE_INPUT_CONTRADICTION = object()\n \n \n@@ -451,9 +455,6 @@ class ClearableFileInput(FileInput):\n             return False\n         return upload\n \n-    def use_required_attribute(self, initial):\n-        return super().use_required_attribute(initial) and not initial\n-\n     def value_omitted_from_data(self, data, files, name):\n         return (\n             super().value_omitted_from_data(data, files, name) and\n"
  },
  "django__django-12304": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-12304",
    "model_patch": "diff --git a/django/db/models/enums.py b/django/db/models/enums.py\nindex f48143ddaf..72c68d25ab 100644\n--- a/django/db/models/enums.py\n+++ b/django/db/models/enums.py\n@@ -59,6 +59,7 @@ class ChoicesMeta(enum.EnumMeta):\n \n \n class Choices(enum.Enum, metaclass=ChoicesMeta):\n+    do_not_call_in_templates = True\n     \"\"\"Class for creating enumerated choices.\"\"\"\n \n     def __str__(self):\n@@ -70,11 +71,13 @@ class Choices(enum.Enum, metaclass=ChoicesMeta):\n \n \n class IntegerChoices(int, Choices):\n+    do_not_call_in_templates = True\n     \"\"\"Class for creating enumerated integer choices.\"\"\"\n     pass\n \n \n class TextChoices(str, Choices):\n+    do_not_call_in_templates = True\n     \"\"\"Class for creating enumerated string choices.\"\"\"\n \n     def _generate_next_value_(name, start, count, last_values):\n"
  },
  "django__django-12308": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-12308",
    "model_patch": "diff --git a/django/contrib/admin/utils.py b/django/contrib/admin/utils.py\nindex e4ddc8f017..58f2579abb 100644\n--- a/django/contrib/admin/utils.py\n+++ b/django/contrib/admin/utils.py\n@@ -398,6 +398,13 @@ def display_for_field(value, field, empty_value_display):\n         return formats.number_format(value)\n     elif isinstance(field, models.FileField) and value:\n         return format_html('<a href=\"{}\">{}</a>', value.url, value)\n+    elif getattr(field, '__class__', None) and getattr(field, 'get_internal_type', lambda: None)() == 'JSONField':\n+        try:\n+            form_field = field.formfield()\n+            # Use the form field's prepare_value to render JSON safely (handles invalid input)\n+            return form_field.prepare_value(value)\n+        except Exception:\n+            return display_for_value(value, empty_value_display)\n     else:\n         return display_for_value(value, empty_value_display)\n \n@@ -457,89 +464,64 @@ def reverse_field_path(model, path):\n         # Field should point to another model\n         if field.is_relation and not (field.auto_created and not field.concrete):\n             related_name = field.related_query_name()\n-            parent = field.remote_field.model\n-        else:\n-            related_name = field.field.name\n-            parent = field.related_model\n-        reversed_path.insert(0, related_name)\n-    return (parent, LOOKUP_SEP.join(reversed_path))\n-\n \n+            parent = field.remote_field.model\n def get_fields_from_path(model, path):\n-    \"\"\" Return list of Fields given path relative to model.\n-\n-    e.g. (ModelX, \"user__groups__name\") -> [\n-        <django.db.models.fields.related.ForeignKey object at 0x...>,\n-        <django.db.models.fields.related.ManyToManyField object at 0x...>,\n-        <django.db.models.fields.CharField object at 0x...>,\n-    ]\n     \"\"\"\n-    pieces = path.split(LOOKUP_SEP)\n+    Given a model and a lookup path (e.g. \"user__groups\"), return a list of\n+    field instances corresponding to each part of the path. Stops when a\n+    non-relation field is encountered.\n+    \"\"\"\n+    parts = path.split(LOOKUP_SEP) if path else []\n     fields = []\n-    for piece in pieces:\n-        if fields:\n-            parent = get_model_from_relation(fields[-1])\n+    parent = model\n+    for part in parts:\n+        try:\n+            field = parent._meta.get_field(part)\n+        except Exception:\n+            # Can't resolve further\n+            break\n+        fields.append(field)\n+        # If the field points to another model, continue there; otherwise stop.\n+        if getattr(field, 'is_relation', False) and not (getattr(field, 'auto_created', False) and not getattr(field, 'concrete', True)):\n+            try:\n+                parent = field.remote_field.model\n+            except Exception:\n+                break\n         else:\n-            parent = model\n-        fields.append(parent._meta.get_field(piece))\n+            break\n     return fields\n-\n-\n-def construct_change_message(form, formsets, add):\n+def construct_change_message(form, formsets, add=False):\n     \"\"\"\n-    Construct a JSON structure describing changes from a changed object.\n-    Translations are deactivated so that strings are stored untranslated.\n-    Translation happens later on LogEntry access.\n+    Construct a change message describing additions, changes and deletions\n+    from a form and its formsets. Returns a JSON-like string representation.\n     \"\"\"\n-    # Evaluating `form.changed_data` prior to disabling translations is required\n-    # to avoid fields affected by localization from being included incorrectly,\n-    # e.g. where date formats differ such as MM/DD/YYYY vs DD/MM/YYYY.\n-    changed_data = form.changed_data\n-    with translation_override(None):\n-        # Deactivate translations while fetching verbose_name for form\n-        # field labels and using `field_name`, if verbose_name is not provided.\n-        # Translations will happen later on LogEntry access.\n-        changed_field_labels = _get_changed_field_labels_from_form(form, changed_data)\n-\n     change_message = []\n     if add:\n-        change_message.append({'added': {}})\n-    elif form.changed_data:\n-        change_message.append({'changed': {'fields': changed_field_labels}})\n+        change_message.append({'added': True})\n+    else:\n+        if form is not None:\n+            changed_data = getattr(form, 'changed_data', [])\n+            if changed_data:\n+                change_message.append({'changed': list(changed_data)})\n     if formsets:\n-        with translation_override(None):\n-            for formset in formsets:\n-                for added_object in formset.new_objects:\n-                    change_message.append({\n-                        'added': {\n-                            'name': str(added_object._meta.verbose_name),\n-                            'object': str(added_object),\n-                        }\n-                    })\n-                for changed_object, changed_fields in formset.changed_objects:\n-                    change_message.append({\n-                        'changed': {\n-                            'name': str(changed_object._meta.verbose_name),\n-                            'object': str(changed_object),\n-                            'fields': _get_changed_field_labels_from_form(formset.forms[0], changed_fields),\n-                        }\n-                    })\n-                for deleted_object in formset.deleted_objects:\n-                    change_message.append({\n-                        'deleted': {\n-                            'name': str(deleted_object._meta.verbose_name),\n-                            'object': str(deleted_object),\n-                        }\n-                    })\n-    return change_message\n-\n-\n-def _get_changed_field_labels_from_form(form, changed_data):\n-    changed_field_labels = []\n-    for field_name in changed_data:\n-        try:\n-            verbose_field_name = form.fields[field_name].label or field_name\n-        except KeyError:\n-            verbose_field_name = field_name\n-        changed_field_labels.append(str(verbose_field_name))\n-    return changed_field_labels\n+        for fs in formsets:\n+            added = []\n+            changed = []\n+            deleted = []\n+            for f in getattr(fs, 'new_objects', []):\n+                added.append(str(getattr(f, '__str__', lambda: f)()))\n+            for f in getattr(fs, 'changed_objects', []):\n+                changed.append(str(getattr(f, '__str__', lambda: f)()))\n+            for f in getattr(fs, 'deleted_objects', []):\n+                deleted.append(str(getattr(f, '__str__', lambda: f)()))\n+            if added or changed or deleted:\n+                msg = {}\n+                if added:\n+                    msg['added'] = added\n+                if changed:\n+                    msg['changed'] = changed\n+                if deleted:\n+                    msg['deleted'] = deleted\n+                change_message.append(msg)\n+    return str(change_message)\n"
  },
  "django__django-12325": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-12325",
    "model_patch": "diff --git a/django/db/models/base.py b/django/db/models/base.py\nindex 8ea6c05ef9..8ec461d7ab 100644\n--- a/django/db/models/base.py\n+++ b/django/db/models/base.py\n@@ -204,7 +204,12 @@ class ModelBase(type):\n             for field in base._meta.local_fields:\n                 if isinstance(field, OneToOneField):\n                     related = resolve_relation(new_class, field.remote_field.model)\n-                    parent_links[make_model_tuple(related)] = field\n+                    key = make_model_tuple(related)\n+                    # Prefer explicit parent_link=True fields over other OneToOneFields\n+                    if getattr(field.remote_field, 'parent_link', False):\n+                        parent_links[key] = field\n+                    else:\n+                        parent_links.setdefault(key, field)\n \n         # Track fields inherited from base models.\n         inherited_attributes = set()\n"
  },
  "django__django-12406": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-12406",
    "model_patch": "diff --git a/django/forms/widgets.py b/django/forms/widgets.py\nindex 40ac1d3162..136c675a22 100644\n--- a/django/forms/widgets.py\n+++ b/django/forms/widgets.py\n@@ -755,6 +755,11 @@ class SelectMultiple(Select):\n \n \n class RadioSelect(ChoiceWidget):\n+    def format_value(self, value):\n+        \"\"\"Return selected values as a list. For radio widgets, None means no selection.\"\"\"\n+        if value is None:\n+            return []\n+        return super().format_value(value)\n     input_type = 'radio'\n     template_name = 'django/forms/widgets/radio.html'\n     option_template_name = 'django/forms/widgets/radio_option.html'\n"
  },
  "django__django-12708": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-12708",
    "model_patch": "diff --git a/django/db/backends/base/schema.py b/django/db/backends/base/schema.py\nindex 61f88401ab..4d5884df89 100644\n--- a/django/db/backends/base/schema.py\n+++ b/django/db/backends/base/schema.py\n@@ -403,9 +403,20 @@ class BaseDatabaseSchemaEditor:\n         meta_constraint_names = {constraint.name for constraint in model._meta.constraints}\n         meta_index_names = {constraint.name for constraint in model._meta.indexes}\n         columns = [model._meta.get_field(field).column for field in fields]\n+        # Avoid ambiguous matches where a UNIQUE constraint may also be reported\n+        # by the database introspection as an index (both unique=True and index=True).\n+        # When we're explicitly deleting an index, prefer non-unique indexes; when\n+        # deleting a unique constraint, prefer non-index uniques. This reduces the\n+        # chance of matching both a UNIQUE constraint and its underlying index.\n+        kwargs = constraint_kwargs.copy()\n+        if kwargs.get('index') is True and 'unique' not in kwargs:\n+            kwargs['unique'] = False\n+        if kwargs.get('unique') is True and 'index' not in kwargs:\n+            kwargs['index'] = False\n+\n         constraint_names = self._constraint_names(\n             model, columns, exclude=meta_constraint_names | meta_index_names,\n-            **constraint_kwargs\n+            **kwargs\n         )\n         if len(constraint_names) != 1:\n             raise ValueError(\"Found wrong number (%s) of constraints for %s(%s)\" % (\n"
  },
  "django__django-12713": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-12713",
    "model_patch": "diff --git a/django/contrib/admin/options.py b/django/contrib/admin/options.py\nindex 69a5fbf389..3fa9a1216c 100644\n--- a/django/contrib/admin/options.py\n+++ b/django/contrib/admin/options.py\n@@ -224,7 +224,7 @@ class BaseModelAdmin(metaclass=forms.MediaDefiningClass):\n         if 'widget' not in kwargs:\n             if db_field.name in self.get_autocomplete_fields(request):\n                 kwargs['widget'] = AutocompleteSelect(db_field.remote_field, self.admin_site, using=db)\n-            elif db_field.name in self.raw_id_fields:\n+            elif 'widget' not in kwargs and db_field.name in self.raw_id_fields:\n                 kwargs['widget'] = widgets.ForeignKeyRawIdWidget(db_field.remote_field, self.admin_site, using=db)\n             elif db_field.name in self.radio_fields:\n                 kwargs['widget'] = widgets.AdminRadioSelect(attrs={\n@@ -250,11 +250,11 @@ class BaseModelAdmin(metaclass=forms.MediaDefiningClass):\n         db = kwargs.get('using')\n \n         autocomplete_fields = self.get_autocomplete_fields(request)\n-        if db_field.name in autocomplete_fields:\n+        if 'widget' not in kwargs and db_field.name in autocomplete_fields:\n             kwargs['widget'] = AutocompleteSelectMultiple(db_field.remote_field, self.admin_site, using=db)\n-        elif db_field.name in self.raw_id_fields:\n+        elif 'widget' not in kwargs and db_field.name in self.raw_id_fields:\n             kwargs['widget'] = widgets.ManyToManyRawIdWidget(db_field.remote_field, self.admin_site, using=db)\n-        elif db_field.name in [*self.filter_vertical, *self.filter_horizontal]:\n+        elif 'widget' not in kwargs and db_field.name in [*self.filter_vertical, *self.filter_horizontal]:\n             kwargs['widget'] = widgets.FilteredSelectMultiple(\n                 db_field.verbose_name,\n                 db_field.name in self.filter_vertical\n"
  },
  "django__django-12774": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-12774",
    "model_patch": "diff --git a/django/db/models/query.py b/django/db/models/query.py\nindex d9c9b0db04..779440ac81 100644\n--- a/django/db/models/query.py\n+++ b/django/db/models/query.py\n@@ -689,8 +689,19 @@ class QuerySet:\n         \"\"\"\n         assert not self.query.is_sliced, \\\n             \"Cannot use 'limit' or 'offset' with in_bulk\"\n-        if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:\n-            raise ValueError(\"in_bulk()'s field_name must be a unique field but %r isn't.\" % field_name)\n+        if field_name != 'pk':\n+            field = self.model._meta.get_field(field_name)\n+            is_unique_field = field.unique\n+            # Consider a field unique if there is a UniqueConstraint on that single field.\n+            if not is_unique_field:\n+                for constraint in getattr(self.model._meta, 'constraints', ()):\n+                    # Import here to avoid circular imports at module load time\n+                    from django.db import models as _models\n+                    if isinstance(constraint, _models.UniqueConstraint) and tuple(constraint.fields) == (field_name,):\n+                        is_unique_field = True\n+                        break\n+            if not is_unique_field:\n+                raise ValueError(\"in_bulk()'s field_name must be a unique field but %r isn't.\" % field_name)\n         if id_list is not None:\n             if not id_list:\n                 return {}\n"
  },
  "django__django-9296": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-9296",
    "model_patch": "diff --git a/django/core/paginator.py b/django/core/paginator.py\nindex 1f92da5d4f..896535d89a 100644\n--- a/django/core/paginator.py\n+++ b/django/core/paginator.py\n@@ -106,6 +106,10 @@ class Paginator:\n         \"\"\"\n         return range(1, self.num_pages + 1)\n \n+    def __iter__(self):\n+        for page_num in self.page_range:\n+            yield self.page(page_num)\n+\n     def _check_object_list_is_ordered(self):\n         \"\"\"\n         Warn if self.object_list is unordered (typically a QuerySet).\n"
  },
  "sphinx-doc__sphinx-10323": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-10323",
    "model_patch": "diff --git a/sphinx/directives/code.py b/sphinx/directives/code.py\nindex dc44ed314..9437fe9a5 100644\n--- a/sphinx/directives/code.py\n+++ b/sphinx/directives/code.py\n@@ -224,9 +224,9 @@ class LiteralIncludeReader:\n                        self.start_filter,\n                        self.end_filter,\n                        self.lines_filter,\n+                       self.dedent_filter,\n                        self.prepend_filter,\n-                       self.append_filter,\n-                       self.dedent_filter]\n+                       self.append_filter]\n             lines = self.read_file(self.filename, location=location)\n             for func in filters:\n                 lines = func(lines, location=location)\n"
  },
  "sphinx-doc__sphinx-10435": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-10435",
    "model_patch": "diff --git a/sphinx/writers/latex.py b/sphinx/writers/latex.py\nindex e0c7d56f8..e69de29bb 100644\n--- a/sphinx/writers/latex.py\n+++ b/sphinx/writers/latex.py\n@@ -1,2061 +0,0 @@\n-\"\"\"Custom docutils writer for LaTeX.\n-\n-Much of this code is adapted from Dave Kuhlman's \"docpy\" writer from his\n-docutils sandbox.\n-\"\"\"\n-\n-import re\n-import warnings\n-from collections import defaultdict\n-from os import path\n-from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Set, Tuple, cast\n-\n-from docutils import nodes, writers\n-from docutils.nodes import Element, Node, Text\n-\n-from sphinx import addnodes, highlighting\n-from sphinx.deprecation import RemovedInSphinx70Warning\n-from sphinx.domains import IndexEntry\n-from sphinx.domains.std import StandardDomain\n-from sphinx.errors import SphinxError\n-from sphinx.locale import _, __, admonitionlabels\n-from sphinx.util import logging, split_into, texescape\n-from sphinx.util.docutils import SphinxTranslator\n-from sphinx.util.nodes import clean_astext, get_prev_node\n-from sphinx.util.template import LaTeXRenderer\n-from sphinx.util.texescape import tex_replace_map\n-\n-try:\n-    from docutils.utils.roman import toRoman\n-except ImportError:\n-    # In Debian/Ubuntu, roman package is provided as roman, not as docutils.utils.roman\n-    from roman import toRoman  # type: ignore\n-\n-if TYPE_CHECKING:\n-    from sphinx.builders.latex import LaTeXBuilder\n-    from sphinx.builders.latex.theming import Theme\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-MAX_CITATION_LABEL_LENGTH = 8\n-LATEXSECTIONNAMES = [\"part\", \"chapter\", \"section\", \"subsection\",\n-                     \"subsubsection\", \"paragraph\", \"subparagraph\"]\n-ENUMERATE_LIST_STYLE = defaultdict(lambda: r'\\arabic',\n-                                   {\n-                                       'arabic': r'\\arabic',\n-                                       'loweralpha': r'\\alph',\n-                                       'upperalpha': r'\\Alph',\n-                                       'lowerroman': r'\\roman',\n-                                       'upperroman': r'\\Roman',\n-                                   })\n-\n-CR = '\\n'\n-BLANKLINE = '\\n\\n'\n-EXTRA_RE = re.compile(r'^(.*\\S)\\s+\\(([^()]*)\\)\\s*$')\n-\n-\n-class collected_footnote(nodes.footnote):\n-    \"\"\"Footnotes that are collected are assigned this class.\"\"\"\n-\n-\n-class UnsupportedError(SphinxError):\n-    category = 'Markup is unsupported in LaTeX'\n-\n-\n-class LaTeXWriter(writers.Writer):\n-\n-    supported = ('sphinxlatex',)\n-\n-    settings_spec = ('LaTeX writer options', '', (\n-        ('Document name', ['--docname'], {'default': ''}),\n-        ('Document class', ['--docclass'], {'default': 'manual'}),\n-        ('Author', ['--author'], {'default': ''}),\n-    ))\n-    settings_defaults: Dict = {}\n-\n-    output = None\n-\n-    def __init__(self, builder: \"LaTeXBuilder\") -> None:\n-        super().__init__()\n-        self.builder = builder\n-        self.theme: Theme = None\n-\n-    def translate(self) -> None:\n-        visitor = self.builder.create_translator(self.document, self.builder, self.theme)\n-        self.document.walkabout(visitor)\n-        self.output = cast(LaTeXTranslator, visitor).astext()\n-\n-\n-# Helper classes\n-\n-class Table:\n-    \"\"\"A table data\"\"\"\n-\n-    def __init__(self, node: Element) -> None:\n-        self.header: List[str] = []\n-        self.body: List[str] = []\n-        self.align = node.get('align', 'default')\n-        self.classes: List[str] = node.get('classes', [])\n-        self.colcount = 0\n-        self.colspec: str = None\n-        self.colwidths: List[int] = []\n-        self.has_problematic = False\n-        self.has_oldproblematic = False\n-        self.has_verbatim = False\n-        self.caption: List[str] = None\n-        self.stubs: List[int] = []\n-\n-        # current position\n-        self.col = 0\n-        self.row = 0\n-\n-        # A dict mapping a table location to a cell_id (cell = rectangular area)\n-        self.cells: Dict[Tuple[int, int], int] = defaultdict(int)\n-        self.cell_id = 0  # last assigned cell_id\n-\n-    def is_longtable(self) -> bool:\n-        \"\"\"True if and only if table uses longtable environment.\"\"\"\n-        return self.row > 30 or 'longtable' in self.classes\n-\n-    def get_table_type(self) -> str:\n-        \"\"\"Returns the LaTeX environment name for the table.\n-\n-        The class currently supports:\n-\n-        * longtable\n-        * tabular\n-        * tabulary\n-        \"\"\"\n-        if self.is_longtable():\n-            return 'longtable'\n-        elif self.has_verbatim:\n-            return 'tabular'\n-        elif self.colspec:\n-            return 'tabulary'\n-        elif self.has_problematic or (self.colwidths and 'colwidths-given' in self.classes):\n-            return 'tabular'\n-        else:\n-            return 'tabulary'\n-\n-    def get_colspec(self) -> str:\n-        \"\"\"Returns a column spec of table.\n-\n-        This is what LaTeX calls the 'preamble argument' of the used table environment.\n-\n-        .. note:: the ``\\\\X`` and ``T`` column type specifiers are defined in ``sphinx.sty``.\n-        \"\"\"\n-        if self.colspec:\n-            return self.colspec\n-        elif self.colwidths and 'colwidths-given' in self.classes:\n-            total = sum(self.colwidths)\n-            colspecs = [r'\\X{%d}{%d}' % (width, total) for width in self.colwidths]\n-            return '{|%s|}' % '|'.join(colspecs) + CR\n-        elif self.has_problematic:\n-            return r'{|*{%d}{\\X{1}{%d}|}}' % (self.colcount, self.colcount) + CR\n-        elif self.get_table_type() == 'tabulary':\n-            # sphinx.sty sets T to be J by default.\n-            return '{|' + ('T|' * self.colcount) + '}' + CR\n-        elif self.has_oldproblematic:\n-            return r'{|*{%d}{\\X{1}{%d}|}}' % (self.colcount, self.colcount) + CR\n-        else:\n-            return '{|' + ('l|' * self.colcount) + '}' + CR\n-\n-    def add_cell(self, height: int, width: int) -> None:\n-        \"\"\"Adds a new cell to a table.\n-\n-        It will be located at current position: (``self.row``, ``self.col``).\n-        \"\"\"\n-        self.cell_id += 1\n-        for col in range(width):\n-            for row in range(height):\n-                assert self.cells[(self.row + row, self.col + col)] == 0\n-                self.cells[(self.row + row, self.col + col)] = self.cell_id\n-\n-    def cell(self, row: int = None, col: int = None) -> \"TableCell\":\n-        \"\"\"Returns a cell object (i.e. rectangular area) containing given position.\n-\n-        If no option arguments: ``row`` or ``col`` are given, the current position;\n-        ``self.row`` and ``self.col`` are used to get a cell object by default.\n-        \"\"\"\n-        try:\n-            if row is None:\n-                row = self.row\n-            if col is None:\n-                col = self.col\n-            return TableCell(self, row, col)\n-        except IndexError:\n-            return None\n-\n-\n-class TableCell:\n-    \"\"\"Data of a cell in a table.\"\"\"\n-\n-    def __init__(self, table: Table, row: int, col: int) -> None:\n-        if table.cells[(row, col)] == 0:\n-            raise IndexError\n-\n-        self.table = table\n-        self.cell_id = table.cells[(row, col)]\n-        self.row = row\n-        self.col = col\n-\n-        # adjust position for multirow/multicol cell\n-        while table.cells[(self.row - 1, self.col)] == self.cell_id:\n-            self.row -= 1\n-        while table.cells[(self.row, self.col - 1)] == self.cell_id:\n-            self.col -= 1\n-\n-    @property\n-    def width(self) -> int:\n-        \"\"\"Returns the cell width.\"\"\"\n-        width = 0\n-        while self.table.cells[(self.row, self.col + width)] == self.cell_id:\n-            width += 1\n-        return width\n-\n-    @property\n-    def height(self) -> int:\n-        \"\"\"Returns the cell height.\"\"\"\n-        height = 0\n-        while self.table.cells[(self.row + height, self.col)] == self.cell_id:\n-            height += 1\n-        return height\n-\n-\n-def escape_abbr(text: str) -> str:\n-    \"\"\"Adjust spacing after abbreviations.\"\"\"\n-    return re.sub(r'\\.(?=\\s|$)', r'.\\@', text)\n-\n-\n-def rstdim_to_latexdim(width_str: str, scale: int = 100) -> str:\n-    \"\"\"Convert `width_str` with rst length to LaTeX length.\"\"\"\n-    match = re.match(r'^(\\d*\\.?\\d*)\\s*(\\S*)$', width_str)\n-    if not match:\n-        raise ValueError\n-    res = width_str\n-    amount, unit = match.groups()[:2]\n-    if scale == 100:\n-        float(amount)  # validate amount is float\n-        if unit in ('', \"px\"):\n-            res = r\"%s\\sphinxpxdimen\" % amount\n-        elif unit == 'pt':\n-            res = '%sbp' % amount  # convert to 'bp'\n-        elif unit == \"%\":\n-            res = r\"%.3f\\linewidth\" % (float(amount) / 100.0)\n-    else:\n-        amount_float = float(amount) * scale / 100.0\n-        if unit in ('', \"px\"):\n-            res = r\"%.5f\\sphinxpxdimen\" % amount_float\n-        elif unit == 'pt':\n-            res = '%.5fbp' % amount_float\n-        elif unit == \"%\":\n-            res = r\"%.5f\\linewidth\" % (amount_float / 100.0)\n-        else:\n-            res = \"%.5f%s\" % (amount_float, unit)\n-    return res\n-\n-\n-class LaTeXTranslator(SphinxTranslator):\n-    builder: \"LaTeXBuilder\" = None\n-\n-    secnumdepth = 2  # legacy sphinxhowto.cls uses this, whereas article.cls\n-    # default is originally 3. For book/report, 2 is already LaTeX default.\n-    ignore_missing_images = False\n-\n-    def __init__(self, document: nodes.document, builder: \"LaTeXBuilder\",\n-                 theme: \"Theme\") -> None:\n-        super().__init__(document, builder)\n-        self.body: List[str] = []\n-        self.theme = theme\n-\n-        # flags\n-        self.in_title = 0\n-        self.in_production_list = 0\n-        self.in_footnote = 0\n-        self.in_caption = 0\n-        self.in_term = 0\n-        self.needs_linetrimming = 0\n-        self.in_minipage = 0\n-        self.no_latex_floats = 0\n-        self.first_document = 1\n-        self.this_is_the_title = 1\n-        self.literal_whitespace = 0\n-        self.in_parsed_literal = 0\n-        self.compact_list = 0\n-        self.first_param = 0\n-        self.in_desc_signature = False\n-\n-        sphinxpkgoptions = []\n-\n-        # sort out some elements\n-        self.elements = self.builder.context.copy()\n-\n-        # initial section names\n-        self.sectionnames = LATEXSECTIONNAMES[:]\n-        if self.theme.toplevel_sectioning == 'section':\n-            self.sectionnames.remove('chapter')\n-\n-        # determine top section level\n-        self.top_sectionlevel = 1\n-        if self.config.latex_toplevel_sectioning:\n-            try:\n-                self.top_sectionlevel = \\\n-                    self.sectionnames.index(self.config.latex_toplevel_sectioning)\n-            except ValueError:\n-                logger.warning(__('unknown %r toplevel_sectioning for class %r') %\n-                               (self.config.latex_toplevel_sectioning, self.theme.docclass))\n-\n-        if self.config.numfig:\n-            self.numfig_secnum_depth = self.config.numfig_secnum_depth\n-            if self.numfig_secnum_depth > 0:  # default is 1\n-                # numfig_secnum_depth as passed to sphinx.sty indices same names as in\n-                # LATEXSECTIONNAMES but with -1 for part, 0 for chapter, 1 for section...\n-                if len(self.sectionnames) < len(LATEXSECTIONNAMES) and \\\n-                   self.top_sectionlevel > 0:\n-                    self.numfig_secnum_depth += self.top_sectionlevel\n-                else:\n-                    self.numfig_secnum_depth += self.top_sectionlevel - 1\n-                # this (minus one) will serve as minimum to LaTeX's secnumdepth\n-                self.numfig_secnum_depth = min(self.numfig_secnum_depth,\n-                                               len(LATEXSECTIONNAMES) - 1)\n-                # if passed key value is < 1 LaTeX will act as if 0; see sphinx.sty\n-                sphinxpkgoptions.append('numfigreset=%s' % self.numfig_secnum_depth)\n-            else:\n-                sphinxpkgoptions.append('nonumfigreset')\n-\n-        if self.config.numfig and self.config.math_numfig:\n-            sphinxpkgoptions.append('mathnumfig')\n-\n-        if (self.config.language not in {'en', 'ja'} and\n-                'fncychap' not in self.config.latex_elements):\n-            # use Sonny style if any language specified (except English)\n-            self.elements['fncychap'] = (r'\\usepackage[Sonny]{fncychap}' + CR +\n-                                         r'\\ChNameVar{\\Large\\normalfont\\sffamily}' + CR +\n-                                         r'\\ChTitleVar{\\Large\\normalfont\\sffamily}')\n-\n-        self.babel = self.builder.babel\n-        if not self.babel.is_supported_language():\n-            # emit warning if specified language is invalid\n-            # (only emitting, nothing changed to processing)\n-            logger.warning(__('no Babel option known for language %r'),\n-                           self.config.language)\n-\n-        minsecnumdepth = self.secnumdepth  # 2 from legacy sphinx manual/howto\n-        if self.document.get('tocdepth'):\n-            # reduce tocdepth if `part` or `chapter` is used for top_sectionlevel\n-            #   tocdepth = -1: show only parts\n-            #   tocdepth =  0: show parts and chapters\n-            #   tocdepth =  1: show parts, chapters and sections\n-            #   tocdepth =  2: show parts, chapters, sections and subsections\n-            #   ...\n-            tocdepth = self.document.get('tocdepth', 999) + self.top_sectionlevel - 2\n-            if len(self.sectionnames) < len(LATEXSECTIONNAMES) and \\\n-               self.top_sectionlevel > 0:\n-                tocdepth += 1  # because top_sectionlevel is shifted by -1\n-            if tocdepth > len(LATEXSECTIONNAMES) - 2:  # default is 5 <-> subparagraph\n-                logger.warning(__('too large :maxdepth:, ignored.'))\n-                tocdepth = len(LATEXSECTIONNAMES) - 2\n-\n-            self.elements['tocdepth'] = r'\\setcounter{tocdepth}{%d}' % tocdepth\n-            minsecnumdepth = max(minsecnumdepth, tocdepth)\n-\n-        if self.config.numfig and (self.config.numfig_secnum_depth > 0):\n-            minsecnumdepth = max(minsecnumdepth, self.numfig_secnum_depth - 1)\n-\n-        if minsecnumdepth > self.secnumdepth:\n-            self.elements['secnumdepth'] = r'\\setcounter{secnumdepth}{%d}' %\\\n-                                           minsecnumdepth\n-\n-        contentsname = document.get('contentsname')\n-        if contentsname:\n-            self.elements['contentsname'] = self.babel_renewcommand(r'\\contentsname',\n-                                                                    contentsname)\n-\n-        if self.elements['maxlistdepth']:\n-            sphinxpkgoptions.append('maxlistdepth=%s' % self.elements['maxlistdepth'])\n-        if sphinxpkgoptions:\n-            self.elements['sphinxpkgoptions'] = '[,%s]' % ','.join(sphinxpkgoptions)\n-        if self.elements['sphinxsetup']:\n-            self.elements['sphinxsetup'] = (r'\\sphinxsetup{%s}' % self.elements['sphinxsetup'])\n-        if self.elements['extraclassoptions']:\n-            self.elements['classoptions'] += ',' + \\\n-                                             self.elements['extraclassoptions']\n-\n-        self.highlighter = highlighting.PygmentsBridge('latex', self.config.pygments_style,\n-                                                       latex_engine=self.config.latex_engine)\n-        self.context: List[Any] = []\n-        self.descstack: List[str] = []\n-        self.tables: List[Table] = []\n-        self.next_table_colspec: str = None\n-        self.bodystack: List[List[str]] = []\n-        self.footnote_restricted: Element = None\n-        self.pending_footnotes: List[nodes.footnote_reference] = []\n-        self.curfilestack: List[str] = []\n-        self.handled_abbrs: Set[str] = set()\n-\n-    def pushbody(self, newbody: List[str]) -> None:\n-        self.bodystack.append(self.body)\n-        self.body = newbody\n-\n-    def popbody(self) -> List[str]:\n-        body = self.body\n-        self.body = self.bodystack.pop()\n-        return body\n-\n-    def astext(self) -> str:\n-        self.elements.update({\n-            'body': ''.join(self.body),\n-            'indices': self.generate_indices()\n-        })\n-        return self.render('latex.tex_t', self.elements)\n-\n-    def hypertarget(self, id: str, withdoc: bool = True, anchor: bool = True) -> str:\n-        if withdoc:\n-            id = self.curfilestack[-1] + ':' + id\n-        return (r'\\phantomsection' if anchor else '') + r'\\label{%s}' % self.idescape(id)\n-\n-    def hypertarget_to(self, node: Element, anchor: bool = False) -> str:\n-        labels = ''.join(self.hypertarget(node_id, anchor=False) for node_id in node['ids'])\n-        if anchor:\n-            return r'\\phantomsection' + labels\n-        else:\n-            return labels\n-\n-    def hyperlink(self, id: str) -> str:\n-        return r'{\\hyperref[%s]{' % self.idescape(id)\n-\n-    def hyperpageref(self, id: str) -> str:\n-        return r'\\autopageref*{%s}' % self.idescape(id)\n-\n-    def escape(self, s: str) -> str:\n-        return texescape.escape(s, self.config.latex_engine)\n-\n-    def idescape(self, id: str) -> str:\n-        return r'\\detokenize{%s}' % str(id).translate(tex_replace_map).\\\n-            encode('ascii', 'backslashreplace').decode('ascii').\\\n-            replace('\\\\', '_')\n-\n-    def babel_renewcommand(self, command: str, definition: str) -> str:\n-        if self.elements['multilingual']:\n-            prefix = r'\\addto\\captions%s{' % self.babel.get_language()\n-            suffix = '}'\n-        else:  # babel is disabled (mainly for Japanese environment)\n-            prefix = ''\n-            suffix = ''\n-\n-        return r'%s\\renewcommand{%s}{%s}%s' % (prefix, command, definition, suffix) + CR\n-\n-    def generate_indices(self) -> str:\n-        def generate(content: List[Tuple[str, List[IndexEntry]]], collapsed: bool) -> None:\n-            ret.append(r'\\begin{sphinxtheindex}' + CR)\n-            ret.append(r'\\let\\bigletter\\sphinxstyleindexlettergroup' + CR)\n-            for i, (letter, entries) in enumerate(content):\n-                if i > 0:\n-                    ret.append(r'\\indexspace' + CR)\n-                ret.append(r'\\bigletter{%s}' % self.escape(letter) + CR)\n-                for entry in entries:\n-                    if not entry[3]:\n-                        continue\n-                    ret.append(r'\\item\\relax\\sphinxstyleindexentry{%s}' %\n-                               self.encode(entry[0]))\n-                    if entry[4]:\n-                        # add \"extra\" info\n-                        ret.append(r'\\sphinxstyleindexextra{%s}' % self.encode(entry[4]))\n-                    ret.append(r'\\sphinxstyleindexpageref{%s:%s}' %\n-                               (entry[2], self.idescape(entry[3])) + CR)\n-            ret.append(r'\\end{sphinxtheindex}' + CR)\n-\n-        ret = []\n-        # latex_domain_indices can be False/True or a list of index names\n-        indices_config = self.config.latex_domain_indices\n-        if indices_config:\n-            for domain in self.builder.env.domains.values():\n-                for indexcls in domain.indices:\n-                    indexname = '%s-%s' % (domain.name, indexcls.name)\n-                    if isinstance(indices_config, list):\n-                        if indexname not in indices_config:\n-                            continue\n-                    content, collapsed = indexcls(domain).generate(\n-                        self.builder.docnames)\n-                    if not content:\n-                        continue\n-                    ret.append(r'\\renewcommand{\\indexname}{%s}' % indexcls.localname + CR)\n-                    generate(content, collapsed)\n-\n-        return ''.join(ret)\n-\n-    def render(self, template_name: str, variables: Dict) -> str:\n-        renderer = LaTeXRenderer(latex_engine=self.config.latex_engine)\n-        for template_dir in self.config.templates_path:\n-            template = path.join(self.builder.confdir, template_dir,\n-                                 template_name)\n-            if path.exists(template):\n-                return renderer.render(template, variables)\n-\n-        return renderer.render(template_name, variables)\n-\n-    @property\n-    def table(self) -> Table:\n-        \"\"\"Get current table.\"\"\"\n-        if self.tables:\n-            return self.tables[-1]\n-        else:\n-            return None\n-\n-    def visit_document(self, node: Element) -> None:\n-        self.curfilestack.append(node.get('docname', ''))\n-        if self.first_document == 1:\n-            # the first document is all the regular content ...\n-            self.first_document = 0\n-        elif self.first_document == 0:\n-            # ... and all others are the appendices\n-            self.body.append(CR + r'\\appendix' + CR)\n-            self.first_document = -1\n-        if 'docname' in node:\n-            self.body.append(self.hypertarget(':doc'))\n-        # \"- 1\" because the level is increased before the title is visited\n-        self.sectionlevel = self.top_sectionlevel - 1\n-\n-    def depart_document(self, node: Element) -> None:\n-        pass\n-\n-    def visit_start_of_file(self, node: Element) -> None:\n-        self.curfilestack.append(node['docname'])\n-        self.body.append(CR + r'\\sphinxstepscope' + CR)\n-\n-    def depart_start_of_file(self, node: Element) -> None:\n-        self.curfilestack.pop()\n-\n-    def visit_section(self, node: Element) -> None:\n-        if not self.this_is_the_title:\n-            self.sectionlevel += 1\n-        self.body.append(BLANKLINE)\n-\n-    def depart_section(self, node: Element) -> None:\n-        self.sectionlevel = max(self.sectionlevel - 1,\n-                                self.top_sectionlevel - 1)\n-\n-    def visit_problematic(self, node: Element) -> None:\n-        self.body.append(r'{\\color{red}\\bfseries{}')\n-\n-    def depart_problematic(self, node: Element) -> None:\n-        self.body.append('}')\n-\n-    def visit_topic(self, node: Element) -> None:\n-        self.in_minipage = 1\n-        self.body.append(CR + r'\\begin{sphinxShadowBox}' + CR)\n-\n-    def depart_topic(self, node: Element) -> None:\n-        self.in_minipage = 0\n-        self.body.append(r'\\end{sphinxShadowBox}' + CR)\n-    visit_sidebar = visit_topic\n-    depart_sidebar = depart_topic\n-\n-    def visit_glossary(self, node: Element) -> None:\n-        pass\n-\n-    def depart_glossary(self, node: Element) -> None:\n-        pass\n-\n-    def visit_productionlist(self, node: Element) -> None:\n-        self.body.append(BLANKLINE)\n-        self.body.append(r'\\begin{productionlist}' + CR)\n-        self.in_production_list = 1\n-\n-    def depart_productionlist(self, node: Element) -> None:\n-        self.body.append(r'\\end{productionlist}' + BLANKLINE)\n-        self.in_production_list = 0\n-\n-    def visit_production(self, node: Element) -> None:\n-        if node['tokenname']:\n-            tn = node['tokenname']\n-            self.body.append(self.hypertarget('grammar-token-' + tn))\n-            self.body.append(r'\\production{%s}{' % self.encode(tn))\n-        else:\n-            self.body.append(r'\\productioncont{')\n-\n-    def depart_production(self, node: Element) -> None:\n-        self.body.append('}' + CR)\n-\n-    def visit_transition(self, node: Element) -> None:\n-        self.body.append(self.elements['transition'])\n-\n-    def depart_transition(self, node: Element) -> None:\n-        pass\n-\n-    def visit_title(self, node: Element) -> None:\n-        parent = node.parent\n-        if isinstance(parent, addnodes.seealso):\n-            # the environment already handles this\n-            raise nodes.SkipNode\n-        elif isinstance(parent, nodes.section):\n-            if self.this_is_the_title:\n-                if len(node.children) != 1 and not isinstance(node.children[0],\n-                                                              nodes.Text):\n-                    logger.warning(__('document title is not a single Text node'),\n-                                   location=node)\n-                if not self.elements['title']:\n-                    # text needs to be escaped since it is inserted into\n-                    # the output literally\n-                    self.elements['title'] = self.escape(node.astext())\n-                self.this_is_the_title = 0\n-                raise nodes.SkipNode\n-            else:\n-                short = ''\n-                if any(node.findall(nodes.image)):\n-                    short = ('[%s]' % self.escape(' '.join(clean_astext(node).split())))\n-\n-                try:\n-                    self.body.append(r'\\%s%s{' % (self.sectionnames[self.sectionlevel], short))\n-                except IndexError:\n-                    # just use \"subparagraph\", it's not numbered anyway\n-                    self.body.append(r'\\%s%s{' % (self.sectionnames[-1], short))\n-                self.context.append('}' + CR + self.hypertarget_to(node.parent))\n-        elif isinstance(parent, nodes.topic):\n-            self.body.append(r'\\sphinxstyletopictitle{')\n-            self.context.append('}' + CR)\n-        elif isinstance(parent, nodes.sidebar):\n-            self.body.append(r'\\sphinxstylesidebartitle{')\n-            self.context.append('}' + CR)\n-        elif isinstance(parent, nodes.Admonition):\n-            self.body.append('{')\n-            self.context.append('}' + CR)\n-        elif isinstance(parent, nodes.table):\n-            # Redirect body output until title is finished.\n-            self.pushbody([])\n-        else:\n-            logger.warning(__('encountered title node not in section, topic, table, '\n-                              'admonition or sidebar'),\n-                           location=node)\n-            self.body.append(r'\\sphinxstyleothertitle{')\n-            self.context.append('}' + CR)\n-        self.in_title = 1\n-\n-    def depart_title(self, node: Element) -> None:\n-        self.in_title = 0\n-        if isinstance(node.parent, nodes.table):\n-            self.table.caption = self.popbody()\n-        else:\n-            self.body.append(self.context.pop())\n-\n-    def visit_subtitle(self, node: Element) -> None:\n-        if isinstance(node.parent, nodes.sidebar):\n-            self.body.append(r'\\sphinxstylesidebarsubtitle{')\n-            self.context.append('}' + CR)\n-        else:\n-            self.context.append('')\n-\n-    def depart_subtitle(self, node: Element) -> None:\n-        self.body.append(self.context.pop())\n-\n-    #############################################################\n-    # Domain-specific object descriptions\n-    #############################################################\n-\n-    # Top-level nodes for descriptions\n-    ##################################\n-\n-    def visit_desc(self, node: Element) -> None:\n-        if self.config.latex_show_urls == 'footnote':\n-            self.body.append(BLANKLINE)\n-            self.body.append(r'\\begin{savenotes}\\begin{fulllineitems}' + CR)\n-        else:\n-            self.body.append(BLANKLINE)\n-            self.body.append(r'\\begin{fulllineitems}' + CR)\n-        if self.table:\n-            self.table.has_problematic = True\n-\n-    def depart_desc(self, node: Element) -> None:\n-        if self.in_desc_signature:\n-            self.body.append(CR + r'\\pysigstopsignatures')\n-            self.in_desc_signature = False\n-        if self.config.latex_show_urls == 'footnote':\n-            self.body.append(CR + r'\\end{fulllineitems}\\end{savenotes}' + BLANKLINE)\n-        else:\n-            self.body.append(CR + r'\\end{fulllineitems}' + BLANKLINE)\n-\n-    def _visit_signature_line(self, node: Element) -> None:\n-        for child in node:\n-            if isinstance(child, addnodes.desc_parameterlist):\n-                self.body.append(CR + r'\\pysiglinewithargsret{')\n-                break\n-        else:\n-            self.body.append(CR + r'\\pysigline{')\n-\n-    def _depart_signature_line(self, node: Element) -> None:\n-        self.body.append('}')\n-\n-    def visit_desc_signature(self, node: Element) -> None:\n-        if node.parent['objtype'] != 'describe' and node['ids']:\n-            hyper = self.hypertarget(node['ids'][0])\n-        else:\n-            hyper = ''\n-        self.body.append(hyper)\n-        if not self.in_desc_signature:\n-            self.in_desc_signature = True\n-            self.body.append(CR + r'\\pysigstartsignatures')\n-        if not node.get('is_multiline'):\n-            self._visit_signature_line(node)\n-        else:\n-            self.body.append(CR + r'\\pysigstartmultiline')\n-\n-    def depart_desc_signature(self, node: Element) -> None:\n-        if not node.get('is_multiline'):\n-            self._depart_signature_line(node)\n-        else:\n-            self.body.append(CR + r'\\pysigstopmultiline')\n-\n-    def visit_desc_signature_line(self, node: Element) -> None:\n-        self._visit_signature_line(node)\n-\n-    def depart_desc_signature_line(self, node: Element) -> None:\n-        self._depart_signature_line(node)\n-\n-    def visit_desc_content(self, node: Element) -> None:\n-        assert self.in_desc_signature\n-        self.body.append(CR + r'\\pysigstopsignatures')\n-        self.in_desc_signature = False\n-\n-    def depart_desc_content(self, node: Element) -> None:\n-        pass\n-\n-    def visit_desc_inline(self, node: Element) -> None:\n-        self.body.append(r'\\sphinxcode{\\sphinxupquote{')\n-\n-    def depart_desc_inline(self, node: Element) -> None:\n-        self.body.append('}}')\n-\n-    # Nodes for high-level structure in signatures\n-    ##############################################\n-\n-    def visit_desc_name(self, node: Element) -> None:\n-        self.body.append(r'\\sphinxbfcode{\\sphinxupquote{')\n-        self.literal_whitespace += 1\n-\n-    def depart_desc_name(self, node: Element) -> None:\n-        self.body.append('}}')\n-        self.literal_whitespace -= 1\n-\n-    def visit_desc_addname(self, node: Element) -> None:\n-        self.body.append(r'\\sphinxcode{\\sphinxupquote{')\n-        self.literal_whitespace += 1\n-\n-    def depart_desc_addname(self, node: Element) -> None:\n-        self.body.append('}}')\n-        self.literal_whitespace -= 1\n-\n-    def visit_desc_type(self, node: Element) -> None:\n-        pass\n-\n-    def depart_desc_type(self, node: Element) -> None:\n-        pass\n-\n-    def visit_desc_returns(self, node: Element) -> None:\n-        self.body.append(r'{ $\\rightarrow$ ')\n-\n-    def depart_desc_returns(self, node: Element) -> None:\n-        self.body.append(r'}')\n-\n-    def visit_desc_parameterlist(self, node: Element) -> None:\n-        # close name, open parameterlist\n-        self.body.append('}{')\n-        self.first_param = 1\n-\n-    def depart_desc_parameterlist(self, node: Element) -> None:\n-        # close parameterlist, open return annotation\n-        self.body.append('}{')\n-\n-    def visit_desc_parameter(self, node: Element) -> None:\n-        if not self.first_param:\n-            self.body.append(', ')\n-        else:\n-            self.first_param = 0\n-        if not node.hasattr('noemph'):\n-            self.body.append(r'\\emph{')\n-\n-    def depart_desc_parameter(self, node: Element) -> None:\n-        if not node.hasattr('noemph'):\n-            self.body.append('}')\n-\n-    def visit_desc_optional(self, node: Element) -> None:\n-        self.body.append(r'\\sphinxoptional{')\n-\n-    def depart_desc_optional(self, node: Element) -> None:\n-        self.body.append('}')\n-\n-    def visit_desc_annotation(self, node: Element) -> None:\n-        self.body.append(r'\\sphinxbfcode{\\sphinxupquote{')\n-\n-    def depart_desc_annotation(self, node: Element) -> None:\n-        self.body.append('}}')\n-\n-    ##############################################\n-\n-    def visit_seealso(self, node: Element) -> None:\n-        self.body.append(BLANKLINE)\n-        self.body.append(r'\\sphinxstrong{%s:}' % admonitionlabels['seealso'] + CR)\n-        self.body.append(r'\\nopagebreak' + BLANKLINE)\n-\n-    def depart_seealso(self, node: Element) -> None:\n-        self.body.append(BLANKLINE)\n-\n-    def visit_rubric(self, node: Element) -> None:\n-        if len(node) == 1 and node.astext() in ('Footnotes', _('Footnotes')):\n-            raise nodes.SkipNode\n-        self.body.append(r'\\subsubsection*{')\n-        self.context.append('}' + CR)\n-        self.in_title = 1\n-\n-    def depart_rubric(self, node: Element) -> None:\n-        self.in_title = 0\n-        self.body.append(self.context.pop())\n-\n-    def visit_footnote(self, node: Element) -> None:\n-        self.in_footnote += 1\n-        label = cast(nodes.label, node[0])\n-        if self.in_parsed_literal:\n-            self.body.append(r'\\begin{footnote}[%s]' % label.astext())\n-        else:\n-            self.body.append('%' + CR)\n-            self.body.append(r'\\begin{footnote}[%s]' % label.astext())\n-        if 'referred' in node:\n-            # TODO: in future maybe output a latex macro with backrefs here\n-            pass\n-        self.body.append(r'\\sphinxAtStartFootnote' + CR)\n-\n-    def depart_footnote(self, node: Element) -> None:\n-        if self.in_parsed_literal:\n-            self.body.append(r'\\end{footnote}')\n-        else:\n-            self.body.append('%' + CR)\n-            self.body.append(r'\\end{footnote}')\n-        self.in_footnote -= 1\n-\n-    def visit_label(self, node: Element) -> None:\n-        raise nodes.SkipNode\n-\n-    def visit_tabular_col_spec(self, node: Element) -> None:\n-        self.next_table_colspec = node['spec']\n-        raise nodes.SkipNode\n-\n-    def visit_table(self, node: Element) -> None:\n-        if len(self.tables) == 1:\n-            if self.table.get_table_type() == 'longtable':\n-                raise UnsupportedError(\n-                    '%s:%s: longtable does not support nesting a table.' %\n-                    (self.curfilestack[-1], node.line or ''))\n-            else:\n-                # change type of parent table to tabular\n-                # see https://groups.google.com/d/msg/sphinx-users/7m3NeOBixeo/9LKP2B4WBQAJ\n-                self.table.has_problematic = True\n-        elif len(self.tables) > 2:\n-            raise UnsupportedError(\n-                '%s:%s: deeply nested tables are not implemented.' %\n-                (self.curfilestack[-1], node.line or ''))\n-\n-        self.tables.append(Table(node))\n-        if self.next_table_colspec:\n-            self.table.colspec = '{%s}' % self.next_table_colspec + CR\n-            if 'colwidths-given' in node.get('classes', []):\n-                logger.info(__('both tabularcolumns and :widths: option are given. '\n-                               ':widths: is ignored.'), location=node)\n-        self.next_table_colspec = None\n-\n-    def depart_table(self, node: Element) -> None:\n-        labels = self.hypertarget_to(node)\n-        table_type = self.table.get_table_type()\n-        table = self.render(table_type + '.tex_t',\n-                            dict(table=self.table, labels=labels))\n-        self.body.append(BLANKLINE)\n-        self.body.append(table)\n-        self.body.append(CR)\n-\n-        self.tables.pop()\n-\n-    def visit_colspec(self, node: Element) -> None:\n-        self.table.colcount += 1\n-        if 'colwidth' in node:\n-            self.table.colwidths.append(node['colwidth'])\n-        if 'stub' in node:\n-            self.table.stubs.append(self.table.colcount - 1)\n-\n-    def depart_colspec(self, node: Element) -> None:\n-        pass\n-\n-    def visit_tgroup(self, node: Element) -> None:\n-        pass\n-\n-    def depart_tgroup(self, node: Element) -> None:\n-        pass\n-\n-    def visit_thead(self, node: Element) -> None:\n-        # Redirect head output until header is finished.\n-        self.pushbody(self.table.header)\n-\n-    def depart_thead(self, node: Element) -> None:\n-        self.popbody()\n-\n-    def visit_tbody(self, node: Element) -> None:\n-        # Redirect body output until table is finished.\n-        self.pushbody(self.table.body)\n-\n-    def depart_tbody(self, node: Element) -> None:\n-        self.popbody()\n-\n-    def visit_row(self, node: Element) -> None:\n-        self.table.col = 0\n-\n-        # fill columns if the row starts with the bottom of multirow cell\n-        while True:\n-            cell = self.table.cell(self.table.row, self.table.col)\n-            if cell is None:  # not a bottom of multirow cell\n-                break\n-            else:  # a bottom of multirow cell\n-                self.table.col += cell.width\n-                if cell.col:\n-                    self.body.append('&')\n-                if cell.width == 1:\n-                    # insert suitable strut for equalizing row heights in given multirow\n-                    self.body.append(r'\\sphinxtablestrut{%d}' % cell.cell_id)\n-                else:  # use \\multicolumn for wide multirow cell\n-                    self.body.append(r'\\multicolumn{%d}{|l|}{\\sphinxtablestrut{%d}}' %\n-                                     (cell.width, cell.cell_id))\n-\n-    def depart_row(self, node: Element) -> None:\n-        self.body.append(r'\\\\' + CR)\n-        cells = [self.table.cell(self.table.row, i) for i in range(self.table.colcount)]\n-        underlined = [cell.row + cell.height == self.table.row + 1 for cell in cells]\n-        if all(underlined):\n-            self.body.append(r'\\hline')\n-        else:\n-            i = 0\n-            underlined.extend([False])  # sentinel\n-            while i < len(underlined):\n-                if underlined[i] is True:\n-                    j = underlined[i:].index(False)\n-                    self.body.append(r'\\cline{%d-%d}' % (i + 1, i + j))\n-                    i += j\n-                i += 1\n-        self.table.row += 1\n-\n-    def visit_entry(self, node: Element) -> None:\n-        if self.table.col > 0:\n-            self.body.append('&')\n-        self.table.add_cell(node.get('morerows', 0) + 1, node.get('morecols', 0) + 1)\n-        cell = self.table.cell()\n-        context = ''\n-        if cell.width > 1:\n-            if self.config.latex_use_latex_multicolumn:\n-                if self.table.col == 0:\n-                    self.body.append(r'\\multicolumn{%d}{|l|}{%%' % cell.width + CR)\n-                else:\n-                    self.body.append(r'\\multicolumn{%d}{l|}{%%' % cell.width + CR)\n-                context = '}%' + CR\n-            else:\n-                self.body.append(r'\\sphinxstartmulticolumn{%d}%%' % cell.width + CR)\n-                context = r'\\sphinxstopmulticolumn' + CR\n-        if cell.height > 1:\n-            # \\sphinxmultirow 2nd arg \"cell_id\" will serve as id for LaTeX macros as well\n-            self.body.append(r'\\sphinxmultirow{%d}{%d}{%%' % (cell.height, cell.cell_id) + CR)\n-            context = '}%' + CR + context\n-        if cell.width > 1 or cell.height > 1:\n-            self.body.append(r'\\begin{varwidth}[t]{\\sphinxcolwidth{%d}{%d}}'\n-                             % (cell.width, self.table.colcount) + CR)\n-            context = (r'\\par' + CR + r'\\vskip-\\baselineskip'\n-                       r'\\vbox{\\hbox{\\strut}}\\end{varwidth}%' + CR + context)\n-            self.needs_linetrimming = 1\n-        if len(list(node.findall(nodes.paragraph))) >= 2:\n-            self.table.has_oldproblematic = True\n-        if isinstance(node.parent.parent, nodes.thead) or (cell.col in self.table.stubs):\n-            if len(node) == 1 and isinstance(node[0], nodes.paragraph) and node.astext() == '':\n-                pass\n-            else:\n-                self.body.append(r'\\sphinxstyletheadfamily ')\n-        if self.needs_linetrimming:\n-            self.pushbody([])\n-        self.context.append(context)\n-\n-    def depart_entry(self, node: Element) -> None:\n-        if self.needs_linetrimming:\n-            self.needs_linetrimming = 0\n-            body = self.popbody()\n-\n-            # Remove empty lines from top of merged cell\n-            while body and body[0] == CR:\n-                body.pop(0)\n-            self.body.extend(body)\n-\n-        self.body.append(self.context.pop())\n-\n-        cell = self.table.cell()\n-        self.table.col += cell.width\n-\n-        # fill columns if next ones are a bottom of wide-multirow cell\n-        while True:\n-            nextcell = self.table.cell()\n-            if nextcell is None:  # not a bottom of multirow cell\n-                break\n-            else:  # a bottom part of multirow cell\n-                self.table.col += nextcell.width\n-                self.body.append('&')\n-                if nextcell.width == 1:\n-                    # insert suitable strut for equalizing row heights in multirow\n-                    # they also serve to clear colour panels which would hide the text\n-                    self.body.append(r'\\sphinxtablestrut{%d}' % nextcell.cell_id)\n-                else:\n-                    # use \\multicolumn for wide multirow cell\n-                    self.body.append(r'\\multicolumn{%d}{l|}{\\sphinxtablestrut{%d}}' %\n-                                     (nextcell.width, nextcell.cell_id))\n-\n-    def visit_acks(self, node: Element) -> None:\n-        # this is a list in the source, but should be rendered as a\n-        # comma-separated list here\n-        bullet_list = cast(nodes.bullet_list, node[0])\n-        list_items = cast(Iterable[nodes.list_item], bullet_list)\n-        self.body.append(BLANKLINE)\n-        self.body.append(', '.join(n.astext() for n in list_items) + '.')\n-        self.body.append(BLANKLINE)\n-        raise nodes.SkipNode\n-\n-    def visit_bullet_list(self, node: Element) -> None:\n-        if not self.compact_list:\n-            self.body.append(r'\\begin{itemize}' + CR)\n-        if self.table:\n-            self.table.has_problematic = True\n-\n-    def depart_bullet_list(self, node: Element) -> None:\n-        if not self.compact_list:\n-            self.body.append(r'\\end{itemize}' + CR)\n-\n-    def visit_enumerated_list(self, node: Element) -> None:\n-        def get_enumtype(node: Element) -> str:\n-            enumtype = node.get('enumtype', 'arabic')\n-            if 'alpha' in enumtype and 26 < node.get('start', 0) + len(node):\n-                # fallback to arabic if alphabet counter overflows\n-                enumtype = 'arabic'\n-\n-            return enumtype\n-\n-        def get_nested_level(node: Element) -> int:\n-            if node is None:\n-                return 0\n-            elif isinstance(node, nodes.enumerated_list):\n-                return get_nested_level(node.parent) + 1\n-            else:\n-                return get_nested_level(node.parent)\n-\n-        enum = \"enum%s\" % toRoman(get_nested_level(node)).lower()\n-        enumnext = \"enum%s\" % toRoman(get_nested_level(node) + 1).lower()\n-        style = ENUMERATE_LIST_STYLE.get(get_enumtype(node))\n-        prefix = node.get('prefix', '')\n-        suffix = node.get('suffix', '.')\n-\n-        self.body.append(r'\\begin{enumerate}' + CR)\n-        self.body.append(r'\\sphinxsetlistlabels{%s}{%s}{%s}{%s}{%s}%%' %\n-                         (style, enum, enumnext, prefix, suffix) + CR)\n-        if 'start' in node:\n-            self.body.append(r'\\setcounter{%s}{%d}' % (enum, node['start'] - 1) + CR)\n-        if self.table:\n-            self.table.has_problematic = True\n-\n-    def depart_enumerated_list(self, node: Element) -> None:\n-        self.body.append(r'\\end{enumerate}' + CR)\n-\n-    def visit_list_item(self, node: Element) -> None:\n-        # Append \"{}\" in case the next character is \"[\", which would break\n-        # LaTeX's list environment (no numbering and the \"[\" is not printed).\n-        self.body.append(r'\\item {} ')\n-\n-    def depart_list_item(self, node: Element) -> None:\n-        self.body.append(CR)\n-\n-    def visit_definition_list(self, node: Element) -> None:\n-        self.body.append(r'\\begin{description}' + CR)\n-        if self.table:\n-            self.table.has_problematic = True\n-\n-    def depart_definition_list(self, node: Element) -> None:\n-        self.body.append(r'\\end{description}' + CR)\n-\n-    def visit_definition_list_item(self, node: Element) -> None:\n-        pass\n-\n-    def depart_definition_list_item(self, node: Element) -> None:\n-        pass\n-\n-    def visit_term(self, node: Element) -> None:\n-        self.in_term += 1\n-        ctx = ''\n-        if node.get('ids'):\n-            ctx = r'\\phantomsection'\n-            for node_id in node['ids']:\n-                ctx += self.hypertarget(node_id, anchor=False)\n-        ctx += r'}'\n-        self.body.append(r'\\sphinxlineitem{')\n-        self.context.append(ctx)\n-\n-    def depart_term(self, node: Element) -> None:\n-        self.body.append(self.context.pop())\n-        self.in_term -= 1\n-\n-    def visit_classifier(self, node: Element) -> None:\n-        self.body.append('{[}')\n-\n-    def depart_classifier(self, node: Element) -> None:\n-        self.body.append('{]}')\n-\n-    def visit_definition(self, node: Element) -> None:\n-        pass\n-\n-    def depart_definition(self, node: Element) -> None:\n-        self.body.append(CR)\n-\n-    def visit_field_list(self, node: Element) -> None:\n-        self.body.append(r'\\begin{quote}\\begin{description}' + CR)\n-        if self.table:\n-            self.table.has_problematic = True\n-\n-    def depart_field_list(self, node: Element) -> None:\n-        self.body.append(r'\\end{description}\\end{quote}' + CR)\n-\n-    def visit_field(self, node: Element) -> None:\n-        pass\n-\n-    def depart_field(self, node: Element) -> None:\n-        pass\n-\n-    visit_field_name = visit_term\n-    depart_field_name = depart_term\n-\n-    visit_field_body = visit_definition\n-    depart_field_body = depart_definition\n-\n-    def visit_paragraph(self, node: Element) -> None:\n-        index = node.parent.index(node)\n-        if (index > 0 and isinstance(node.parent, nodes.compound) and\n-                not isinstance(node.parent[index - 1], nodes.paragraph) and\n-                not isinstance(node.parent[index - 1], nodes.compound)):\n-            # insert blank line, if the paragraph follows a non-paragraph node in a compound\n-            self.body.append(r'\\noindent' + CR)\n-        elif index == 1 and isinstance(node.parent, (nodes.footnote, footnotetext)):\n-            # don't insert blank line, if the paragraph is second child of a footnote\n-            # (first one is label node)\n-            pass\n-        else:\n-            # the \\sphinxAtStartPar is to allow hyphenation of first word of\n-            # a paragraph in narrow contexts such as in a table cell\n-            # added as two items (cf. line trimming in depart_entry())\n-            self.body.extend([CR, r'\\sphinxAtStartPar' + CR])\n-\n-    def depart_paragraph(self, node: Element) -> None:\n-        self.body.append(CR)\n-\n-    def visit_centered(self, node: Element) -> None:\n-        self.body.append(CR + r'\\begin{center}')\n-        if self.table:\n-            self.table.has_problematic = True\n-\n-    def depart_centered(self, node: Element) -> None:\n-        self.body.append(CR + r'\\end{center}')\n-\n-    def visit_hlist(self, node: Element) -> None:\n-        self.compact_list += 1\n-        ncolumns = node['ncolumns']\n-        if self.compact_list > 1:\n-            self.body.append(r'\\setlength{\\multicolsep}{0pt}' + CR)\n-        self.body.append(r'\\begin{multicols}{' + ncolumns + r'}\\raggedright' + CR)\n-        self.body.append(r'\\begin{itemize}\\setlength{\\itemsep}{0pt}'\n-                         r'\\setlength{\\parskip}{0pt}' + CR)\n-        if self.table:\n-            self.table.has_problematic = True\n-\n-    def depart_hlist(self, node: Element) -> None:\n-        self.compact_list -= 1\n-        self.body.append(r'\\end{itemize}\\raggedcolumns\\end{multicols}' + CR)\n-\n-    def visit_hlistcol(self, node: Element) -> None:\n-        pass\n-\n-    def depart_hlistcol(self, node: Element) -> None:\n-        # \\columnbreak would guarantee same columns as in html output.  But\n-        # some testing with long items showed that columns may be too uneven.\n-        # And in case only of short items, the automatic column breaks should\n-        # match the ones pre-computed by the hlist() directive.\n-        # self.body.append(r'\\columnbreak\\n')\n-        pass\n-\n-    def latex_image_length(self, width_str: str, scale: int = 100) -> str:\n-        try:\n-            return rstdim_to_latexdim(width_str, scale)\n-        except ValueError:\n-            logger.warning(__('dimension unit %s is invalid. Ignored.'), width_str)\n-            return None\n-\n-    def is_inline(self, node: Element) -> bool:\n-        \"\"\"Check whether a node represents an inline element.\"\"\"\n-        return isinstance(node.parent, nodes.TextElement)\n-\n-    def visit_image(self, node: Element) -> None:\n-        pre: List[str] = []  # in reverse order\n-        post: List[str] = []\n-        include_graphics_options = []\n-        has_hyperlink = isinstance(node.parent, nodes.reference)\n-        if has_hyperlink:\n-            is_inline = self.is_inline(node.parent)\n-        else:\n-            is_inline = self.is_inline(node)\n-        if 'width' in node:\n-            if 'scale' in node:\n-                w = self.latex_image_length(node['width'], node['scale'])\n-            else:\n-                w = self.latex_image_length(node['width'])\n-            if w:\n-                include_graphics_options.append('width=%s' % w)\n-        if 'height' in node:\n-            if 'scale' in node:\n-                h = self.latex_image_length(node['height'], node['scale'])\n-            else:\n-                h = self.latex_image_length(node['height'])\n-            if h:\n-                include_graphics_options.append('height=%s' % h)\n-        if 'scale' in node:\n-            if not include_graphics_options:\n-                # if no \"width\" nor \"height\", \\sphinxincludegraphics will fit\n-                # to the available text width if oversized after rescaling.\n-                include_graphics_options.append('scale=%s'\n-                                                % (float(node['scale']) / 100.0))\n-        if 'align' in node:\n-            align_prepost = {\n-                # By default latex aligns the top of an image.\n-                (1, 'top'): ('', ''),\n-                (1, 'middle'): (r'\\raisebox{-0.5\\height}{', '}'),\n-                (1, 'bottom'): (r'\\raisebox{-\\height}{', '}'),\n-                (0, 'center'): (r'{\\hspace*{\\fill}', r'\\hspace*{\\fill}}'),\n-                # These 2 don't exactly do the right thing.  The image should\n-                # be floated alongside the paragraph.  See\n-                # https://www.w3.org/TR/html4/struct/objects.html#adef-align-IMG\n-                (0, 'left'): ('{', r'\\hspace*{\\fill}}'),\n-                (0, 'right'): (r'{\\hspace*{\\fill}', '}'),\n-            }\n-            try:\n-                pre.append(align_prepost[is_inline, node['align']][0])\n-                post.append(align_prepost[is_inline, node['align']][1])\n-            except KeyError:\n-                pass\n-        if self.in_parsed_literal:\n-            pre.append(r'{\\sphinxunactivateextrasandspace ')\n-            post.append('}')\n-        if not is_inline and not has_hyperlink:\n-            pre.append(CR + r'\\noindent')\n-            post.append(CR)\n-        pre.reverse()\n-        if node['uri'] in self.builder.images:\n-            uri = self.builder.images[node['uri']]\n-        else:\n-            # missing image!\n-            if self.ignore_missing_images:\n-                return\n-            uri = node['uri']\n-        if uri.find('://') != -1:\n-            # ignore remote images\n-            return\n-        self.body.extend(pre)\n-        options = ''\n-        if include_graphics_options:\n-            options = '[%s]' % ','.join(include_graphics_options)\n-        base, ext = path.splitext(uri)\n-        if self.in_title and base:\n-            # Lowercase tokens forcely because some fncychap themes capitalize\n-            # the options of \\sphinxincludegraphics unexpectedly (ex. WIDTH=...).\n-            self.body.append(r'\\lowercase{\\sphinxincludegraphics%s}{{%s}%s}' %\n-                             (options, base, ext))\n-        else:\n-            self.body.append(r'\\sphinxincludegraphics%s{{%s}%s}' %\n-                             (options, base, ext))\n-        self.body.extend(post)\n-\n-    def depart_image(self, node: Element) -> None:\n-        pass\n-\n-    def visit_figure(self, node: Element) -> None:\n-        align = self.elements['figure_align']\n-        if self.no_latex_floats:\n-            align = \"H\"\n-        if self.table:\n-            # TODO: support align option\n-            if 'width' in node:\n-                length = self.latex_image_length(node['width'])\n-                if length:\n-                    self.body.append(r'\\begin{sphinxfigure-in-table}[%s]' % length + CR)\n-                    self.body.append(r'\\centering' + CR)\n-            else:\n-                self.body.append(r'\\begin{sphinxfigure-in-table}' + CR)\n-                self.body.append(r'\\centering' + CR)\n-            if any(isinstance(child, nodes.caption) for child in node):\n-                self.body.append(r'\\capstart')\n-            self.context.append(r'\\end{sphinxfigure-in-table}\\relax' + CR)\n-        elif node.get('align', '') in ('left', 'right'):\n-            length = None\n-            if 'width' in node:\n-                length = self.latex_image_length(node['width'])\n-            elif isinstance(node[0], nodes.image) and 'width' in node[0]:\n-                length = self.latex_image_length(node[0]['width'])\n-            self.body.append(BLANKLINE)     # Insert a blank line to prevent infinite loop\n-                                            # https://github.com/sphinx-doc/sphinx/issues/7059\n-            self.body.append(r'\\begin{wrapfigure}{%s}{%s}' %\n-                             ('r' if node['align'] == 'right' else 'l', length or '0pt') + CR)\n-            self.body.append(r'\\centering')\n-            self.context.append(r'\\end{wrapfigure}' + CR)\n-        elif self.in_minipage:\n-            self.body.append(CR + r'\\begin{center}')\n-            self.context.append(r'\\end{center}' + CR)\n-        else:\n-            self.body.append(CR + r'\\begin{figure}[%s]' % align + CR)\n-            self.body.append(r'\\centering' + CR)\n-            if any(isinstance(child, nodes.caption) for child in node):\n-                self.body.append(r'\\capstart' + CR)\n-            self.context.append(r'\\end{figure}' + CR)\n-\n-    def depart_figure(self, node: Element) -> None:\n-        self.body.append(self.context.pop())\n-\n-    def visit_caption(self, node: Element) -> None:\n-        self.in_caption += 1\n-        if isinstance(node.parent, captioned_literal_block):\n-            self.body.append(r'\\sphinxSetupCaptionForVerbatim{')\n-        elif self.in_minipage and isinstance(node.parent, nodes.figure):\n-            self.body.append(r'\\captionof{figure}{')\n-        elif self.table and node.parent.tagname == 'figure':\n-            self.body.append(r'\\sphinxfigcaption{')\n-        else:\n-            self.body.append(r'\\caption{')\n-\n-    def depart_caption(self, node: Element) -> None:\n-        self.body.append('}')\n-        if isinstance(node.parent, nodes.figure):\n-            labels = self.hypertarget_to(node.parent)\n-            self.body.append(labels)\n-        self.in_caption -= 1\n-\n-    def visit_legend(self, node: Element) -> None:\n-        self.body.append(CR + r'\\begin{sphinxlegend}')\n-\n-    def depart_legend(self, node: Element) -> None:\n-        self.body.append(r'\\end{sphinxlegend}' + CR)\n-\n-    def visit_admonition(self, node: Element) -> None:\n-        self.body.append(CR + r'\\begin{sphinxadmonition}{note}')\n-        self.no_latex_floats += 1\n-\n-    def depart_admonition(self, node: Element) -> None:\n-        self.body.append(r'\\end{sphinxadmonition}' + CR)\n-        self.no_latex_floats -= 1\n-\n-    def _visit_named_admonition(self, node: Element) -> None:\n-        label = admonitionlabels[node.tagname]\n-        self.body.append(CR + r'\\begin{sphinxadmonition}{%s}{%s:}' %\n-                         (node.tagname, label))\n-        self.no_latex_floats += 1\n-\n-    def _depart_named_admonition(self, node: Element) -> None:\n-        self.body.append(r'\\end{sphinxadmonition}' + CR)\n-        self.no_latex_floats -= 1\n-\n-    visit_attention = _visit_named_admonition\n-    depart_attention = _depart_named_admonition\n-    visit_caution = _visit_named_admonition\n-    depart_caution = _depart_named_admonition\n-    visit_danger = _visit_named_admonition\n-    depart_danger = _depart_named_admonition\n-    visit_error = _visit_named_admonition\n-    depart_error = _depart_named_admonition\n-    visit_hint = _visit_named_admonition\n-    depart_hint = _depart_named_admonition\n-    visit_important = _visit_named_admonition\n-    depart_important = _depart_named_admonition\n-    visit_note = _visit_named_admonition\n-    depart_note = _depart_named_admonition\n-    visit_tip = _visit_named_admonition\n-    depart_tip = _depart_named_admonition\n-    visit_warning = _visit_named_admonition\n-    depart_warning = _depart_named_admonition\n-\n-    def visit_versionmodified(self, node: Element) -> None:\n-        pass\n-\n-    def depart_versionmodified(self, node: Element) -> None:\n-        pass\n-\n-    def visit_target(self, node: Element) -> None:\n-        def add_target(id: str) -> None:\n-            # indexing uses standard LaTeX index markup, so the targets\n-            # will be generated differently\n-            if id.startswith('index-'):\n-                return\n-\n-            # equations also need no extra blank line nor hypertarget\n-            # TODO: fix this dependency on mathbase extension internals\n-            if id.startswith('equation-'):\n-                return\n-\n-            # insert blank line, if the target follows a paragraph node\n-            index = node.parent.index(node)\n-            if index > 0 and isinstance(node.parent[index - 1], nodes.paragraph):\n-                self.body.append(CR)\n-\n-            # do not generate \\phantomsection in \\section{}\n-            anchor = not self.in_title\n-            self.body.append(self.hypertarget(id, anchor=anchor))\n-\n-        # skip if visitor for next node supports hyperlink\n-        next_node: Node = node\n-        while isinstance(next_node, nodes.target):\n-            next_node = next_node.next_node(ascend=True)\n-\n-        domain = cast(StandardDomain, self.builder.env.get_domain('std'))\n-        if isinstance(next_node, HYPERLINK_SUPPORT_NODES):\n-            return\n-        elif domain.get_enumerable_node_type(next_node) and domain.get_numfig_title(next_node):\n-            return\n-\n-        if 'refuri' in node:\n-            return\n-        if 'anonymous' in node:\n-            return\n-        if node.get('refid'):\n-            prev_node = get_prev_node(node)\n-            if isinstance(prev_node, nodes.reference) and node['refid'] == prev_node['refid']:\n-                # a target for a hyperlink reference having alias\n-                pass\n-            else:\n-                add_target(node['refid'])\n-        for id in node['ids']:\n-            add_target(id)\n-\n-    def depart_target(self, node: Element) -> None:\n-        pass\n-\n-    def visit_attribution(self, node: Element) -> None:\n-        self.body.append(CR + r'\\begin{flushright}' + CR)\n-        self.body.append('---')\n-\n-    def depart_attribution(self, node: Element) -> None:\n-        self.body.append(CR + r'\\end{flushright}' + CR)\n-\n-    def visit_index(self, node: Element) -> None:\n-        def escape(value: str) -> str:\n-            value = self.encode(value)\n-            value = value.replace(r'\\{', r'\\sphinxleftcurlybrace{}')\n-            value = value.replace(r'\\}', r'\\sphinxrightcurlybrace{}')\n-            value = value.replace('\"', '\"\"')\n-            value = value.replace('@', '\"@')\n-            value = value.replace('!', '\"!')\n-            value = value.replace('|', r'\\textbar{}')\n-            return value\n-\n-        def style(string: str) -> str:\n-            match = EXTRA_RE.match(string)\n-            if match:\n-                return match.expand(r'\\\\spxentry{\\1}\\\\spxextra{\\2}')\n-            else:\n-                return r'\\spxentry{%s}' % string\n-\n-        if not node.get('inline', True):\n-            self.body.append(CR)\n-        entries = node['entries']\n-        for type, string, _tid, ismain, _key in entries:\n-            m = ''\n-            if ismain:\n-                m = '|spxpagem'\n-            try:\n-                if type == 'single':\n-                    try:\n-                        p1, p2 = [escape(x) for x in split_into(2, 'single', string)]\n-                        P1, P2 = style(p1), style(p2)\n-                        self.body.append(r'\\index{%s@%s!%s@%s%s}' % (p1, P1, p2, P2, m))\n-                    except ValueError:\n-                        p = escape(split_into(1, 'single', string)[0])\n-                        P = style(p)\n-                        self.body.append(r'\\index{%s@%s%s}' % (p, P, m))\n-                elif type == 'pair':\n-                    p1, p2 = [escape(x) for x in split_into(2, 'pair', string)]\n-                    P1, P2 = style(p1), style(p2)\n-                    self.body.append(r'\\index{%s@%s!%s@%s%s}\\index{%s@%s!%s@%s%s}' %\n-                                     (p1, P1, p2, P2, m, p2, P2, p1, P1, m))\n-                elif type == 'triple':\n-                    p1, p2, p3 = [escape(x) for x in split_into(3, 'triple', string)]\n-                    P1, P2, P3 = style(p1), style(p2), style(p3)\n-                    self.body.append(\n-                        r'\\index{%s@%s!%s %s@%s %s%s}'\n-                        r'\\index{%s@%s!%s, %s@%s, %s%s}'\n-                        r'\\index{%s@%s!%s %s@%s %s%s}' %\n-                        (p1, P1, p2, p3, P2, P3, m,\n-                         p2, P2, p3, p1, P3, P1, m,\n-                         p3, P3, p1, p2, P1, P2, m))\n-                elif type == 'see':\n-                    p1, p2 = [escape(x) for x in split_into(2, 'see', string)]\n-                    P1 = style(p1)\n-                    self.body.append(r'\\index{%s@%s|see{%s}}' % (p1, P1, p2))\n-                elif type == 'seealso':\n-                    p1, p2 = [escape(x) for x in split_into(2, 'seealso', string)]\n-                    P1 = style(p1)\n-                    self.body.append(r'\\index{%s@%s|see{%s}}' % (p1, P1, p2))\n-                else:\n-                    logger.warning(__('unknown index entry type %s found'), type)\n-            except ValueError as err:\n-                logger.warning(str(err))\n-        if not node.get('inline', True):\n-            self.body.append(r'\\ignorespaces ')\n-        raise nodes.SkipNode\n-\n-    def visit_raw(self, node: Element) -> None:\n-        if not self.is_inline(node):\n-            self.body.append(CR)\n-        if 'latex' in node.get('format', '').split():\n-            self.body.append(node.astext())\n-        if not self.is_inline(node):\n-            self.body.append(CR)\n-        raise nodes.SkipNode\n-\n-    def visit_reference(self, node: Element) -> None:\n-        if not self.in_title:\n-            for id in node.get('ids'):\n-                anchor = not self.in_caption\n-                self.body += self.hypertarget(id, anchor=anchor)\n-        if not self.is_inline(node):\n-            self.body.append(CR)\n-        uri = node.get('refuri', '')\n-        if not uri and node.get('refid'):\n-            uri = '%' + self.curfilestack[-1] + '#' + node['refid']\n-        if self.in_title or not uri:\n-            self.context.append('')\n-        elif uri.startswith('#'):\n-            # references to labels in the same document\n-            id = self.curfilestack[-1] + ':' + uri[1:]\n-            self.body.append(self.hyperlink(id))\n-            self.body.append(r'\\emph{')\n-            if self.config.latex_show_pagerefs and not \\\n-                    self.in_production_list:\n-                self.context.append('}}} (%s)' % self.hyperpageref(id))\n-            else:\n-                self.context.append('}}}')\n-        elif uri.startswith('%'):\n-            # references to documents or labels inside documents\n-            hashindex = uri.find('#')\n-            if hashindex == -1:\n-                # reference to the document\n-                id = uri[1:] + '::doc'\n-            else:\n-                # reference to a label\n-                id = uri[1:].replace('#', ':')\n-            self.body.append(self.hyperlink(id))\n-            if (len(node) and\n-                    isinstance(node[0], nodes.Element) and\n-                    'std-term' in node[0].get('classes', [])):\n-                # don't add a pageref for glossary terms\n-                self.context.append('}}}')\n-                # mark up as termreference\n-                self.body.append(r'\\sphinxtermref{')\n-            else:\n-                self.body.append(r'\\sphinxcrossref{')\n-                if self.config.latex_show_pagerefs and not self.in_production_list:\n-                    self.context.append('}}} (%s)' % self.hyperpageref(id))\n-                else:\n-                    self.context.append('}}}')\n-        else:\n-            if len(node) == 1 and uri == node[0]:\n-                if node.get('nolinkurl'):\n-                    self.body.append(r'\\sphinxnolinkurl{%s}' % self.encode_uri(uri))\n-                else:\n-                    self.body.append(r'\\sphinxurl{%s}' % self.encode_uri(uri))\n-                raise nodes.SkipNode\n-            else:\n-                self.body.append(r'\\sphinxhref{%s}{' % self.encode_uri(uri))\n-                self.context.append('}')\n-\n-    def depart_reference(self, node: Element) -> None:\n-        self.body.append(self.context.pop())\n-        if not self.is_inline(node):\n-            self.body.append(CR)\n-\n-    def visit_number_reference(self, node: Element) -> None:\n-        if node.get('refid'):\n-            id = self.curfilestack[-1] + ':' + node['refid']\n-        else:\n-            id = node.get('refuri', '')[1:].replace('#', ':')\n-\n-        title = self.escape(node.get('title', '%s')).replace(r'\\%s', '%s')\n-        if r'\\{name\\}' in title or r'\\{number\\}' in title:\n-            # new style format (cf. \"Fig.%{number}\")\n-            title = title.replace(r'\\{name\\}', '{name}').replace(r'\\{number\\}', '{number}')\n-            text = escape_abbr(title).format(name=r'\\nameref{%s}' % self.idescape(id),\n-                                             number=r'\\ref{%s}' % self.idescape(id))\n-        else:\n-            # old style format (cf. \"Fig.%{number}\")\n-            text = escape_abbr(title) % (r'\\ref{%s}' % self.idescape(id))\n-        hyperref = r'\\hyperref[%s]{%s}' % (self.idescape(id), text)\n-        self.body.append(hyperref)\n-\n-        raise nodes.SkipNode\n-\n-    def visit_download_reference(self, node: Element) -> None:\n-        pass\n-\n-    def depart_download_reference(self, node: Element) -> None:\n-        pass\n-\n-    def visit_pending_xref(self, node: Element) -> None:\n-        pass\n-\n-    def depart_pending_xref(self, node: Element) -> None:\n-        pass\n-\n-    def visit_emphasis(self, node: Element) -> None:\n-        self.body.append(r'\\sphinxstyleemphasis{')\n-\n-    def depart_emphasis(self, node: Element) -> None:\n-        self.body.append('}')\n-\n-    def visit_literal_emphasis(self, node: Element) -> None:\n-        self.body.append(r'\\sphinxstyleliteralemphasis{\\sphinxupquote{')\n-\n-    def depart_literal_emphasis(self, node: Element) -> None:\n-        self.body.append('}}')\n-\n-    def visit_strong(self, node: Element) -> None:\n-        self.body.append(r'\\sphinxstylestrong{')\n-\n-    def depart_strong(self, node: Element) -> None:\n-        self.body.append('}')\n-\n-    def visit_literal_strong(self, node: Element) -> None:\n-        self.body.append(r'\\sphinxstyleliteralstrong{\\sphinxupquote{')\n-\n-    def depart_literal_strong(self, node: Element) -> None:\n-        self.body.append('}}')\n-\n-    def visit_abbreviation(self, node: Element) -> None:\n-        abbr = node.astext()\n-        self.body.append(r'\\sphinxstyleabbreviation{')\n-        # spell out the explanation once\n-        if node.hasattr('explanation') and abbr not in self.handled_abbrs:\n-            self.context.append('} (%s)' % self.encode(node['explanation']))\n-            self.handled_abbrs.add(abbr)\n-        else:\n-            self.context.append('}')\n-\n-    def depart_abbreviation(self, node: Element) -> None:\n-        self.body.append(self.context.pop())\n-\n-    def visit_manpage(self, node: Element) -> None:\n-        return self.visit_literal_emphasis(node)\n-\n-    def depart_manpage(self, node: Element) -> None:\n-        return self.depart_literal_emphasis(node)\n-\n-    def visit_title_reference(self, node: Element) -> None:\n-        self.body.append(r'\\sphinxtitleref{')\n-\n-    def depart_title_reference(self, node: Element) -> None:\n-        self.body.append('}')\n-\n-    def visit_thebibliography(self, node: Element) -> None:\n-        citations = cast(Iterable[nodes.citation], node)\n-        labels = (cast(nodes.label, citation[0]) for citation in citations)\n-        longest_label = max((label.astext() for label in labels), key=len)\n-        if len(longest_label) > MAX_CITATION_LABEL_LENGTH:\n-            # adjust max width of citation labels not to break the layout\n-            longest_label = longest_label[:MAX_CITATION_LABEL_LENGTH]\n-\n-        self.body.append(CR + r'\\begin{sphinxthebibliography}{%s}' %\n-                         self.encode(longest_label) + CR)\n-\n-    def depart_thebibliography(self, node: Element) -> None:\n-        self.body.append(r'\\end{sphinxthebibliography}' + CR)\n-\n-    def visit_citation(self, node: Element) -> None:\n-        label = cast(nodes.label, node[0])\n-        self.body.append(r'\\bibitem[%s]{%s:%s}' % (self.encode(label.astext()),\n-                                                   node['docname'], node['ids'][0]))\n-\n-    def depart_citation(self, node: Element) -> None:\n-        pass\n-\n-    def visit_citation_reference(self, node: Element) -> None:\n-        if self.in_title:\n-            pass\n-        else:\n-            self.body.append(r'\\sphinxcite{%s:%s}' % (node['docname'], node['refname']))\n-            raise nodes.SkipNode\n-\n-    def depart_citation_reference(self, node: Element) -> None:\n-        pass\n-\n-    def visit_literal(self, node: Element) -> None:\n-        if self.in_title:\n-            self.body.append(r'\\sphinxstyleliteralintitle{\\sphinxupquote{')\n-            return\n-        elif 'kbd' in node['classes']:\n-            self.body.append(r'\\sphinxkeyboard{\\sphinxupquote{')\n-            return\n-        lang = node.get(\"language\", None)\n-        if 'code' not in node['classes'] or not lang:\n-            self.body.append(r'\\sphinxcode{\\sphinxupquote{')\n-            return\n-\n-        opts = self.config.highlight_options.get(lang, {})\n-        hlcode = self.highlighter.highlight_block(\n-            node.astext(), lang, opts=opts, location=node)\n-        # TODO: Use nowrap option once LaTeX formatter supports it\n-        # https://github.com/pygments/pygments/pull/1343\n-        hlcode = hlcode.replace(r'\\begin{Verbatim}[commandchars=\\\\\\{\\}]',\n-                                r'\\sphinxcode{\\sphinxupquote{')\n-        # get consistent trailer\n-        hlcode = hlcode.rstrip()[:-14]  # strip \\end{Verbatim}\n-        self.body.append(hlcode)\n-        self.body.append('}}')\n-        raise nodes.SkipNode\n-\n-    def depart_literal(self, node: Element) -> None:\n-        self.body.append('}}')\n-\n-    def visit_footnote_reference(self, node: Element) -> None:\n-        raise nodes.SkipNode\n-\n-    def visit_footnotemark(self, node: Element) -> None:\n-        self.body.append(r'\\sphinxfootnotemark[')\n-\n-    def depart_footnotemark(self, node: Element) -> None:\n-        self.body.append(']')\n-\n-    def visit_footnotetext(self, node: Element) -> None:\n-        label = cast(nodes.label, node[0])\n-        self.body.append('%' + CR)\n-        self.body.append(r'\\begin{footnotetext}[%s]' % label.astext())\n-        self.body.append(r'\\sphinxAtStartFootnote' + CR)\n-\n-    def depart_footnotetext(self, node: Element) -> None:\n-        # the \\ignorespaces in particular for after table header use\n-        self.body.append('%' + CR)\n-        self.body.append(r'\\end{footnotetext}\\ignorespaces ')\n-\n-    def visit_captioned_literal_block(self, node: Element) -> None:\n-        pass\n-\n-    def depart_captioned_literal_block(self, node: Element) -> None:\n-        pass\n-\n-    def visit_literal_block(self, node: Element) -> None:\n-        if node.rawsource != node.astext():\n-            # most probably a parsed-literal block -- don't highlight\n-            self.in_parsed_literal += 1\n-            self.body.append(r'\\begin{sphinxalltt}' + CR)\n-        else:\n-            labels = self.hypertarget_to(node)\n-            if isinstance(node.parent, captioned_literal_block):\n-                labels += self.hypertarget_to(node.parent)\n-            if labels and not self.in_footnote:\n-                self.body.append(CR + r'\\def\\sphinxLiteralBlockLabel{' + labels + '}')\n-\n-            lang = node.get('language', 'default')\n-            linenos = node.get('linenos', False)\n-            highlight_args = node.get('highlight_args', {})\n-            highlight_args['force'] = node.get('force', False)\n-            opts = self.config.highlight_options.get(lang, {})\n-\n-            hlcode = self.highlighter.highlight_block(\n-                node.rawsource, lang, opts=opts, linenos=linenos,\n-                location=node, **highlight_args\n-            )\n-            if self.in_footnote:\n-                self.body.append(CR + r'\\sphinxSetupCodeBlockInFootnote')\n-                hlcode = hlcode.replace(r'\\begin{Verbatim}',\n-                                        r'\\begin{sphinxVerbatim}')\n-            # if in table raise verbatim flag to avoid \"tabulary\" environment\n-            # and opt for sphinxVerbatimintable to handle caption & long lines\n-            elif self.table:\n-                self.table.has_problematic = True\n-                self.table.has_verbatim = True\n-                hlcode = hlcode.replace(r'\\begin{Verbatim}',\n-                                        r'\\begin{sphinxVerbatimintable}')\n-            else:\n-                hlcode = hlcode.replace(r'\\begin{Verbatim}',\n-                                        r'\\begin{sphinxVerbatim}')\n-            # get consistent trailer\n-            hlcode = hlcode.rstrip()[:-14]  # strip \\end{Verbatim}\n-            if self.table and not self.in_footnote:\n-                hlcode += r'\\end{sphinxVerbatimintable}'\n-            else:\n-                hlcode += r'\\end{sphinxVerbatim}'\n-\n-            hllines = str(highlight_args.get('hl_lines', []))[1:-1]\n-            if hllines:\n-                self.body.append(CR + r'\\fvset{hllines={, %s,}}%%' % hllines)\n-            self.body.append(CR + hlcode + CR)\n-            if hllines:\n-                self.body.append(r'\\sphinxresetverbatimhllines' + CR)\n-            raise nodes.SkipNode\n-\n-    def depart_literal_block(self, node: Element) -> None:\n-        self.body.append(CR + r'\\end{sphinxalltt}' + CR)\n-        self.in_parsed_literal -= 1\n-    visit_doctest_block = visit_literal_block\n-    depart_doctest_block = depart_literal_block\n-\n-    def visit_line(self, node: Element) -> None:\n-        self.body.append(r'\\item[] ')\n-\n-    def depart_line(self, node: Element) -> None:\n-        self.body.append(CR)\n-\n-    def visit_line_block(self, node: Element) -> None:\n-        if isinstance(node.parent, nodes.line_block):\n-            self.body.append(r'\\item[]' + CR)\n-            self.body.append(r'\\begin{DUlineblock}{\\DUlineblockindent}' + CR)\n-        else:\n-            self.body.append(CR + r'\\begin{DUlineblock}{0em}' + CR)\n-        if self.table:\n-            self.table.has_problematic = True\n-\n-    def depart_line_block(self, node: Element) -> None:\n-        self.body.append(r'\\end{DUlineblock}' + CR)\n-\n-    def visit_block_quote(self, node: Element) -> None:\n-        # If the block quote contains a single object and that object\n-        # is a list, then generate a list not a block quote.\n-        # This lets us indent lists.\n-        done = 0\n-        if len(node.children) == 1:\n-            child = node.children[0]\n-            if isinstance(child, (nodes.bullet_list, nodes.enumerated_list)):\n-                done = 1\n-        if not done:\n-            self.body.append(r'\\begin{quote}' + CR)\n-            if self.table:\n-                self.table.has_problematic = True\n-\n-    def depart_block_quote(self, node: Element) -> None:\n-        done = 0\n-        if len(node.children) == 1:\n-            child = node.children[0]\n-            if isinstance(child, (nodes.bullet_list, nodes.enumerated_list)):\n-                done = 1\n-        if not done:\n-            self.body.append(r'\\end{quote}' + CR)\n-\n-    # option node handling copied from docutils' latex writer\n-\n-    def visit_option(self, node: Element) -> None:\n-        if self.context[-1]:\n-            # this is not the first option\n-            self.body.append(', ')\n-\n-    def depart_option(self, node: Element) -> None:\n-        # flag that the first option is done.\n-        self.context[-1] += 1\n-\n-    def visit_option_argument(self, node: Element) -> None:\n-        \"\"\"The delimiter between an option and its argument.\"\"\"\n-        self.body.append(node.get('delimiter', ' '))\n-\n-    def depart_option_argument(self, node: Element) -> None:\n-        pass\n-\n-    def visit_option_group(self, node: Element) -> None:\n-        self.body.append(r'\\item [')\n-        # flag for first option\n-        self.context.append(0)\n-\n-    def depart_option_group(self, node: Element) -> None:\n-        self.context.pop()  # the flag\n-        self.body.append('] ')\n-\n-    def visit_option_list(self, node: Element) -> None:\n-        self.body.append(r'\\begin{optionlist}{3cm}' + CR)\n-        if self.table:\n-            self.table.has_problematic = True\n-\n-    def depart_option_list(self, node: Element) -> None:\n-        self.body.append(r'\\end{optionlist}' + CR)\n-\n-    def visit_option_list_item(self, node: Element) -> None:\n-        pass\n-\n-    def depart_option_list_item(self, node: Element) -> None:\n-        pass\n-\n-    def visit_option_string(self, node: Element) -> None:\n-        ostring = node.astext()\n-        self.body.append(self.encode(ostring))\n-        raise nodes.SkipNode\n-\n-    def visit_description(self, node: Element) -> None:\n-        self.body.append(' ')\n-\n-    def depart_description(self, node: Element) -> None:\n-        pass\n-\n-    def visit_superscript(self, node: Element) -> None:\n-        self.body.append(r'$^{\\text{')\n-\n-    def depart_superscript(self, node: Element) -> None:\n-        self.body.append('}}$')\n-\n-    def visit_subscript(self, node: Element) -> None:\n-        self.body.append(r'$_{\\text{')\n-\n-    def depart_subscript(self, node: Element) -> None:\n-        self.body.append('}}$')\n-\n-    def visit_inline(self, node: Element) -> None:\n-        classes = node.get('classes', [])\n-        if classes in [['menuselection']]:\n-            self.body.append(r'\\sphinxmenuselection{')\n-            self.context.append('}')\n-        elif classes in [['guilabel']]:\n-            self.body.append(r'\\sphinxguilabel{')\n-            self.context.append('}')\n-        elif classes in [['accelerator']]:\n-            self.body.append(r'\\sphinxaccelerator{')\n-            self.context.append('}')\n-        elif classes and not self.in_title:\n-            self.body.append(r'\\DUrole{%s}{' % ','.join(classes))\n-            self.context.append('}')\n-        else:\n-            self.context.append('')\n-\n-    def depart_inline(self, node: Element) -> None:\n-        self.body.append(self.context.pop())\n-\n-    def visit_generated(self, node: Element) -> None:\n-        pass\n-\n-    def depart_generated(self, node: Element) -> None:\n-        pass\n-\n-    def visit_compound(self, node: Element) -> None:\n-        pass\n-\n-    def depart_compound(self, node: Element) -> None:\n-        pass\n-\n-    def visit_container(self, node: Element) -> None:\n-        classes = node.get('classes', [])\n-        for c in classes:\n-            self.body.append('\\n\\\\begin{sphinxuseclass}{%s}' % c)\n-\n-    def depart_container(self, node: Element) -> None:\n-        classes = node.get('classes', [])\n-        for _c in classes:\n-            self.body.append('\\n\\\\end{sphinxuseclass}')\n-\n-    def visit_decoration(self, node: Element) -> None:\n-        pass\n-\n-    def depart_decoration(self, node: Element) -> None:\n-        pass\n-\n-    # docutils-generated elements that we don't support\n-\n-    def visit_header(self, node: Element) -> None:\n-        raise nodes.SkipNode\n-\n-    def visit_footer(self, node: Element) -> None:\n-        raise nodes.SkipNode\n-\n-    def visit_docinfo(self, node: Element) -> None:\n-        raise nodes.SkipNode\n-\n-    # text handling\n-\n-    def encode(self, text: str) -> str:\n-        text = self.escape(text)\n-        if self.literal_whitespace:\n-            # Insert a blank before the newline, to avoid\n-            # ! LaTeX Error: There's no line here to end.\n-            text = text.replace(CR, r'~\\\\' + CR).replace(' ', '~')\n-        return text\n-\n-    def encode_uri(self, text: str) -> str:\n-        # TODO: it is probably wrong that this uses texescape.escape()\n-        #       this must be checked against hyperref package exact dealings\n-        #       mainly, %, #, {, } and \\ need escaping via a \\ escape\n-        # in \\href, the tilde is allowed and must be represented literally\n-        return self.encode(text).replace(r'\\textasciitilde{}', '~').\\\n-            replace(r'\\sphinxhyphen{}', '-').\\\n-            replace(r'\\textquotesingle{}', \"'\")\n-\n-    def visit_Text(self, node: Text) -> None:\n-        text = self.encode(node.astext())\n-        self.body.append(text)\n-\n-    def depart_Text(self, node: Text) -> None:\n-        pass\n-\n-    def visit_comment(self, node: Element) -> None:\n-        raise nodes.SkipNode\n-\n-    def visit_meta(self, node: Element) -> None:\n-        # only valid for HTML\n-        raise nodes.SkipNode\n-\n-    def visit_system_message(self, node: Element) -> None:\n-        pass\n-\n-    def depart_system_message(self, node: Element) -> None:\n-        self.body.append(CR)\n-\n-    def visit_math(self, node: Element) -> None:\n-        if self.in_title:\n-            self.body.append(r'\\protect\\(%s\\protect\\)' % node.astext())\n-        else:\n-            self.body.append(r'\\(%s\\)' % node.astext())\n-        raise nodes.SkipNode\n-\n-    def visit_math_block(self, node: Element) -> None:\n-        if node.get('label'):\n-            label = \"equation:%s:%s\" % (node['docname'], node['label'])\n-        else:\n-            label = None\n-\n-        if node.get('nowrap'):\n-            if label:\n-                self.body.append(r'\\label{%s}' % label)\n-            self.body.append(node.astext())\n-        else:\n-            from sphinx.util.math import wrap_displaymath\n-            self.body.append(wrap_displaymath(node.astext(), label,\n-                                              self.config.math_number_all))\n-        raise nodes.SkipNode\n-\n-    def visit_math_reference(self, node: Element) -> None:\n-        label = \"equation:%s:%s\" % (node['docname'], node['target'])\n-        eqref_format = self.config.math_eqref_format\n-        if eqref_format:\n-            try:\n-                ref = r'\\ref{%s}' % label\n-                self.body.append(eqref_format.format(number=ref))\n-            except KeyError as exc:\n-                logger.warning(__('Invalid math_eqref_format: %r'), exc,\n-                               location=node)\n-                self.body.append(r'\\eqref{%s}' % label)\n-        else:\n-            self.body.append(r'\\eqref{%s}' % label)\n-\n-    def depart_math_reference(self, node: Element) -> None:\n-        pass\n-\n-    @property\n-    def docclasses(self) -> Tuple[str, str]:\n-        \"\"\"Prepends prefix to sphinx document classes\"\"\"\n-        warnings.warn('LaTeXWriter.docclasses() is deprecated.',\n-                      RemovedInSphinx70Warning, stacklevel=2)\n-        return ('howto', 'manual')\n-\n-\n-# FIXME: Workaround to avoid circular import\n-# refs: https://github.com/sphinx-doc/sphinx/issues/5433\n-from sphinx.builders.latex.nodes import ( # NOQA isort:skip\n-    HYPERLINK_SUPPORT_NODES, captioned_literal_block, footnotetext,\n-)\n"
  },
  "sphinx-doc__sphinx-10466": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-10466",
    "model_patch": "diff --git a/sphinx/builders/gettext.py b/sphinx/builders/gettext.py\nindex e8bc547b7..84ce7ce92 100644\n--- a/sphinx/builders/gettext.py\n+++ b/sphinx/builders/gettext.py\n@@ -32,8 +32,17 @@ class Message:\n     \"\"\"An entry of translatable message.\"\"\"\n     def __init__(self, text: str, locations: List[Tuple[str, int]], uuids: List[str]):\n         self.text = text\n-        self.locations = locations\n-        self.uuids = uuids\n+        # Remove duplicate locations (preserve first occurrence)\n+        seen = set()\n+        unique_locations = []\n+        unique_uuids = []\n+        for loc, uid in zip(locations, uuids):\n+            if loc not in seen:\n+                seen.add(loc)\n+                unique_locations.append(loc)\n+                unique_uuids.append(uid)\n+        self.locations = unique_locations\n+        self.uuids = unique_uuids\n \n \n class Catalog:\n"
  },
  "sphinx-doc__sphinx-10673": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-10673",
    "model_patch": "diff --git a/sphinx/directives/other.py b/sphinx/directives/other.py\nindex 35e16e623..b3e303c60 100644\n--- a/sphinx/directives/other.py\n+++ b/sphinx/directives/other.py\n@@ -1,387 +1,23 @@\n-import re\n-from typing import TYPE_CHECKING, Any, Dict, List, cast\n-\n-from docutils import nodes\n-from docutils.nodes import Element, Node\n-from docutils.parsers.rst import directives\n-from docutils.parsers.rst.directives.admonitions import BaseAdmonition\n-from docutils.parsers.rst.directives.misc import Class\n-from docutils.parsers.rst.directives.misc import Include as BaseInclude\n-\n-from sphinx import addnodes\n-from sphinx.domains.changeset import VersionChange  # NOQA  # for compatibility\n-from sphinx.locale import _, __\n-from sphinx.util import docname_join, logging, url_re\n-from sphinx.util.docutils import SphinxDirective\n-from sphinx.util.matching import Matcher, patfilter\n-from sphinx.util.nodes import explicit_title_re\n-from sphinx.util.typing import OptionSpec\n-\n-if TYPE_CHECKING:\n-    from sphinx.application import Sphinx\n-\n-\n-glob_re = re.compile(r'.*[*?\\[].*')\n-logger = logging.getLogger(__name__)\n-\n-\n-def int_or_nothing(argument: str) -> int:\n-    if not argument:\n-        return 999\n-    return int(argument)\n-\n-\n-class TocTree(SphinxDirective):\n-    \"\"\"\n-    Directive to notify Sphinx about the hierarchical structure of the docs,\n-    and to include a table-of-contents like tree in the current document.\n-    \"\"\"\n-    has_content = True\n-    required_arguments = 0\n-    optional_arguments = 0\n-    final_argument_whitespace = False\n-    option_spec = {\n-        'maxdepth': int,\n-        'name': directives.unchanged,\n-        'caption': directives.unchanged_required,\n-        'glob': directives.flag,\n-        'hidden': directives.flag,\n-        'includehidden': directives.flag,\n-        'numbered': int_or_nothing,\n-        'titlesonly': directives.flag,\n-        'reversed': directives.flag,\n-    }\n-\n-    def run(self) -> List[Node]:\n-        subnode = addnodes.toctree()\n-        subnode['parent'] = self.env.docname\n-\n-        # (title, ref) pairs, where ref may be a document, or an external link,\n-        # and title may be None if the document's title is to be used\n-        subnode['entries'] = []\n-        subnode['includefiles'] = []\n-        subnode['maxdepth'] = self.options.get('maxdepth', -1)\n-        subnode['caption'] = self.options.get('caption')\n-        subnode['glob'] = 'glob' in self.options\n-        subnode['hidden'] = 'hidden' in self.options\n-        subnode['includehidden'] = 'includehidden' in self.options\n-        subnode['numbered'] = self.options.get('numbered', 0)\n-        subnode['titlesonly'] = 'titlesonly' in self.options\n-        self.set_source_info(subnode)\n-        wrappernode = nodes.compound(classes=['toctree-wrapper'])\n-        wrappernode.append(subnode)\n-        self.add_name(wrappernode)\n-\n-        ret = self.parse_content(subnode)\n-        ret.append(wrappernode)\n-        return ret\n-\n-    def parse_content(self, toctree: addnodes.toctree) -> List[Node]:\n-        suffixes = self.config.source_suffix\n-\n-        # glob target documents\n-        all_docnames = self.env.found_docs.copy()\n-        all_docnames.remove(self.env.docname)  # remove current document\n-\n-        ret: List[Node] = []\n-        excluded = Matcher(self.config.exclude_patterns)\n-        for entry in self.content:\n-            if not entry:\n-                continue\n-            # look for explicit titles (\"Some Title <document>\")\n-            explicit = explicit_title_re.match(entry)\n-            if (toctree['glob'] and glob_re.match(entry) and\n-                    not explicit and not url_re.match(entry)):\n-                patname = docname_join(self.env.docname, entry)\n-                docnames = sorted(patfilter(all_docnames, patname))\n-                for docname in docnames:\n-                    all_docnames.remove(docname)  # don't include it again\n-                    toctree['entries'].append((None, docname))\n-                    toctree['includefiles'].append(docname)\n-                if not docnames:\n-                    logger.warning(__('toctree glob pattern %r didn\\'t match any documents'),\n-                                   entry, location=toctree)\n-            else:\n-                if explicit:\n-                    ref = explicit.group(2)\n-                    title = explicit.group(1)\n-                    docname = ref\n-                else:\n-                    ref = docname = entry\n-                    title = None\n-                # remove suffixes (backwards compatibility)\n-                for suffix in suffixes:\n-                    if docname.endswith(suffix):\n-                        docname = docname[:-len(suffix)]\n-                        break\n-                # absolutize filenames\n-                docname = docname_join(self.env.docname, docname)\n-                if url_re.match(ref) or ref == 'self':\n-                    toctree['entries'].append((title, ref))\n                 elif docname not in self.env.found_docs:\n                     if excluded(self.env.doc2path(docname, False)):\n                         message = __('toctree contains reference to excluded document %r')\n                         subtype = 'excluded'\n                     else:\n-                        message = __('toctree contains reference to nonexisting document %r')\n-                        subtype = 'not_readable'\n-\n-                    logger.warning(message, docname, type='toc', subtype=subtype,\n-                                   location=toctree)\n-                    self.env.note_reread()\n+                        # Allow references to special builder-generated pages\n+                        # like genindex, modindex, and search without warning.\n+                        special_pages = ('genindex','modindex','search')\n+                        if docname in special_pages:\n+                            # treat as existing; add to entries and includefiles\n+                            toctree['entries'].append((title, docname))\n+                            toctree['includefiles'].append(docname)\n+                            # ensure it's not considered for globbing again\n+                            if docname in all_docnames:\n+                                all_docnames.remove(docname)\n+                        else:\n+                            message = __('toctree contains reference to nonexisting document %r')\n+                            subtype = 'not_readable'\n+\n+                            logger.warning(message, docname, type='toc', subtype=subtype,\n+                                           location=toctree)\n+                            self.env.note_reread()\n                 else:\n-                    if docname in all_docnames:\n-                        all_docnames.remove(docname)\n-                    else:\n-                        logger.warning(__('duplicated entry found in toctree: %s'), docname,\n-                                       location=toctree)\n-\n-                    toctree['entries'].append((title, docname))\n-                    toctree['includefiles'].append(docname)\n-\n-        # entries contains all entries (self references, external links etc.)\n-        if 'reversed' in self.options:\n-            toctree['entries'] = list(reversed(toctree['entries']))\n-            toctree['includefiles'] = list(reversed(toctree['includefiles']))\n-\n-        return ret\n-\n-\n-class Author(SphinxDirective):\n-    \"\"\"\n-    Directive to give the name of the author of the current document\n-    or section. Shown in the output only if the show_authors option is on.\n-    \"\"\"\n-    has_content = False\n-    required_arguments = 1\n-    optional_arguments = 0\n-    final_argument_whitespace = True\n-    option_spec: OptionSpec = {}\n-\n-    def run(self) -> List[Node]:\n-        if not self.config.show_authors:\n-            return []\n-        para: Element = nodes.paragraph(translatable=False)\n-        emph = nodes.emphasis()\n-        para += emph\n-        if self.name == 'sectionauthor':\n-            text = _('Section author: ')\n-        elif self.name == 'moduleauthor':\n-            text = _('Module author: ')\n-        elif self.name == 'codeauthor':\n-            text = _('Code author: ')\n-        else:\n-            text = _('Author: ')\n-        emph += nodes.Text(text)\n-        inodes, messages = self.state.inline_text(self.arguments[0], self.lineno)\n-        emph.extend(inodes)\n-\n-        ret: List[Node] = [para]\n-        ret += messages\n-        return ret\n-\n-\n-class SeeAlso(BaseAdmonition):\n-    \"\"\"\n-    An admonition mentioning things to look at as reference.\n-    \"\"\"\n-    node_class = addnodes.seealso\n-\n-\n-class TabularColumns(SphinxDirective):\n-    \"\"\"\n-    Directive to give an explicit tabulary column definition to LaTeX.\n-    \"\"\"\n-    has_content = False\n-    required_arguments = 1\n-    optional_arguments = 0\n-    final_argument_whitespace = True\n-    option_spec: OptionSpec = {}\n-\n-    def run(self) -> List[Node]:\n-        node = addnodes.tabular_col_spec()\n-        node['spec'] = self.arguments[0]\n-        self.set_source_info(node)\n-        return [node]\n-\n-\n-class Centered(SphinxDirective):\n-    \"\"\"\n-    Directive to create a centered line of bold text.\n-    \"\"\"\n-    has_content = False\n-    required_arguments = 1\n-    optional_arguments = 0\n-    final_argument_whitespace = True\n-    option_spec: OptionSpec = {}\n-\n-    def run(self) -> List[Node]:\n-        if not self.arguments:\n-            return []\n-        subnode: Element = addnodes.centered()\n-        inodes, messages = self.state.inline_text(self.arguments[0], self.lineno)\n-        subnode.extend(inodes)\n-\n-        ret: List[Node] = [subnode]\n-        ret += messages\n-        return ret\n-\n-\n-class Acks(SphinxDirective):\n-    \"\"\"\n-    Directive for a list of names.\n-    \"\"\"\n-    has_content = True\n-    required_arguments = 0\n-    optional_arguments = 0\n-    final_argument_whitespace = False\n-    option_spec: OptionSpec = {}\n-\n-    def run(self) -> List[Node]:\n-        node = addnodes.acks()\n-        node.document = self.state.document\n-        self.state.nested_parse(self.content, self.content_offset, node)\n-        if len(node.children) != 1 or not isinstance(node.children[0],\n-                                                     nodes.bullet_list):\n-            logger.warning(__('.. acks content is not a list'),\n-                           location=(self.env.docname, self.lineno))\n-            return []\n-        return [node]\n-\n-\n-class HList(SphinxDirective):\n-    \"\"\"\n-    Directive for a list that gets compacted horizontally.\n-    \"\"\"\n-    has_content = True\n-    required_arguments = 0\n-    optional_arguments = 0\n-    final_argument_whitespace = False\n-    option_spec: OptionSpec = {\n-        'columns': int,\n-    }\n-\n-    def run(self) -> List[Node]:\n-        ncolumns = self.options.get('columns', 2)\n-        node = nodes.paragraph()\n-        node.document = self.state.document\n-        self.state.nested_parse(self.content, self.content_offset, node)\n-        if len(node.children) != 1 or not isinstance(node.children[0],\n-                                                     nodes.bullet_list):\n-            logger.warning(__('.. hlist content is not a list'),\n-                           location=(self.env.docname, self.lineno))\n-            return []\n-        fulllist = node.children[0]\n-        # create a hlist node where the items are distributed\n-        npercol, nmore = divmod(len(fulllist), ncolumns)\n-        index = 0\n-        newnode = addnodes.hlist()\n-        newnode['ncolumns'] = str(ncolumns)\n-        for column in range(ncolumns):\n-            endindex = index + ((npercol + 1) if column < nmore else npercol)\n-            bullet_list = nodes.bullet_list()\n-            bullet_list += fulllist.children[index:endindex]\n-            newnode += addnodes.hlistcol('', bullet_list)\n-            index = endindex\n-        return [newnode]\n-\n-\n-class Only(SphinxDirective):\n-    \"\"\"\n-    Directive to only include text if the given tag(s) are enabled.\n-    \"\"\"\n-    has_content = True\n-    required_arguments = 1\n-    optional_arguments = 0\n-    final_argument_whitespace = True\n-    option_spec: OptionSpec = {}\n-\n-    def run(self) -> List[Node]:\n-        node = addnodes.only()\n-        node.document = self.state.document\n-        self.set_source_info(node)\n-        node['expr'] = self.arguments[0]\n-\n-        # Same as util.nested_parse_with_titles but try to handle nested\n-        # sections which should be raised higher up the doctree.\n-        memo: Any = self.state.memo\n-        surrounding_title_styles = memo.title_styles\n-        surrounding_section_level = memo.section_level\n-        memo.title_styles = []\n-        memo.section_level = 0\n-        try:\n-            self.state.nested_parse(self.content, self.content_offset,\n-                                    node, match_titles=True)\n-            title_styles = memo.title_styles\n-            if (not surrounding_title_styles or\n-                    not title_styles or\n-                    title_styles[0] not in surrounding_title_styles or\n-                    not self.state.parent):\n-                # No nested sections so no special handling needed.\n-                return [node]\n-            # Calculate the depths of the current and nested sections.\n-            current_depth = 0\n-            parent = self.state.parent\n-            while parent:\n-                current_depth += 1\n-                parent = parent.parent\n-            current_depth -= 2\n-            title_style = title_styles[0]\n-            nested_depth = len(surrounding_title_styles)\n-            if title_style in surrounding_title_styles:\n-                nested_depth = surrounding_title_styles.index(title_style)\n-            # Use these depths to determine where the nested sections should\n-            # be placed in the doctree.\n-            n_sects_to_raise = current_depth - nested_depth + 1\n-            parent = cast(nodes.Element, self.state.parent)\n-            for _i in range(n_sects_to_raise):\n-                if parent.parent:\n-                    parent = parent.parent\n-            parent.append(node)\n-            return []\n-        finally:\n-            memo.title_styles = surrounding_title_styles\n-            memo.section_level = surrounding_section_level\n-\n-\n-class Include(BaseInclude, SphinxDirective):\n-    \"\"\"\n-    Like the standard \"Include\" directive, but interprets absolute paths\n-    \"correctly\", i.e. relative to source directory.\n-    \"\"\"\n-\n-    def run(self) -> List[Node]:\n-        if self.arguments[0].startswith('<') and \\\n-           self.arguments[0].endswith('>'):\n-            # docutils \"standard\" includes, do not do path processing\n-            return super().run()\n-        rel_filename, filename = self.env.relfn2path(self.arguments[0])\n-        self.arguments[0] = filename\n-        self.env.note_included(filename)\n-        return super().run()\n-\n-\n-def setup(app: \"Sphinx\") -> Dict[str, Any]:\n-    directives.register_directive('toctree', TocTree)\n-    directives.register_directive('sectionauthor', Author)\n-    directives.register_directive('moduleauthor', Author)\n-    directives.register_directive('codeauthor', Author)\n-    directives.register_directive('seealso', SeeAlso)\n-    directives.register_directive('tabularcolumns', TabularColumns)\n-    directives.register_directive('centered', Centered)\n-    directives.register_directive('acks', Acks)\n-    directives.register_directive('hlist', HList)\n-    directives.register_directive('only', Only)\n-    directives.register_directive('include', Include)\n-\n-    # register the standard rst class directive under a different name\n-    # only for backwards compatibility now\n-    directives.register_directive('cssclass', Class)\n-    # new standard name when default-domain with \"class\" is in effect\n-    directives.register_directive('rst-class', Class)\n-\n-    return {\n-        'version': 'builtin',\n-        'parallel_read_safe': True,\n-        'parallel_write_safe': True,\n-    }\n"
  },
  "sphinx-doc__sphinx-7590": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-7590",
    "model_patch": "diff --git a/sphinx/domains/cpp.py b/sphinx/domains/cpp.py\nindex fe52d881d..78d587b01 100644\n--- a/sphinx/domains/cpp.py\n+++ b/sphinx/domains/cpp.py\n@@ -855,6 +855,33 @@ class ASTStringLiteral(ASTLiteral):\n         signode.append(nodes.Text(txt, txt))\n \n \n+\n+class ASTUserDefinedLiteral(ASTLiteral):\n+    def __init__(self, literal, suffix):\n+        # literal: the raw literal text or an ASTLiteral instance\n+        # suffix: the user-defined suffix (including any nested-name qualifiers)\n+        self.literal = literal\n+        self.suffix = suffix\n+\n+    def _stringify(self, transform):\n+        # Represent as literal followed immediately by suffix\n+        lit = self.literal if isinstance(self.literal, str) else str(self.literal)\n+        return lit + self.suffix\n+\n+    def get_id(self, version: int) -> str:\n+        # Try to build a reasonable id combining underlying literal id and suffix\n+        if hasattr(self.literal, 'get_id'):\n+            base = self.literal.get_id(version)\n+            if base.endswith('E'):\n+                return base[:-1] + self.suffix + 'E'\n+            return base + self.suffix\n+        else:\n+            return 'L' + str(self.literal) + self.suffix + 'E'\n+\n+    def describe_signature(self, signode, mode, env, symbol):\n+        txt = str(self)\n+        signode.append(nodes.Text(txt, txt))\n+\n class ASTCharLiteral(ASTLiteral):\n     def __init__(self, prefix: str, data: str) -> None:\n         self.prefix = prefix  # may be None when no prefix\n@@ -4664,10 +4691,30 @@ class DefinitionParser(BaseParser):\n             if self.match(regex):\n                 while self.current_char in 'uUlLfF':\n                     self.pos += 1\n-                return ASTNumberLiteral(self.definition[pos:self.pos])\n+                numtxt = self.definition[pos:self.pos]\n+                save = self.pos\n+                # UDL suffix is attached immediately (no whitespace) and starts with identifier char or ':'\n+                if  (self.current_char is not None and (self.current_char.isalpha() or self.current_char == '_' or self.current_char == ':')):\n+                    i = self.pos\n+                    while i < len(self.definition) and (self.definition[i].isalnum() or self.definition[i] in '_:'):\n+                        i += 1\n+                    udl = self.definition[self.pos:i]\n+                    self.pos = i\n+                    return ASTUserDefinedLiteral(numtxt, udl)\n+                self.pos = save\n+                return ASTNumberLiteral(numtxt)\n \n         string = self._parse_string()\n         if string is not None:\n+            save = self.pos\n+            if  (self.current_char is not None and (self.current_char.isalpha() or self.current_char == '_' or self.current_char == ':')):\n+                i = self.pos\n+                while i < len(self.definition) and (self.definition[i].isalnum() or self.definition[i] in '_:'):\n+                    i += 1\n+                udl = self.definition[self.pos:i]\n+                self.pos = i\n+                return ASTUserDefinedLiteral(string, udl)\n+            self.pos = save\n             return ASTStringLiteral(string)\n \n         # character-literal\n"
  },
  "sphinx-doc__sphinx-7748": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-7748",
    "model_patch": "diff --git a/sphinx/ext/autodoc/__init__.py b/sphinx/ext/autodoc/__init__.py\nindex f8e4be999..dbd369442 100644\n--- a/sphinx/ext/autodoc/__init__.py\n+++ b/sphinx/ext/autodoc/__init__.py\n@@ -1049,24 +1049,38 @@ class DocstringSignatureMixin:\n             # no lines in docstring, no match\n             if not doclines:\n                 continue\n-            # match first line of docstring against signature RE\n-            match = py_ext_sig_re.match(doclines[0])\n-            if not match:\n+            # collect consecutive leading signature lines (to support overloaded signatures)\n+            args_list = []\n+            retann_first = None\n+            consumed = 0\n+            for line in doclines:\n+                match = py_ext_sig_re.match(line)\n+                if not match:\n+                    break\n+                exmod, path, base, args, retann = match.groups()\n+                # the base name must match ours\n+                valid_names = [self.objpath[-1]]  # type: ignore\n+                if isinstance(self, ClassDocumenter):\n+                    valid_names.append('__init__')\n+                    if hasattr(self.object, '__mro__'):\n+                        valid_names.extend(cls.__name__ for cls in self.object.__mro__)\n+                if base not in valid_names:\n+                    # stop collecting if the signature is for a different name\n+                    break\n+                args_list.append(args or '')\n+                if retann_first is None and retann:\n+                    retann_first = retann\n+                consumed += 1\n+            # if we found at least one signature line, use them\n+            if not args_list:\n                 continue\n-            exmod, path, base, args, retann = match.groups()\n-            # the base name must match ours\n-            valid_names = [self.objpath[-1]]  # type: ignore\n-            if isinstance(self, ClassDocumenter):\n-                valid_names.append('__init__')\n-                if hasattr(self.object, '__mro__'):\n-                    valid_names.extend(cls.__name__ for cls in self.object.__mro__)\n-            if base not in valid_names:\n-                continue\n-            # re-prepare docstring to ignore more leading indentation\n+            # re-prepare docstring to ignore all leading signature lines we consumed\n             tab_width = self.directive.state.document.settings.tab_width  # type: ignore\n-            self._new_docstrings[i] = prepare_docstring('\\n'.join(doclines[1:]),\n+            self._new_docstrings[i] = prepare_docstring('\\n'.join(doclines[consumed:]),\n                                                         tabsize=tab_width)\n-            result = args, retann\n+            # combine collected argument strings into one (preserve multiple overloads)\n+            combined_args = '\\n'.join(args_list)\n+            result = combined_args, retann_first\n             # don't look any further\n             break\n         return result\n"
  },
  "sphinx-doc__sphinx-7757": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-7757",
    "model_patch": "diff --git a/sphinx/util/inspect.py b/sphinx/util/inspect.py\nindex 15f0d66e2..05bd4cb46 100644\n--- a/sphinx/util/inspect.py\n+++ b/sphinx/util/inspect.py\n@@ -86,9 +86,7 @@ def getargspec(func: Callable) -> Any:\n         kind = param.kind\n         name = param.name\n \n-        if kind is Parameter.POSITIONAL_ONLY:\n-            args.append(name)\n-        elif kind is Parameter.POSITIONAL_OR_KEYWORD:\n+        if kind in (Parameter.POSITIONAL_ONLY, Parameter.POSITIONAL_OR_KEYWORD):\n             args.append(name)\n             if param.default is not param.empty:\n                 defaults += (param.default,)  # type: ignore\n"
  },
  "sphinx-doc__sphinx-7985": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-7985",
    "model_patch": "diff --git a/sphinx/builders/linkcheck.py b/sphinx/builders/linkcheck.py\nindex dd5317087..d5000485d 100644\n--- a/sphinx/builders/linkcheck.py\n+++ b/sphinx/builders/linkcheck.py\n@@ -213,7 +213,52 @@ class CheckExternalLinksBuilder(Builder):\n             if len(uri) == 0 or uri.startswith(('#', 'mailto:', 'ftp:')):\n                 return 'unchecked', '', 0\n             elif not uri.startswith(('http:', 'https:')):\n-                return 'local', '', 0\n+                # resolve local/internal links against the environment\n+                # possible forms: docname#anchor, #anchor (current doc), docname\n+                target = uri\n+                anchor = None\n+                if '#' in uri:\n+                    target, anchor = uri.split('#', 1)\n+                # determine target docname: if empty, use current doc\n+                if not target:\n+                    doc = docname\n+                else:\n+                    # strip any file extension and leading/trailing slashes\n+                    doc = target.rstrip('/').split('#')[0]\n+                    if doc.endswith(('.html', '.htm', '.rst')):\n+                        doc = doc.rsplit('.', 1)[0]\n+                # normalize doc paths: remove leading './'\n+                if doc.startswith('./'):\n+                    doc = doc[2:]\n+                # check existence of the target document\n+                if doc not in self.env.all_docs and doc not in self.env.titles and doc not in getattr(self.env, 'found_docs', {}):\n+                    return 'broken', __('Local target %r not found') % uri, 0\n+                # if an anchor is present, try to find it\n+                if anchor:\n+                    # check standard labels and anonymous labels\n+                    labfound = False\n+                    try:\n+                        std = self.env.domains.get('std')\n+                        if std:\n+                            # domain data accessible via domain object methods\n+                            if anchor in std.data.get('labels', {}) or anchor in std.data.get('anonlabels', {}):\n+                                labfound = True\n+                    except Exception:\n+                        labfound = False\n+                    # check toc section ids for the doc\n+                    if not labfound:\n+                        secids = self.env.toc_secnumbers.get(doc, {})\n+                        if anchor in secids:\n+                            labfound = True\n+                    # check titles node ids\n+                    if not labfound:\n+                        title = self.env.titles.get(doc)\n+                        if title and getattr(title, 'ids', None):\n+                            if anchor in title.ids:\n+                                labfound = True\n+                    if not labfound:\n+                        return 'broken', \"Anchor '%s' not found\" % anchor, 0\n+                return 'working', '', 0\n             elif uri in self.good:\n                 return 'working', 'old', 0\n             elif uri in self.broken:\n"
  },
  "sphinx-doc__sphinx-8035": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-8035",
    "model_patch": "diff --git a/sphinx/ext/autodoc/__init__.py b/sphinx/ext/autodoc/__init__.py\nindex 8c3509c3b..b984bd4ff 100644\n--- a/sphinx/ext/autodoc/__init__.py\n+++ b/sphinx/ext/autodoc/__init__.py\n@@ -859,7 +859,7 @@ class ModuleDocumenter(Documenter):\n         'show-inheritance': bool_option, 'synopsis': identity,\n         'platform': identity, 'deprecated': bool_option,\n         'member-order': member_order_option, 'exclude-members': members_set_option,\n-        'private-members': bool_option, 'special-members': members_option,\n+        'private-members': members_option, 'special-members': members_option,\n         'imported-members': bool_option, 'ignore-module-all': bool_option\n     }  # type: Dict[str, Callable]\n \n@@ -1279,7 +1279,7 @@ class ClassDocumenter(DocstringSignatureMixin, ModuleLevelDocumenter):  # type:\n         'noindex': bool_option, 'inherited-members': inherited_members_option,\n         'show-inheritance': bool_option, 'member-order': member_order_option,\n         'exclude-members': members_set_option,\n-        'private-members': bool_option, 'special-members': members_option,\n+        'private-members': members_option, 'special-members': members_option,\n     }  # type: Dict[str, Callable]\n \n     _signature_class = None  # type: Any\n"
  },
  "sphinx-doc__sphinx-8056": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-8056",
    "model_patch": "diff --git a/sphinx/ext/napoleon/docstring.py b/sphinx/ext/napoleon/docstring.py\nindex b3daa06f1..cd9070e21 100644\n--- a/sphinx/ext/napoleon/docstring.py\n+++ b/sphinx/ext/napoleon/docstring.py\n@@ -386,23 +386,31 @@ class GoogleDocstring:\n         else:\n             return [prefix]\n \n-    def _format_docutils_params(self, fields: List[Tuple[str, str, List[str]]],\n-                                field_role: str = 'param', type_role: str = 'type'\n-                                ) -> List[str]:\n+    def _format_docutils_params(self, fields: List[Tuple[str, str, List[str]]], field_role: str = 'param', type_role: str = 'type') -> List[str]:\n         lines = []\n         for _name, _type, _desc in fields:\n             _desc = self._strip_empty(_desc)\n+            # Handle comma-separated parameter names like 'x1, x2' by splitting\n+            if ', ' in _name:\n+                names = [n.strip() for n in _name.split(',')]\n+            else:\n+                names = [_name]\n             if any(_desc):\n                 _desc = self._fix_field_desc(_desc)\n-                field = ':%s %s: ' % (field_role, _name)\n-                lines.extend(self._format_block(field, _desc))\n+                # Attach the description to the first name and empty desc for aliases\n+                for i, nm in enumerate(names):\n+                    field = ':%s %s: ' % (field_role, nm)\n+                    if i == 0:\n+                        lines.extend(self._format_block(field, _desc))\n+                    else:\n+                        lines.append(':%s %s:' % (field_role, nm))\n             else:\n-                lines.append(':%s %s:' % (field_role, _name))\n-\n+                for nm in names:\n+                    lines.append(':%s %s:' % (field_role, nm))\n             if _type:\n-                lines.append(':%s %s: %s' % (type_role, _name, _type))\n+                for nm in names:\n+                    lines.append(':%s %s: %s' % (type_role, nm, _type))\n         return lines + ['']\n-\n     def _format_field(self, _name: str, _type: str, _desc: List[str]) -> List[str]:\n         _desc = self._strip_empty(_desc)\n         has_desc = any(_desc)\n"
  },
  "sphinx-doc__sphinx-8265": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-8265",
    "model_patch": "diff --git a/sphinx/util/inspect.py b/sphinx/util/inspect.py\nindex 378174993..249e21065 100644\n--- a/sphinx/util/inspect.py\n+++ b/sphinx/util/inspect.py\n@@ -358,6 +358,7 @@ def safe_getmembers(object: Any, predicate: Callable[[str], bool] = None,\n         except AttributeError:\n             continue\n         if not predicate or predicate(value):\n+\n             results.append((key, value))\n     results.sort()\n     return results\n@@ -365,6 +366,9 @@ def safe_getmembers(object: Any, predicate: Callable[[str], bool] = None,\n \n def object_description(object: Any) -> str:\n     \"\"\"A repr() implementation that returns text safe to use in reST context.\"\"\"\n+    if isinstance(object, tuple):\n+        # Ensure tuples are rendered with parentheses (e.g. (1, 1, 1))\n+        return \"(%s)\" % \", \".join(object_description(x) for x in object)\n     if isinstance(object, dict):\n         try:\n             sorted_keys = sorted(object)\n"
  },
  "sphinx-doc__sphinx-8269": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-8269",
    "model_patch": "diff --git a/sphinx/builders/linkcheck.py b/sphinx/builders/linkcheck.py\nindex 9b54afc7c..abb8927ae 100644\n--- a/sphinx/builders/linkcheck.py\n+++ b/sphinx/builders/linkcheck.py\n@@ -166,7 +166,8 @@ class CheckExternalLinksBuilder(Builder):\n                     # Read the whole document and see if #anchor exists\n                     response = requests.get(req_url, stream=True, config=self.app.config,\n                                             auth=auth_info, **kwargs)\n-                    found = check_anchor(response, unquote(anchor))\n+                    response.raise_for_status()\n+\tfound = check_anchor(response, unquote(anchor))\n \n                     if not found:\n                         raise Exception(__(\"Anchor '%s' not found\") % anchor)\n"
  },
  "sphinx-doc__sphinx-8475": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-8475",
    "model_patch": "diff --git a/sphinx/builders/linkcheck.py b/sphinx/builders/linkcheck.py\nindex 1dc0337c3..4104fe658 100644\n--- a/sphinx/builders/linkcheck.py\n+++ b/sphinx/builders/linkcheck.py\n@@ -20,7 +20,7 @@ from urllib.parse import unquote, urlparse\n \n from docutils import nodes\n from docutils.nodes import Node\n-from requests.exceptions import HTTPError\n+from requests.exceptions import HTTPError, TooManyRedirects\n \n from sphinx.application import Sphinx\n from sphinx.builders import Builder\n@@ -172,7 +172,7 @@ class CheckExternalLinksBuilder(Builder):\n                                                  config=self.app.config, auth=auth_info,\n                                                  **kwargs)\n                         response.raise_for_status()\n-                    except HTTPError:\n+                    except (HTTPError, TooManyRedirects):\n                         # retry with GET request if that fails, some servers\n                         # don't like HEAD requests.\n                         response = requests.get(req_url, stream=True, config=self.app.config,\n"
  },
  "sphinx-doc__sphinx-8548": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-8548",
    "model_patch": "diff --git a/sphinx/ext/autodoc/__init__.py b/sphinx/ext/autodoc/__init__.py\nindex ddfd2b365..c00e54c47 100644\n--- a/sphinx/ext/autodoc/__init__.py\n+++ b/sphinx/ext/autodoc/__init__.py\n@@ -746,17 +746,32 @@ class Documenter:\n                         keep = has_doc or self.options.undoc_members\n                 else:\n                     keep = False\n-            elif (namespace, membername) in attr_docs:\n-                if want_all and isprivate:\n-                    if self.options.private_members is None:\n-                        keep = False\n+            else:\n+                key = (namespace, membername)\n+                # if not found, try base classes' namespaces for inherited attributes\n+                if key not in attr_docs and inspect.isclass(self.object):\n+                    for cls in self.object.__mro__[1:]:\n+                        parts = namespace.split('.') if namespace else []\n+                        if parts:\n+                            parts[-1] = getattr(cls, '__name__', parts[-1])\n+                            test_ns = '.'.join(parts)\n+                        else:\n+                            test_ns = getattr(cls, '__name__', namespace)\n+                        base_key = (test_ns, membername)\n+                        if base_key in attr_docs:\n+                            key = base_key\n+                            break\n+                if key in attr_docs:\n+                    if want_all and isprivate:\n+                        if self.options.private_members is None:\n+                            keep = False\n+                        else:\n+                            keep = membername in self.options.private_members\n                     else:\n-                        keep = membername in self.options.private_members\n-                else:\n-                    # keep documented attributes\n-                    keep = True\n-                isattr = True\n-            elif want_all and isprivate:\n+                        # keep documented attributes\n+                        keep = True\n+                    isattr = True\n+            if want_all and isprivate:\n                 if has_doc or self.options.undoc_members:\n                     if self.options.private_members is None:\n                         keep = False\n@@ -2496,4 +2511,4 @@ def setup(app: Sphinx) -> Dict[str, Any]:\n     app.setup_extension('sphinx.ext.autodoc.type_comment')\n     app.setup_extension('sphinx.ext.autodoc.typehints')\n \n-    return {'version': sphinx.__display_version__, 'parallel_read_safe': True}\n+    return {'version': sphinx.__display_version__, 'parallel_read_safe': True}\n\\ No newline at end of file\n"
  },
  "sphinx-doc__sphinx-8721": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-8721",
    "model_patch": "diff --git a/sphinx/ext/viewcode.py b/sphinx/ext/viewcode.py\nindex a7d52a91c..6b7f17672 100644\n--- a/sphinx/ext/viewcode.py\n+++ b/sphinx/ext/viewcode.py\n@@ -52,6 +52,9 @@ def _get_full_modname(app: Sphinx, modname: str, attribute: str) -> Optional[str\n \n def doctree_read(app: Sphinx, doctree: Node) -> None:\n     env = app.builder.env\n+    if app.builder.name.startswith(\"epub\") and not env.config.viewcode_enable_epub:\n+        return\n+\n     if not hasattr(env, '_viewcode_modules'):\n         env._viewcode_modules = {}  # type: ignore\n     if app.builder.name == \"singlehtml\":\n"
  },
  "sphinx-doc__sphinx-9230": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-9230",
    "model_patch": "diff --git a/sphinx/ext/napoleon/docstring.py b/sphinx/ext/napoleon/docstring.py\nindex d8cb75a5f..6c2b8ec63 100644\n--- a/sphinx/ext/napoleon/docstring.py\n+++ b/sphinx/ext/napoleon/docstring.py\n@@ -1160,8 +1160,34 @@ class NumpyDocstring(GoogleDocstring):\n     def _escape_args_and_kwargs(self, name: str) -> str:\n         func = super()._escape_args_and_kwargs\n \n-        if \", \" in name:\n-            return \", \".join(func(param) for param in name.split(\", \"))\n+        # Split on top-level commas only (ignore commas inside parentheses),\n+        # so types like \"dict(str, str) opc_meta\" are not split incorrectly.\n+        if \",\" in name:\n+            parts = []\n+            buf = []\n+            depth = 0\n+            i = 0\n+            while i < len(name):\n+                ch = name[i]\n+                if ch == '(':\n+                    depth += 1\n+                    buf.append(ch)\n+                elif ch == ')':\n+                    depth = max(depth - 1, 0)\n+                    buf.append(ch)\n+                elif ch == ',' and depth == 0:\n+                    # if next char is space, consume it to mimic previous behavior\n+                    part = ''.join(buf).strip()\n+                    parts.append(part)\n+                    buf = []\n+                    if i + 1 < len(name) and name[i+1] == ' ':\n+                        i += 1\n+                else:\n+                    buf.append(ch)\n+                i += 1\n+            if buf:\n+                parts.append(''.join(buf).strip())\n+            return \", \".join(func(param) for param in parts)\n         else:\n             return func(name)\n \n"
  },
  "sphinx-doc__sphinx-9281": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-9281",
    "model_patch": "diff --git a/sphinx/util/inspect.py b/sphinx/util/inspect.py\nindex a415a7074..3e781af3e 100644\n--- a/sphinx/util/inspect.py\n+++ b/sphinx/util/inspect.py\n@@ -457,6 +457,16 @@ def object_description(object: Any) -> str:\n         else:\n             return \"frozenset({%s})\" % \", \".join(object_description(x)\n                                                  for x in sorted_values)\n+    # Render Enum members as Class.Member for readability\n+    try:\n+        import enum as _enum_module\n+    except Exception:\n+        _enum_module = None\n+    if _enum_module is not None and isinstance(object, _enum_module.Enum):\n+        try:\n+            return \"%s.%s\" % (object.__class__.__name__, object.name)\n+        except Exception:\n+            pass\n     try:\n         s = repr(object)\n     except Exception as exc:\n@@ -465,8 +475,6 @@ def object_description(object: Any) -> str:\n     # ``<__main__.A at 0x7f68cb685710>``\n     s = memory_address_re.sub('', s)\n     return s.replace('\\n', ' ')\n-\n-\n def is_builtin_class_method(obj: Any, attr_name: str) -> bool:\n     \"\"\"If attr_name is implemented at builtin class, return True.\n \n"
  },
  "sphinx-doc__sphinx-9320": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-9320",
    "model_patch": "diff --git a/sphinx/cmd/quickstart.py b/sphinx/cmd/quickstart.py\nindex a40a21073..3ac6398fc 100644\n--- a/sphinx/cmd/quickstart.py\n+++ b/sphinx/cmd/quickstart.py\n@@ -146,7 +146,10 @@ def do_prompt(text: str, default: str = None, validator: Callable[[str], Any] =\n         else:\n             prompt = colorize(COLOR_QUESTION, prompt, input_mode=False)\n         x = term_input(prompt).strip()\n-        if default and not x:\n+        if default is not None and not x:\n+            # If default is an empty string, return it (signals exit) without validation\n+            if default == '':\n+                return default\n             x = default\n         try:\n             x = validator(x)\n"
  },
  "sphinx-doc__sphinx-9367": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-9367",
    "model_patch": "diff --git a/sphinx/pycode/ast.py b/sphinx/pycode/ast.py\nindex f541ec0a9..713f4426e 100644\n--- a/sphinx/pycode/ast.py\n+++ b/sphinx/pycode/ast.py\n@@ -214,7 +214,10 @@ class _UnparseVisitor(ast.NodeVisitor):\n \n     def visit_Tuple(self, node: ast.Tuple) -> str:\n         if node.elts:\n-            return \"(\" + \", \".join(self.visit(e) for e in node.elts) + \")\"\n+            if len(node.elts) == 1:\n+                return \"(\" + self.visit(node.elts[0]) + \",)\"\n+            else:\n+                return \"(\" + \", \".join(self.visit(e) for e in node.elts) + \")\"\n         else:\n             return \"()\"\n \n"
  },
  "sphinx-doc__sphinx-9461": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-9461",
    "model_patch": "diff --git a/sphinx/ext/autodoc/__init__.py b/sphinx/ext/autodoc/__init__.py\nindex 1cecb1f79..ccf24986a 100644\n--- a/sphinx/ext/autodoc/__init__.py\n+++ b/sphinx/ext/autodoc/__init__.py\n@@ -2661,7 +2661,7 @@ class PropertyDocumenter(DocstringStripSignatureMixin, ClassLevelDocumenter):  #\n     @classmethod\n     def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                             ) -> bool:\n-        return inspect.isproperty(member) and isinstance(parent, ClassDocumenter)\n+        return (inspect.isproperty(member) or (inspect.isclassmethod(member) and isinstance(getattr(member, '__func__', None), property))) and isinstance(parent, ClassDocumenter)\n \n     def document_members(self, all_members: bool = False) -> None:\n         pass\n"
  },
  "sphinx-doc__sphinx-9698": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-9698",
    "model_patch": "diff --git a/sphinx/domains/python.py b/sphinx/domains/python.py\nindex cd02eaad5..53c74251d 100644\n--- a/sphinx/domains/python.py\n+++ b/sphinx/domains/python.py\n@@ -789,14 +789,20 @@ class PyMethod(PyObject):\n                 clsname = '.'.join([modname, clsname])\n         except ValueError:\n             if modname:\n-                return _('%s() (in module %s)') % (name, modname)\n+                if 'property' in self.options:\n+                    return _('%s (in module %s)') % (name, modname)\n+                else:\n+                    return _('%s() (in module %s)') % (name, modname)\n             else:\n-                return '%s()' % name\n+                if 'property' in self.options:\n+                    return '%s' % name\n+                else:\n+                    return '%s()' % name\n \n         if 'classmethod' in self.options:\n             return _('%s() (%s class method)') % (methname, clsname)\n         elif 'property' in self.options:\n-            return _('%s() (%s property)') % (methname, clsname)\n+            return _('%s (%s property)') % (methname, clsname)\n         elif 'staticmethod' in self.options:\n             return _('%s() (%s static method)') % (methname, clsname)\n         else:\n"
  },
  "django__django-12143": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-12143",
    "model_patch": "diff --git a/django/contrib/admin/options.py b/django/contrib/admin/options.py\nindex 137e6faa0f..a100f1c499 100644\n--- a/django/contrib/admin/options.py\n+++ b/django/contrib/admin/options.py\n@@ -1631,7 +1631,7 @@ class ModelAdmin(BaseModelAdmin):\n \n     def _get_edited_object_pks(self, request, prefix):\n         \"\"\"Return POST data values of list_editable primary keys.\"\"\"\n-        pk_pattern = re.compile(r'{}-\\d+-{}$'.format(prefix, self.model._meta.pk.name))\n+        pk_pattern = re.compile(r'{}-\\d+-{}$'.format(re.escape(prefix), self.model._meta.pk.name))\n         return [value for key, value in request.POST.items() if pk_pattern.match(key)]\n \n     def _get_list_editable_queryset(self, request, prefix):\n"
  },
  "django__django-12273": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "django__django-12273",
    "model_patch": "diff --git a/django/db/models/base.py b/django/db/models/base.py\nindex 844c01e95e..f0f8924809 100644\n--- a/django/db/models/base.py\n+++ b/django/db/models/base.py\n@@ -803,10 +803,12 @@ class Model(metaclass=ModelBase):\n         meta = cls._meta\n         inserted = False\n         for parent, field in meta.parents.items():\n-            # Make sure the link fields are synced between parent and self.\n-            if (field and getattr(self, parent._meta.pk.attname) is None and\n-                    getattr(self, field.attname) is not None):\n-                setattr(self, parent._meta.pk.attname, getattr(self, field.attname))\n+            # Note: do not pre-sync the parent's primary key from the child's\n+            # link field here. The parent's PK should be determined by saving\n+            # the parent itself; the child's link field will be populated from\n+            # the parent's PK after the parent is saved (see below). Pre-syncing\n+            # could prevent a reset of the child's primary key (setting it to\n+            # None) from resulting in an INSERT on save().\n             parent_inserted = self._save_parents(cls=parent, using=using, update_fields=update_fields)\n             updated = self._save_table(\n                 cls=parent, using=using, update_fields=update_fields,\n"
  },
  "sphinx-doc__sphinx-11510": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-11510",
    "model_patch": "diff --git a/sphinx/__init__.py b/sphinx/__init__.py\nindex 7fc253850..33f215371 100644\n--- a/sphinx/__init__.py\n+++ b/sphinx/__init__.py\n@@ -19,6 +19,20 @@ warnings.filterwarnings('ignore', \"'U' mode is deprecated\",\n warnings.filterwarnings('ignore', 'The frontend.Option class .*',\n                         DeprecationWarning, module='docutils.frontend')\n \n+# Monkey-patch docutils FileInput to SphinxFileInput\n+# Ensure docutils' FileInput uses SphinxFileInput so included files trigger\n+# Sphinx's read path (and emit 'source-read') when read by docutils directives.\n+try:\n+    import docutils.io as _docutils_io\n+    from .io import SphinxFileInput as _SphinxFileInput\n+    # Only override if it appears to be the original FileInput class.\n+    if getattr(_docutils_io, 'FileInput', None) is not _SphinxFileInput:\n+        _docutils_io.FileInput = _SphinxFileInput\n+except Exception:\n+    # If anything goes wrong, don't prevent Sphinx from functioning.\n+    pass\n+\n+\n __version__ = '7.2.0'\n __display_version__ = __version__  # used for command line version\n \n"
  },
  "sphinx-doc__sphinx-8551": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-8551",
    "model_patch": "diff --git a/sphinx/util/docfields.py b/sphinx/util/docfields.py\nindex 404bb127f..6da1ff86b 100644\n--- a/sphinx/util/docfields.py\n+++ b/sphinx/util/docfields.py\n@@ -70,8 +70,11 @@ class Field:\n                   contnode: Node = None, env: \"BuildEnvironment\" = None) -> Node:\n         if not rolename:\n             return contnode or innernode(target, target)\n-        refnode = addnodes.pending_xref('', refdomain=domain, refexplicit=False,\n-                                        reftype=rolename, reftarget=target)\n+        if env:\n+            kwargs = {'py:module': env.ref_context.get('py:module'), 'py:class': env.ref_context.get('py:class')}\n+        else:\n+            kwargs = {}\n+        refnode = addnodes.pending_xref('', refdomain=domain, refexplicit=False, reftype=rolename, reftarget=target, **kwargs)\n         refnode += contnode or innernode(target, target)\n         if env:\n             env.get_domain(domain).process_field_xref(refnode)\n"
  },
  "sphinx-doc__sphinx-8638": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-8638",
    "model_patch": "diff --git a/sphinx/domains/python.py b/sphinx/domains/python.py\nindex 000e2e8d3..d76b54551 100644\n--- a/sphinx/domains/python.py\n+++ b/sphinx/domains/python.py\n@@ -1157,9 +1157,15 @@ class PythonDomain(Domain):\n                     else:\n                         # \"fuzzy\" searching mode\n                         searchname = '.' + name\n-                        matches = [(oname, self.objects[oname]) for oname in self.objects\n-                                   if oname.endswith(searchname) and\n-                                   self.objects[oname].objtype in objtypes]\n+                        if modname:\n+                            matches = [(oname, self.objects[oname]) for oname in self.objects\n+                                       if oname.startswith(modname + searchname) and\n+                                       oname.endswith(searchname) and\n+                                       self.objects[oname].objtype in objtypes]\n+                        else:\n+                            matches = [(oname, self.objects[oname]) for oname in self.objects\n+                                       if oname.endswith(searchname) and\n+                                       self.objects[oname].objtype in objtypes]\n         else:\n             # NOTE: searching for exact match, object type is not considered\n             if name in self.objects:\n"
  },
  "sphinx-doc__sphinx-9229": {
    "model_name_or_path": "openai/gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-9229",
    "model_patch": "diff --git a/sphinx/pycode/parser.py b/sphinx/pycode/parser.py\nindex a00b481ce..ab05260d7 100644\n--- a/sphinx/pycode/parser.py\n+++ b/sphinx/pycode/parser.py\n@@ -400,16 +400,25 @@ class VariableCommentPicker(ast.NodeVisitor):\n \n     def visit_Expr(self, node: ast.Expr) -> None:\n         \"\"\"Handles Expr node and pick up a comment if string.\"\"\"\n+        # Support both ast.Str (older versions) and ast.Constant (py3.8+)\n         if (isinstance(self.previous, (ast.Assign, ast.AnnAssign)) and\n-                isinstance(node.value, ast.Str)):\n+                (isinstance(node.value, ast.Str) or (hasattr(ast, 'Constant') and isinstance(node.value, ast.Constant) and isinstance(getattr(node.value, 'value', None), str)))):\n             try:\n                 targets = get_assign_targets(self.previous)\n                 varnames = get_lvar_names(targets[0], self.get_self())\n                 for varname in varnames:\n-                    if isinstance(node.value.s, str):\n-                        docstring = node.value.s\n+                    if isinstance(node.value, ast.Str):\n+                        if isinstance(node.value.s, str):\n+                            docstring = node.value.s\n+                        else:\n+                            docstring = node.value.s.decode(self.encoding or 'utf-8')\n                     else:\n-                        docstring = node.value.s.decode(self.encoding or 'utf-8')\n+                        # ast.Constant with a string value\n+                        val = getattr(node.value, 'value', None)\n+                        if isinstance(val, str):\n+                            docstring = val\n+                        else:\n+                            docstring = getattr(node.value, 's', '')\n \n                     self.add_variable_comment(varname, dedent_docstring(docstring))\n                     self.add_entry(varname)\n"
  }
}